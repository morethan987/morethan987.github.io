<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>LLM on More&#39;s awesome website</title>
    <link>https://morethan987.github.io/en/tags/llm/</link>
    <description>Recent content in LLM on More&#39;s awesome website</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <managingEditor>morthan@qq.com (Morethan)</managingEditor>
    <webMaster>morthan@qq.com (Morethan)</webMaster>
    <copyright>Â© 2025 Morethan</copyright>
    <lastBuildDate>Wed, 28 May 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://morethan987.github.io/en/tags/llm/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Notes on the Foundation of Large Models</title>
      <link>https://morethan987.github.io/en/blog/llm-foundation-notes/</link>
      <pubDate>Wed, 28 May 2025 00:00:00 +0000</pubDate>
      <author>morthan@qq.com (Morethan)</author>
      <guid>https://morethan987.github.io/en/blog/llm-foundation-notes/</guid>
      <description>&lt;div class=&#34;lead text-neutral-500 dark:text-neutral-400 !mb-9 text-xl&#34;&gt;
  Notes written after reading the textbook &amp;ldquo;Foundation of Large Models&amp;rdquo; from Zhejiang University
&lt;/div&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Preface
    &lt;div id=&#34;preface&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none&#34;&gt;
        &lt;a class=&#34;text-primary-300 dark:text-neutral-700 !no-underline&#34; href=&#34;#preface&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;
    
&lt;/h2&gt;
&lt;p&gt;The development of large model technology can be described as &amp;ldquo;rapidly evolving,&amp;rdquo; with seemingly groundbreaking advancements happening every day. ðŸ¤”&lt;/p&gt;</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://morethan987.github.io/blog/llm-foundation-notes/featured.svg" />
    </item>
    
    <item>
      <title>Practical LLM Training</title>
      <link>https://morethan987.github.io/en/blog/llm-training-playbook/</link>
      <pubDate>Wed, 28 May 2025 00:00:00 +0000</pubDate>
      <author>morthan@qq.com (Morethan)</author>
      <guid>https://morethan987.github.io/en/blog/llm-training-playbook/</guid>
      <description>&lt;div class=&#34;lead text-neutral-500 dark:text-neutral-400 !mb-9 text-xl&#34;&gt;
  After understanding the implementation of a network architecture, a natural question arises: how do I train such a model? This section documents various engineering insights that can answer this question.
&lt;/div&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Preface
    &lt;div id=&#34;preface&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none&#34;&gt;
        &lt;a class=&#34;text-primary-300 dark:text-neutral-700 !no-underline&#34; href=&#34;#preface&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;
    
&lt;/h2&gt;
&lt;p&gt;Large model architectures come in all shapes and sizes, but to truly understand one, you need to put it into practice.&lt;/p&gt;</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://morethan987.github.io/blog/llm-training-playbook/featured.svg" />
    </item>
    
    <item>
      <title>On the Memory of LLM</title>
      <link>https://morethan987.github.io/en/blog/llm-memory/</link>
      <pubDate>Sun, 04 May 2025 00:00:00 +0000</pubDate>
      <author>morthan@qq.com (Morethan)</author>
      <guid>https://morethan987.github.io/en/blog/llm-memory/</guid>
      <description>&lt;div class=&#34;lead text-neutral-500 dark:text-neutral-400 !mb-9 text-xl&#34;&gt;
  A summary of thoughts on the memory of large language models
&lt;/div&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Introduction
    &lt;div id=&#34;introduction&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none&#34;&gt;
        &lt;a class=&#34;text-primary-300 dark:text-neutral-700 !no-underline&#34; href=&#34;#introduction&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;
    
&lt;/h2&gt;
&lt;p&gt;This reflection originates from a common issue encountered in AI-assisted programming: the need to repeatedly familiarize itself with the code repository from scratch.&lt;/p&gt;</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://morethan987.github.io/blog/llm-memory/featured.svg" />
    </item>
    
    <item>
      <title>Notes on LLM Engineering Practices</title>
      <link>https://morethan987.github.io/en/blog/llm-engineering-note/</link>
      <pubDate>Wed, 28 May 2025 00:00:00 +0000</pubDate>
      <author>morthan@qq.com (Morethan)</author>
      <guid>https://morethan987.github.io/en/blog/llm-engineering-note/</guid>
      <description>&lt;div class=&#34;lead text-neutral-500 dark:text-neutral-400 !mb-9 text-xl&#34;&gt;
  This blog post is the zeroth article in the &amp;ldquo;AI Engineering&amp;rdquo; series. Besides serving as an index for the entire series, it also summarizes key knowledge points related to practical large model applications.
&lt;/div&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Introduction
    &lt;div id=&#34;introduction&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none&#34;&gt;
        &lt;a class=&#34;text-primary-300 dark:text-neutral-700 !no-underline&#34; href=&#34;#introduction&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;
    
&lt;/h2&gt;
&lt;p&gt;Currently, large model technology is developing rapidly. Open-source base models like DeepSeek are gradually catching up with closed-source models, and general-purpose large models are achieving increasingly higher accuracy across various tasks, pushing their capability limits higher.&lt;/p&gt;</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://morethan987.github.io/blog/llm-engineering-note/featured.svg" />
    </item>
    
  </channel>
</rss>
