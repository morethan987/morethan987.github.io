


[{"content":" Notes written after reading the textbook \u0026ldquo;Foundation of Large Models\u0026rdquo; from Zhejiang University Preface # The development of large model technology can be described as \u0026ldquo;rapidly evolving,\u0026rdquo; with seemingly groundbreaking advancements happening every day. ü§î\nIn reality, how effective are these \u0026ldquo;groundbreaking advancements\u0026rdquo;? What is their actual value in engineering practice? Ultimately, these questions must be traced back to those \u0026ldquo;ordinary\u0026rdquo; classic principles.\nIt\u0026rsquo;s certainly a good thing to keep up with the latest developments in large model technology: it helps us understand research directions and broaden our thinking. However, we should not only focus on the ever-evolving top-tier technologies but also pay attention to the underlying, unchanging fundamentals behind them.\nThis is the significance of taking the time to read textbooks in an era of rapid technological advancement.\nThis article primarily focuses on the book ‚ÄúFoundation of Large Models‚Äù written by Zhejiang University, which is still being continuously updated.\nBasics of Language Models # Architecture of Large Language Models # Parameter-Efficient Fine-Tuning # Model Editing # ","date":"28 May 2025","externalUrl":null,"permalink":"/en/blog/llm-foundation-notes/","section":"Blogs","summary":"\u003cdiv class=\"lead text-neutral-500 dark:text-neutral-400 !mb-9 text-xl\"\u003e\n  Notes written after reading the textbook \u0026ldquo;Foundation of Large Models\u0026rdquo; from Zhejiang University\n\u003c/div\u003e\n\n\n\n\u003ch2 class=\"relative group\"\u003ePreface \n    \u003cdiv id=\"preface\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#preface\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h2\u003e\n\u003cp\u003eThe development of large model technology can be described as \u0026ldquo;rapidly evolving,\u0026rdquo; with seemingly groundbreaking advancements happening every day. ü§î\u003c/p\u003e","title":"Notes on the Foundation of Large Models","type":"blog"},{"content":" After understanding the implementation of a network architecture, a natural question arises: how do I train such a model? This section documents various engineering insights that can answer this question. Preface # Large model architectures come in all shapes and sizes, but to truly understand one, you need to put it into practice.\n\u0026ldquo;Practice is the sole criterion for testing truth.\u0026quot;ü´°\n","date":"28 May 2025","externalUrl":null,"permalink":"/en/blog/llm-training-playbook/","section":"Blogs","summary":"\u003cdiv class=\"lead text-neutral-500 dark:text-neutral-400 !mb-9 text-xl\"\u003e\n  After understanding the implementation of a network architecture, a natural question arises: how do I train such a model? This section documents various engineering insights that can answer this question.\n\u003c/div\u003e\n\n\n\n\u003ch2 class=\"relative group\"\u003ePreface \n    \u003cdiv id=\"preface\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#preface\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h2\u003e\n\u003cp\u003eLarge model architectures come in all shapes and sizes, but to truly understand one, you need to put it into practice.\u003c/p\u003e","title":"Practical LLM Training","type":"blog"},{"content":" A novel brain-inspired architecture that introduces the concept of \u0026ldquo;temporality\u0026rdquo; to existing time-agnostic neuron models, giving rise to a series of brain-like emergent phenomena. Preface # This paper caught my attention precisely because of the \u0026ldquo;magical\u0026rdquo; emergent phenomena it claims‚Äîintroducing temporal information into neuron models results in human-like thought patterns of attention. These phenomena are not manually designed but naturally emerge during training, which is the most exciting part.\nThe most intuitive example is the official \u0026ldquo;maze model.\u0026rdquo; From the visualized results, we can directly observe the model\u0026rsquo;s attention focus moving through the maze, much like how humans solve maze problems. Additionally, the official team provided an interactive demo page: Demo, where you can directly command the model to solve mazes and observe its attention focus in real time.\nMoreover, the official code repository includes detailed documentation and code comments, with clean and intuitive file organization‚Äîa kind of \u0026ldquo;artistic sense\u0026rdquo; only programmers would appreciate.\nAfter wading through mountains of spaghetti code, one might truly appreciate the \u0026ldquo;elegance\u0026rdquo; of well-structured code üòá and deeply understand the effort behind this \u0026ldquo;elegance.\u0026rdquo; Introduction # Neural networks were initially inspired by biological brains but have diverged significantly. Biological brains exhibit complex neurodynamic processes that evolve over time, while modern neural networks, for the sake of large-scale deep learning, have largely discarded \u0026ldquo;temporal\u0026rdquo; features.\nThe official paper provides a clear rationale for this research, so I won\u0026rsquo;t elaborate further:\n\u0026ldquo;Why conduct this research? Admittedly, modern AI\u0026rsquo;s outstanding performance in many practical domains might suggest that simulating neurodynamics is unnecessary or that explicitly considering the temporal dimension of intelligence is counterproductive. However, human intelligence exhibits high flexibility, data efficiency, and exceptional generalization to unseen scenarios, existing in an open world where learning and adaptation are tightly bound to the arrow of time. Thus, human intelligence naturally possesses commonsense reasoning, ontological inference, transparency/interpretability, and strong generalization‚Äîqualities not convincingly demonstrated in current AI.\u0026rdquo;\nThe core technical contributions of this paper are as follows:\nDecoupled internal temporal dimension Neuron-level models (NLMs) Neural activity synchronization Of course, without delving into the code implementation, these terms are meaningless. But the author\u0026rsquo;s perspective can still be understood:\nReasoning models and loops: The author critiques current reasoning models, pointing out that the \u0026ldquo;continue scaling current model architectures\u0026rdquo; approach has been questioned by many studies. While loop mechanisms can indeed improve performance, the author argues that loop mechanisms are important, but the precise timing and interactions between neuron activities unlocked by loops are equally critical. Interesting side effects: CTM\u0026rsquo;s internal loops resemble human thinking. Without any explicit supervision, it automatically allocates appropriate computational resources to tasks of varying difficulty‚Äîsimple tasks terminate early, while complex tasks undergo deeper computation. Information can be encoded in temporal dynamics, granting the network stronger information compression capabilities. My thoughts on the third point: According to the \u0026ldquo;compression is intelligence\u0026rdquo; view, encoding information into temporal dynamics would greatly enhance the network\u0026rsquo;s information \u0026ldquo;compression rate,\u0026rdquo; thereby improving \u0026ldquo;intelligence\u0026rdquo; to some extent. Finally, the author clarifies the goal of this research:\n\u0026ldquo;By explicitly modeling neural timing mechanisms in CTM, we aim to pioneer new pathways for developing more biologically plausible and high-performance AI systems.\u0026rdquo;\nMethodology # Overview of the CTM architecture: ‚ë† The synaptic model (weights represented by blue lines) generates pre-activation values by simulating interactions between neurons. Each neuron retains ‚ë° a pre-activation history, where the latest data is used by ‚ë¢ the neuron-level model (weights represented by red lines) to produce ‚ë£ post-activation values. The system also maintains ‚ë§ a post-activation history, from which ‚ë• a synchronization matrix is computed. Based on this matrix, ‚ë¶ neuron pairs are selected, and the resulting ‚ëß latent representations are used by CTM to ‚ë® generate outputs and modulate data via cross-attention. The modulated data (e.g., attention outputs) are ‚ë© concatenated with post-activation values as input for the next internal clock step.\nIf the goal were merely to introduce a temporal dimension, traditional RNN architectures could achieve it. CTM\u0026rsquo;s innovations lie in:\nReplacing traditional activation functions with neuron-level models. Using neural synchronization as latent representations to modulate data and generate outputs. Continuous Thought: Internal Sequence Dimension # The paper defines an internal temporal dimension:\n$$ t\\in \\{1,\\dots,T\\} $$The superscript in the diagram refers to a specific time step, and each time step involves a complete computation cycle (from ‚ë† to ‚ë©).\nThis internal dimension is not a new concept‚ÄîRNNs and Transformers also use it. However, traditional architectures process data sequentially, implicitly tying the internal temporal dimension to the input data order.\nFor example, RNNs also have an internal temporal dimension, but at each time step, the model receives a new token input, generates the next token based on the internal hidden state, and updates the hidden state. If we only look at token inputs, we essentially default to using the data\u0026rsquo;s inherent order as the model\u0026rsquo;s internal temporal dimension.\nTransformers are inherently \u0026ldquo;orderless\u0026rdquo;: At each time step, the attention mechanism processes all tokens in parallel. Perhaps this is the source of their power ü§î. The only step related to input data order is \u0026ldquo;positional embedding.\u0026rdquo; CTM completely decouples this association, making internal processing independent of input data. It\u0026rsquo;s not just order-agnostic but also independent of input sequence length, which might be the point the author emphasizes more ü§î.\nThis \u0026ldquo;decoupling\u0026rdquo; allows the model\u0026rsquo;s internal \u0026ldquo;thinking\u0026rdquo; to have arbitrary length, iteratively construct and refine internal representations, and extend to \u0026ldquo;non-sequential\u0026rdquo; tasks like solving mazes.\nCurrent CoT (Chain-of-Thought) techniques can be seen as attempting \u0026ldquo;decoupling\u0026rdquo; work: The input data is already there, and directly generating answers doesn\u0026rsquo;t work well, so intermediate tokens‚Äîthe so-called thought process‚Äîare produced with lengths unaffected by the data sequence. But when models overthink simple problems, Anthropic leaves the question of \u0026ldquo;how much thinking is appropriate\u0026rdquo; to the model user, which is a pragmatic engineering solution. CTM, however, seems poised to solve this problem üòÉ by decoupling the data sequence from the internal temporal dimension at the architectural level ü§î.\nFor a deeper understanding of current CoT techniques, check out Why we think - Lil\u0026rsquo;log, which offers a more academic yet accessible explanation.\nRecurrent Weights: Synaptic Model # This section explains the synaptic model, corresponding to step ‚ë† in the diagram. Formally, the synaptic model performs the following computation:\n$$ a^t=f_{\\theta_{syn}}(concat(z^t,o^t))\\in R^D $$Here, \\(z^t\\in R^D\\) represents the post-activation vector at time step \\(t\\), and \\(o^t\\) is the modulated data computed in the previous time step. These are concatenated and fed into the synaptic model to produce \\(a^t\\), the pre-activation vector.\nNote the lowercase \\(z\\) here versus the uppercase \\(Z\\) in the diagram: \\(z\\) refers to the post-activation vector for a single neuron, while \\(Z\\) is a matrix of post-activation vectors for all neurons. The synaptic model is essentially a function \\(f_{\\theta_{syn}}\\), which can be expressed in various ways. Experiments in the paper use an MLP, specifically a U-NET-esque MLP.\nThe most recent \\(M\\) pre-activation vectors are stored in a matrix \\(A^t\\):\n$$ A^t=[a^{t-M+1}\\quad a^{t-M+1}\\dots a^{t}]\\in R^{D\\times M} $$The first \\(M\\) elements in the history sequence and the initial \\(z\\) values at \\(t=1\\) require initialization. Experiments show that making them learnable parameters yields the best results.\nNote that \\(z^t\\) is not the input data at time step \\(t\\)! As shown in the diagram, the entire computation flow hardly involves input data‚Äîonly when computing \\(o^t\\) (the orange block) does external input data participate ü§î. Parameter-Private Neuron-Level Models # The synaptic model explains how a single neuron (synapse) processes input and generates a history matrix. Now, assume there are \\(D\\) identical neurons, each with its own \\(A^t\\). We denote the history vector of neuron \\(d\\) as \\(A^t_{d}\\).\nEach neuron then performs the following computation:\n$$ z^{t+1}_{d}=g_{\\theta_{d}}(A^t_{d}) $$Here, \\(\\theta_{d}\\) represents the private computational parameters of neuron \\(d\\); \\(z^{t+1}_{d}\\) is the post-activation value of neuron \\(d\\) at the next time step, meaning:\n$$ z^t=[z^{t+1}_{1} \\quad z^{t+1}_{2}\\dots z^{t+1}_{D}] $$\\(g_{\\theta_{d}}\\) is a single-hidden-layer MLP that takes an \\(M\\)-dimensional vector \\(A^t_{d}\\) and outputs \\(z^{t+1}_{d}\\in R^D\\).\nIn plain terms, this step means: Each neuron generates its next post-activation value based on its last \\(M\\) pre-activation values.\nüò¢ I initially thought \\(z_{d}^t\\) was a vector, but it\u0026rsquo;s actually a scalar. Neuron Synchronization: Input and Output Modulation # This step ensures that data-model interactions no longer rely on a single moment\u0026rsquo;s model state but on continuous, dynamic neural activity. Specifically, post-activation vectors are collected into a matrix:\n$$ Z^t = [z^1 \\quad z^2 \\dots z^t]\\in R^{D\\times t} $$Neuron synchronization is then described as the inner product of \\(Z^t\\):\n$$ S^t=Z^t \\cdot (Z^t)^T\\in R^{D\\times D} $$ Mathematically, \\(S^t\\) is an unnormalized covariance matrix where the \\((i,j)\\) element represents the \u0026ldquo;degree of coordination\u0026rdquo; between the \\(i\\)-th neuron and the \\(j\\)-th neuron across all time steps, which is what the paper refers to as \u0026ldquo;neural synchronization.\u0026rdquo; This neuron synchronization matrix undergoes downsampling: Randomly select \\(D_{out}\\) and \\(D_{action}\\) elements to form two neuron synchronization representation vectors, \\(S^t_{out}\\in R^{D_{out}}\\) and \\(S^t_{action}\\in R^{D_{action}}\\).\n\\(S^t_{out}\\) is projected into the output space:\n$$ \\mathbf{y}^t = W_{out}\\cdot S^t_{out} $$\\(S^t_{action}\\) is used to generate actions in the world (respecting the original paper\u0026rsquo;s phrasing, though it\u0026rsquo;s a bit confusing ü§î):\n$$ q^t = W_{in}\\cdot S^t_{action} $$Here, \\(W_{in}\\) and \\(W_{out}\\) are learnable matrix parameters.\nThe computed \\(q^t\\) undergoes additional computation to produce \\(o^t\\in R^{d_{input}}\\), which is concatenated with \\(z^t\\) in the first step. In the author\u0026rsquo;s experiments, this additional computation is typically an attention layer:\n$$ o^t=Attention(Q=q^t, KV=FeatureExtractor(data)) $$Here, \\(FeatureExtractor(\\cdot)\\) is another neural network model, such as ResNet.\nSince \\(S^t\\) aggregates information from all time steps, later steps may have a greater impact. To enhance model flexibility, a learnable decay coefficient is introduced:\n$$ \\mathbf{R}_{i j}^{t}=\\left[\\begin{array}{llll} \\exp \\left(-r_{i j}(t-1)\\right) \u0026 \\exp \\left(-r_{i j}(t-2)\\right) \u0026 \\cdots \u0026 \\exp (0) \\end{array}\\right]^{\\top} \\in \\mathbb{R}^{t} $$This coefficient scales the original neuron synchronization matrix elements:\n$$ \\mathbf{S}_{i j}^{t}=\\frac{\\left(\\mathbf{Z}_{i}^{t}\\right)^{\\top} \\cdot \\operatorname{diag}\\left(\\mathbf{R}_{i j}^{t}\\right) \\cdot\\left(\\mathbf{Z}_{j}^{t}\\right)}{\\sqrt{\\sum_{\\tau=1}^{t}\\left[\\mathbf{R}_{i j}^{t}\\right]_{\\tau}}} $$ The author\u0026rsquo;s experiments show that these learnable decay coefficients are rarely used: In ImageNet classification, only 3 out of 8196 coefficients are effective; in maze-solving, it\u0026rsquo;s slightly higher but still around 3%. Loss Function: Optimization Across Time Steps # Using a classification task as an example, the loss at a given time step is:\n$$ \\mathcal{L}^{t}=\\operatorname{CrossEntropy}\\left(\\mathbf{y}^{t}, y_{\\text {true}}\\right) $$The author also defines a confidence vector \\(C^t\\):\n$$ C^t=1-\\frac{\\mathcal{L}^{t}}{\\sqrt{ (\\mathcal{L}^{t})^T \\cdot \\mathcal{L}^{t} }} $$Collecting these losses across all time steps yields two matrices: \\(\\mathcal{L}\\in R^T\\) and \\(C\\in R^T\\).\nA natural question arises: How should \\(\\mathcal{L}\\) be reduced to a scalar loss function for learning? This paper\u0026rsquo;s loss function aims to optimize CTM\u0026rsquo;s performance along the internal temporal dimension. Instead of forcing the model to output results at a specific time step (typically the last), the author adopts a dynamic aggregation strategy, integrating information from two internal moments: the point of minimum loss and the point of maximum confidence. This approach offers the following advantages:\nEncourages CTM to build meaningful representations and computations across multiple internal moments. Naturally forms a curriculum learning effect, where the model first uses later steps for complex computations and gradually transitions to earlier steps for simpler tasks. Enables CTM to adaptively adjust computational intensity based on the inherent difficulty of each data point. The specific operations are as follows:\n$$ \\begin{matrix} t_{1}=argmin(\\mathcal{L})\\\\ t_{2}=argmax(C)\\\\ L=\\frac{\\mathcal{L}^{t_{1}} + \\mathcal{L}^{t_{2}}}{2} \\end{matrix} $$Stochastic gradient descent is then used to optimize the model parameters. This concludes the algorithm.\nThe author reiterates the superiority of this architecture:\n\u0026ldquo;Introducing temporality as a fundamental functional element of CTM has many beneficial properties. Most critically, we can train CTM without restricting the number of internal clock cycles used. This seemingly minor freedom is profound‚Äîit allows CTM to allocate variable computational resources to different data points. This adaptive/dynamic computation aligns with modern test-time scaling paradigms, with the key difference being that this highly regarded modeling feature is a natural derivative of CTM, not an afterthought or constraint imposed during learning.\u0026rdquo;\nSummary # At this point, the paper\u0026rsquo;s core ideas are clear. However, some algorithmic details require further contemplation ü§î.\nInternal Temporal Dimension # The paper seems to use two different terms for the \u0026ldquo;internal temporal dimension\u0026rdquo; concept: In the conceptual introduction (Section 1), it\u0026rsquo;s called the \u0026ldquo;internal temporal dimension,\u0026rdquo; while in the loss function section (final section), it\u0026rsquo;s referred to as the \u0026ldquo;internal thought dimension\u0026rdquo; ü§î.\nThis suggests that the \u0026ldquo;internal temporal dimension\u0026rdquo; here differs from RNN time steps, despite their similarities. But how exactly? ü§î\nIn the section introducing the \u0026ldquo;internal temporal dimension,\u0026rdquo; the paper emphasizes CTM\u0026rsquo;s distinction from traditional models:\n\u0026ldquo;Unlike traditional sequence models (e.g., RNNs or Transformers)‚Äîwhich process inputs sequentially (e.g., words in a sentence or frames in a video)‚ÄîCTM operates along an autonomously generated internal thought timeline.\u0026rdquo;\nCombining the interactive webpage \u0026rsquo;s detailed videos and text, I realized something astonishing, as noted at the end of the Recurrent Weights Synaptic Model section:\n\\(z^t\\) is not the input data at time step \\(t\\)! As shown in the diagram, the entire computation flow hardly involves input data‚Äîonly when computing \\(o^t\\) (the orange block) does external input data participate.\nThis is indeed a surprising design. I initially misinterpreted \\(z^t\\) as input data üò¢. With the correct interpretation, the separation of input data and internal temporal dimension becomes understandable: The cyclic generation of internal activation vectors is independent of the data sequence.\nA simple example: If the input data is a natural language text, CTM can generate many internal thought steps while the input data remains the same text. Of course, you could also \u0026ldquo;generate\u0026rdquo; new characters, much like a Transformer.\nThis means that for any input‚Äîsequential or non-sequential‚Äîthe model can freely unfold arbitrary-length internal thinking. Compared to CoT techniques, this design offers greater expressiveness: CoT expresses thoughts in language, while CTM\u0026rsquo;s thought process is encoded in internal parameters.\nSynaptic Model and Neuron-Level Model # The above diagram vividly illustrates the operations performed by the synaptic model part and the neuron-level model part: the former enables information interaction between different neurons at the same moment, while the latter facilitates the interaction of historical information across different moments for the same neuron. As shown in the figure, a vertical information fusion is immediately followed by a horizontal information fusion ü§î\nHow does the model interact with external data? It is achieved by downsampling the neuronal synchronization matrix and then applying a linear projection. This sounds very abstract, but from another perspective, the logic becomes clear: if we imagine CTM as a brain, then sampling the neural synchronization matrix is akin to extracting a portion of information from it, which is then mapped through a matrix to drive behavior.\nA very reasonable idea is: would replacing this matrix with an MLP network yield better results? ü§î Does the downsampling strategy impact the capabilities of the CTM model? Could some form of attention mechanism be used to construct an \u0026ldquo;intelligent\u0026rdquo; downsampling strategy?\nü§î I‚Äôm actually quite curious: if the number of neurons \\(D\\) is scaled up to the magnitude of neurons in the human brain, and an attention mechanism is employed, would some form of sparse activation features naturally emerge? Code Analysis # I don\u0026rsquo;t have time to study the code carefully now üò¢ Finals are coming, I have to go review.\n","date":"14 May 2025","externalUrl":null,"permalink":"/en/blog/ctm-note/","section":"Blogs","summary":"\u003cp\u003e\n\n\u003cdiv class=\"lead text-neutral-500 dark:text-neutral-400 !mb-9 text-xl\"\u003e\n  A novel brain-inspired architecture that introduces the concept of \u0026ldquo;temporality\u0026rdquo; to existing time-agnostic neuron models, giving rise to a series of brain-like emergent phenomena.\n\u003c/div\u003e\n\u003c/p\u003e\n\n\n\u003ch2 class=\"relative group\"\u003ePreface \n    \u003cdiv id=\"preface\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#preface\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h2\u003e\n\u003cp\u003eThis paper caught my attention precisely because of the \u0026ldquo;magical\u0026rdquo; emergent phenomena it claims‚Äîintroducing temporal information into neuron models results in \u003cstrong\u003ehuman-like thought patterns\u003c/strong\u003e of attention. These phenomena are not manually designed but naturally emerge during training, which is the most exciting part.\u003c/p\u003e","title":"CTM Reading Notes","type":"blog"},{"content":" A summary of thoughts on the memory of large language models Introduction # This reflection originates from a common issue encountered in AI-assisted programming: the need to repeatedly familiarize itself with the code repository from scratch.\nCurrent AI-assisted programming tools provide a wealth of foundational information to help AI quickly understand a code project. They use various functions to read project code, directory structures, and edit file contents. During each interaction, the AI model goes through a \u0026ldquo;from-scratch\u0026rdquo; familiarization process with the project: when you instruct it to solve a specific problem, the AI explores your code repository step by step, understands the relevant details, and makes the corresponding changes. This seems reasonable enough.\nHowever, when you start a new round of conversation, the AI completely forgets what changes it made previously and also forgets the details of your code repository that are worth \u0026ldquo;remembering.\u0026rdquo; It then begins a new round of exploration and familiarization with the code repository.\nThe obvious issues are as follows:\nWaste of computational resources: Repeatedly exploring the repository without utilizing the useful information obtained from the historical exploration process. Difficulty in handling complex projects: The need to start from scratch each time leads to an incomplete understanding of the repository, resulting in incoherent code. But this issue is not unique to AI-assisted programming. The \u0026ldquo;seven-second memory\u0026rdquo; characteristic of large language models (LLMs) limits their application in multiple commercial fields:\nCode development: As mentioned, it cannot form a coherent understanding of a project. Intelligent education: It struggles to provide coherent guidance to students. Emotional companionship: It fails to form a coherent user profile. In summary, the problem boils down to one thing: Current large language models cannot achieve a coherent, accurate, and dynamically adjustable understanding of complex matters.\nIn contrast, human performance is quite different: A company employee may not initially understand the core logic and important details of a project, but over time, they become more and more familiar with the project code and can even apply their unique understanding to code development; a human teacher can gradually explore a student\u0026rsquo;s characteristics based on their own experience and tailor their teaching accordingly; your good friend will not forget the travel experiences you shared yesterday, nor will they forget your personality traits.\nThis is also an important reason why current large language model technologies are difficult to implement: Although large language models have consumed all the text information on the internet and are even expanding into multimodal information such as images and videos, there is no effective \u0026ldquo;experience accumulation\u0026rdquo; pathway for specific engineering projects that cannot be fully conveyed through text (such as project concepts and technology stack selection). No matter how powerful the large language model is, it cannot effectively handle these tasks.\nCurrent Status Analysis # First, we provide a general analysis of the current focus of large language model evaluations, highlighting that current evaluations emphasize isolated tasks while neglecting coherent tasks. We then analyze the typical interaction patterns, demonstrating that current interaction modes lack effective and coherent experience accumulation.\nLarge Language Model Evaluation # There are a multitude of benchmark tests for large language models. Since Qwen 3 recently released its benchmark scores, we will use the benchmark scores from Qwen 3 as an example here.\nArenaHard # Official GitHub link: ArenaHard; Online demo: Arena-Hard-Auto Benchmark Viewer\nArenaHard is a benchmark test based on model versus model scoring. The test set is derived from 200,000 real user queries collected from the Chatbot Arena, from which 500 high-quality prompts are selected as the test set.\nEach test in the test set is sent to the model under test to obtain the corresponding response. The responses of the baseline model (GPT-4-0314) have been pre-generated by the official team. Another large model is then used as a referee to evaluate the responses of these two models. Finally, the model\u0026rsquo;s win-loss record is collected and further processed to calculate a score out of 100. This score represents \u0026ldquo;the probability that the referee model expects you to defeat GPT-4-0314 on 500 high-difficulty instructions.\u0026rdquo;\nThe benchmark test has released version 2.0, which is significantly more challenging than before. As of May 3, 2025, the latest benchmark results for the high-difficulty prompts and style control tests are as follows:\nModel Scores (%) CI (%) 0 o3-2025-04-16 85.9 (-0.8 / +0.9) 1 o4-mini-2025-04-16-high 79.1 (-1.4 / +1.2) 2 gemini-2.5 79.0 (-2.1 / +1.8) 3 o4-mini-2025-04-16 74.6 (-1.8 / +1.6) 4 gemini-2.5-flash 68.6 (-1.6 / +1.6) 5 o3-mini-2025-01-31-high 66.1 (-1.5 / +2.1) 6 o1-2024-12-17-high 61.0 (-2.0 / +2.1) 7 claude-3-7-sonnet-20250219-thinking-16k 59.8 (-2.0 / +1.8) 8 Qwen3-235B-A22B 58.4 (-1.9 / +2.1) 9 deepseek-r1 58.0 (-2.2 / +2.0) 10 o1-2024-12-17 55.9 (-2.2 / +1.8) 11 gpt-4.5-preview 50.0 (-1.9 / +2.0) 12 o3-mini-2025-01-31 50.0 (-0.0 / +0.0) 13 gpt-4.1 50.0 (-1.9 / +1.7) 14 gpt-4.1-mini 46.9 (-2.4 / +2.1) 15 Qwen3-32B 44.5 (-2.2 / +2.1) 16 QwQ-32B 43.5 (-2.5 / +2.1) 17 Qwen3-30B-A3B 33.9 (-1.6 / +1.5) 18 claude-3-5-sonnet-20241022 33.0 (-2.3 / +1.8) 19 s1.1-32B 22.3 (-1.7 / +1.5) 20 llama4-maverick-instruct-basic 17.2 (-1.5 / +1.2) 21 Athene-V2-Chat 16.4 (-1.4 / +1.4) 22 gemma-3-27b-it 15.0 (-1.4 / +1.0) 23 Qwen3-4B 15.0 (-1.1 / +1.5) 24 gpt-4.1-nano 13.7 (-1.1 / +1.0) 25 Llama-3.1-Nemotron-70B-Instruct-HF 10.3 (-0.8 / +1.0) 26 Qwen2.5-72B-Instruct 10.1 (-0.9 / +1.3) 27 OpenThinker2-32B 3.2 (-0.3 / +0.3) From the above introduction, it is clear that ArenaHard only tests the model\u0026rsquo;s ability to solve isolated problems.\nAIME # AIME, the American Invitational Mathematics Examination, is not strictly a benchmark test for large language models, but it does reflect a model\u0026rsquo;s mathematical reasoning ability.\nThe competition consists of 15 questions, all of which have numerical answers, with no points for steps. The final score is simply the accuracy rate, which may be averaged or take the highest value over multiple tests, but there is no unified standard.\nClearly, AIME also only tests the large language model\u0026rsquo;s ability to solve isolated problems in one go.\nTypical Interaction Patterns # Do large language models have memory? Yes, but only short-term memory. The current interaction between humans and large language models can be summarized as follows:\nHumans provide background information for the task. The large language model attempts to complete the task based on the background information. The model continuously interacts with humans during task execution (short-term memory). The model completes the task. The so-called \u0026ldquo;short-term\u0026rdquo; memory is reflected in its \u0026ldquo;short\u0026rdquo; duration: no longer than the length of the large language model\u0026rsquo;s context window. The specific implementation method is to simply put all the interaction history between the user and the model into the window.\nAlthough the context window of existing large language models is getting longer and longer, and can even reach 2 million tokens (equivalent to 1.5 copies of Dream of the Red Chamber), this does not mean that we can put all interaction information into the window and expect the model to generate content at a low cost and with high accuracy.üò¢ Summary # The current focus of evaluation and interaction patterns are very suitable for isolated tasks. However, in reality, the tasks we need to complete are all \u0026ldquo;subtasks\u0026rdquo; under a larger goal framework, and these \u0026ldquo;subtasks\u0026rdquo; are interrelated and mutually restrictive.\nAn interesting phenomenon is that after we try to solve one of the subtasks, solving the other tasks becomes easier. The reason is that some of the exploration results obtained in the process of solving other subtasks are beneficial to solving other subtasks because these subtasks all belong to the same goal.\nTherefore, the direction for improvement is now clear: introduce an online learning long-term memory module for large language models, enabling them to accumulate experience from historical exploration.\nRelated Work # Titans # The Titans architecture is Google\u0026rsquo;s latest improvement on the Transformer architecture, proposing the concept of \u0026ldquo;Test-time Memory.\u0026rdquo; It aims to expand the model\u0026rsquo;s context length while providing a more accurate attention mechanism. It introduces a parameter network that can be dynamically updated at test time to store long-term memory information and enhance the performance of the attention mechanism.\nKBLaM # KBLaM is an official implementation of \u0026ldquo;KBLaM: Knowledge Base augmented Language Model.\u0026rdquo;\nTTRL # The TTRL algorithm can update the model\u0026rsquo;s weight parameters using reinforcement learning at test time, achieving real-time dynamic updates of model parameters.\nrStar # rStar uses Monte Carlo Tree Search (MCTS) to explore high-quality reasoning paths. Unlike model distillation methods that require a strong model as guidance, this framework borrows the idea of peer-to-peer verification learning, using a voting mechanism to obtain a simulated \u0026ldquo;correct answer\u0026rdquo; to guide the generation of high-quality paths.\nTokenformer # The Tokenformer framework embraces the philosophy of \u0026ldquo;everything can be tokenized.\u0026rdquo;\nSD-LoRA # SD-LoRA\nDifferential Transformer # The Differential Transformer improves the attention mechanism in traditional Transformer architectures by introducing the concept of \u0026ldquo;differential attention.\u0026rdquo; Unlike conventional softmax attention, the Diff Transformer divides the query and key vectors into two separate groups, computes two independent softmax attention maps, and then eliminates noise through a subtraction operation. This mechanism is analogous to the differential amplifier in electronic engineering, which cancels out common-mode noise by taking the difference between two signals.\nIdeas and Thoughts # Memory in Large Language Models # When we run a large language model offline and ask it a question: \u0026ldquo;Who proposed the theory of relativity?\u0026rdquo; It can tell us: \u0026ldquo;The theory of relativity was proposed by Einstein.\u0026rdquo; This actually shows that the model\u0026rsquo;s parameters already store relevant factual information, which is the so-called \u0026ldquo;memory.\u0026rdquo;\nThis section mainly answers: Where is the memory of Transformer stored?\nDeconstructing Transformer # Research on the interpretability of Transformer has been ongoing. This article interprets the structure of Transformer based on DeepMind\u0026rsquo;s related research and Anthropic\u0026rsquo;s related research.\nIn summary: The residual connections throughout the process are regarded as a \u0026ldquo;data bus,\u0026rdquo; running through the entire model\u0026rsquo;s computation process; the Attention Layer and MLP Layer are seen as the read/write heads for information, reading and writing data on the data bus.\nEmbedding # The embedding process is the first step in the Transformer processing pipeline:\nTokenization: Tokenize the text. Embedding: Convert tokens into vectors and add positional encoding. Intuitively, this step turns a sentence into a cluster of vectors. The embedding matrix is used to convert tokens into reasonable numerical vectors. This means that the embedding matrix stores some factual information, including relationships between different entities and differences between tokens. However, this information is based on the average of a large amount of text and cannot represent dynamic semantics.\nFor example, the word \u0026ldquo;bank\u0026rdquo; can mean both \u0026ldquo;riverbank\u0026rdquo; and \u0026ldquo;financial institution,\u0026rdquo; but the embedding matrix describes a kind of \u0026ldquo;average\u0026rdquo; of these two meanings.\nMulti-head Attention # As mentioned above, the embedding matrix alone cannot describe dynamic semantics based on context. Therefore, the multi-head attention mechanism was introduced to construct more precise word vectors.\nA vivid description is: After embedding, a cluster of vectors is generated, and these vectors influence each other. The parameters in the multi-head attention layer model this complex mutual influence. According to the circuit interpretation framework, the Attention Layer calculates the \u0026ldquo;increment of mutual influence\u0026rdquo; between this cluster of vectors and writes this increment back into the \u0026ldquo;data bus.\u0026rdquo;\nMLP # Research by DeepMind has shown that factual information is mostly stored in the MLP layer. Here, the operations in the MLP layer are broken down into two steps:\nFact matching Calculating the facts to be injected The vectors processed by the multi-head attention have been shifted to the correct positions in the current context, and these vectors already contain a large amount of complex information superimposed together (a large number of explicit semantics are additively fused into a single vector). The role of the embedding matrix here is similar to a checklist: each row is a check item (the specific meaning of these items is also highly superimposed), and the check items and word vectors are matched through dot products to obtain \u0026ldquo;matching scores,\u0026rdquo; which are then filtered out by the nonlinear truncation function (ReLU) to remove unrelated matches. This step is the \u0026ldquo;fact matching\u0026rdquo; process.\nFor example, when using an extended matrix to detect the token \u0026ldquo;relativity,\u0026rdquo; a certain row might simultaneously check: \u0026ldquo;whether it is an academic concept + whether it is a ball sport + whether it is the name of a galaxy.\u0026rdquo; The detection results could be relevant or irrelevant, but they will all be linearly combined in the end.\nWhy can they be combined? Because of the distributive property of vector dot products: \u0026ldquo;combine first then match\u0026rdquo; is equivalent to \u0026ldquo;match first then combine.\u0026rdquo; The projection matrix then acts as the \u0026ldquo;calculation of fact increment\u0026rdquo;: based on the matching scores, it calculates which factual information needs to be added to the original word vectors, adds all the necessary factual information together to form a fact increment vector.\nThe MLP layer then writes the \u0026ldquo;fact increment vector\u0026rdquo; back into the residual flow.\nStacking # Transformer stacks the above structure repeatedly, and the information extracted by deeper layers becomes more complex and harder to understand.\nIt should be noted that the injection of factual information does not occur in a single isolated MLP layer, nor does it happen solely within one Transformer Block. Instead, it iterates continuously throughout the entire network, gradually producing a clearer and more pronounced impact.\nThis complex mechanism of factual information injection poses significant challenges to interpretability efforts and makes it difficult to precisely control the flow of information within large models. However, this process precisely highlights the advantage of storing factual information in neural networks: for a given piece of complex information, it initially associates with a broad and ambiguous set of related information. Then, through computations in each layer of the network, it progressively refines and eventually converges to the most highly relevant information.\nThis is also one of the reasons why Attention Layers and MLP Layers alternate: after factual information is injected in the MLP Layer, the Attention Layer is immediately used to trim it‚Äîweakening weaker associations and amplifying stronger ones‚Äîpreventing a flood of redundant information from propagating to the next MLP Layer, which could trigger even more chaotic associations. Time Encoding # The Transformer inherently lacks awareness of the order between tokens‚Äîthe actual representation of positional sequence relies on \u0026ldquo;positional embeddings.\u0026rdquo; To more efficiently embed positional information into tokens, various methods have emerged:\nSinusoidal Embedding: Introduced in the original Transformer paper, this is an absolute positional encoding that directly adds positional encoding (PE) information to the original token embeddings. Rotary Position Embedding (RoPE): This method modifies the Q and K matrices by splitting each dimension into pairs of even dimensions and applying a rotation in the complex plane. Learnable Positional Embedding: Similar to sinusoidal embedding, this is also an absolute positional encoding, but it is trained directly via backpropagation. Similarly, to adapt to streaming data input, we should also encode the time at which information is acquired: data from the internet a decade ago should differ from current information. In this process, large models can implicitly learn the \u0026ldquo;sequence of information\u0026rdquo; just as they learn \u0026ldquo;token order,\u0026rdquo; allowing them to grasp the concept of \u0026ldquo;time\u0026rdquo; in a latent manner.\nAn effective time encoding should have the following characteristics:\nThe greater the time span, the larger the difference should be. It should preserve the periodicity of time. Fact Injection # The above text has described how factual information participates in token generation as model parameters. But how is this information stored in the parameters? Simply put, it is by using backpropagation to adjust the parameters. However, if we continue to ask:\nWhat kind of training samples are needed to store a particular factual piece of information? How many training samples are required? How does the stored information \u0026ldquo;accommodate\u0026rdquo; new information? Is the process of storing factual information inseparable from the entire network training process? If we knew the answers to the above questions, we could build a framework that continuously absorbs factual information: simply keep inputting training samples in a specified format and execute backpropagation in real time.\nExisting Approaches # This section mainly answers: What are the current methods to achieve \u0026ldquo;long-term memory\u0026rdquo;? What are their respective advantages and disadvantages?\nRAG # RAG was originally designed to alleviate the hallucination problem in large language models, but its essence is to use an external information processing module (vector database) to directly incorporate useful natural language information into the prompt.\nFollowing this line of thinking, not only can it alleviate the hallucination problem in large language models, but it can also enable the model to perform specific tasks directly (prompt engineering). Many current AI applications are implemented in this way:\nCursor: Automatically adds background information about the code repository and a variety of tools, then relies on the model\u0026rsquo;s own capabilities to complete complex code writing tasks. Manus: Provides more complex information processing tools, using the large language model\u0026rsquo;s ultra-long context to record all collected information to complete tasks. MCP: Although it is just a tool calling protocol, its underlying layer is still prompt engineering. There have been many promising results in using RAG to achieve long-term memory:\nmem0: A large language model memory system based on a vector database, allowing the model to remember user preferences from conversations. graphiti: A memory system based on a vectorized graph database, building a real-time knowledge graph for large language models, capable of storing a large amount of past factual information and accurately extracting relevant memories. memobase: A memory system based on file storage combined with a vector database, writing user preferences and other information into user profile files and continuously updating them. The advantages of this approach are very clear:\nSimple and convenient: Humans are very familiar with natural language, so using natural language to drive program execution is relatively easy to understand. Controllable information: By explicitly programming to integrate information into a long but logically coherent prompt, it is easy to control complex information. Application of large language model technology: The large language model itself is a top-tier natural language processing technology, capable of participating in various aspects of background information organization and enhancing the ability to handle complex information. The disadvantages of using RAG are also present:\nContext window limitation: The attention layer of the large language model can only support a limited context, and too long a context will be automatically truncated. Attention dispersion: Filling too much information into the context window will degrade the performance of the model\u0026rsquo;s attention mechanism, making it difficult to capture truly important information. Limited expressive ability: Complex information is difficult to express in natural language, and even if it is forcibly expressed, key information will be lost, leading to understanding ÂÅèÂ∑Æ by the large language model. For example, the development philosophy of a complex software project, if not experienced in practice, will become empty words when described in natural language. Difficult memory maintenance: Even with the support of large language model technology, updating, maintaining, and extracting memories remains a tricky problem, and it is difficult to balance response speed, accuracy, and cost. Fine-Tuning # I need to look into papers on fine-tuning, especially those on \u0026ldquo;efficient fine-tuning\u0026rdquo;, to see if they can enhance online learning capabilities. ü§î If possible, I should also check out research on incremental learning.\nBut for now, I\u0026rsquo;ll have to put this on hold\u0026hellip; üò¢ Gotta focus on reviewing for finals.\nNetwork Parameters # There is currently very little work in this area. Although OpenAI has recently launched a memory function, it has not revealed the specific implementation method, and there is not much discussion online. Here, we mainly summarize the implementation method of the Titans architecture.\nEarly Ideas # This section contains some very early ideas.\nConsider the question: Where does the capability of large language models come from? It comes from the vast amount of internet text.\nLarge language models are trained on a vast amount of text, using backpropagation to compress the mutual influence of text and factual information into the parameters of the neural network. While modeling language, they also complete the modeling of the abstract concepts represented by language. Of course, this process is very slow and requires a lot of computational power.\nAfter completing such a large amount of knowledge compression, how to handle new information? This is a problem that still needs to be solved, but it is not an unsolvable problem: Human knowledge is accumulated from little to much, and the brain must process and store continuously input new information and integrate new information with existing information.\nReferences # Arena-Hard: An Open-Source High-Quality Large Language Model Evaluation Benchmark Titans KBLaM TTRL rStar Tokenformer SD-LoRA Differential Transformer A Mathematical Framework for Transformer Circuits An Intuitive Explanation of the Attention Mechanism, the Core of Transformer Fact Finding: Attempting to Reverse-Engineer Factual Recall on the Neuron Level (Post 1) Fact Finding: Simplifying the Circuit (Post 2) An Intuitive Explanation of How Large Language Models Store Facts Diff TransformerÔºöËÆ©Ê≥®ÊÑèÂäõÊú∫Âà∂Êõ¥Êô∫ËÉΩÔºåÊ∂àÈô§Âô™Èü≥ÔºåÊèêÂçáÂ§ßËØ≠Ë®ÄÊ®°ÂûãÊÄßËÉΩ-Áü•‰πé ","date":"4 May 2025","externalUrl":null,"permalink":"/en/blog/llm-memory/","section":"Blogs","summary":"\u003cdiv class=\"lead text-neutral-500 dark:text-neutral-400 !mb-9 text-xl\"\u003e\n  A summary of thoughts on the memory of large language models\n\u003c/div\u003e\n\n\n\n\u003ch2 class=\"relative group\"\u003eIntroduction \n    \u003cdiv id=\"introduction\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#introduction\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h2\u003e\n\u003cp\u003eThis reflection originates from a common issue encountered in AI-assisted programming: the need to repeatedly familiarize itself with the code repository from scratch.\u003c/p\u003e","title":"On the Memory of LLM","type":"blog"},{"content":" Summarizing and documenting the process of tinkering with Ubuntu for future reference. Introduction # Ubuntu, as a popular Linux distribution, offers better ecosystem support compared to other Linux distros. The most notable advantage is that when you encounter issues, you\u0026rsquo;re more likely to find tutorials and solutions for Ubuntu.\nThis article primarily focuses on the GUI-based personal edition of Ubuntu. For server-specific Ubuntu systems, operations depend on your actual business needs. The article Cloud Server Setup can serve as a reference.\nUbuntu Installation # The installation was done a long time ago, so no detailed records exist. If needed, please search for keywords like \u0026ldquo;installing Ubuntu on a portable hard drive.\u0026rdquo; Here‚Äôs a relatively recent guide: Installing Ubuntu on a Portable Hard Drive.\nThe installation process is no longer traceable. The current setup involves installing Ubuntu on a portable hard drive, allowing it to be used on-the-go.\nTo use it, simply plug in the hard drive before powering on the computer. Quickly press a specific key to enter the BIOS boot menu, set the priority to the highest, save, and exit to boot into Ubuntu.\nTo switch back to Windows, just unplug the hard drive and power on normally‚Äîno additional steps are required.\nInterface Customization # I personally prefer an Apple-style interface, so I specifically chose an Apple-inspired theme: WhiteSur.\nThe installation process is straightforward:\ngit clone https://github.com/vinceliuice/WhiteSur-gtk-theme.git --depth=1 cd WhiteSur-gtk-theme ./install.sh # Run the installation script For detailed configuration, refer to the instructions on the GitHub page.\nAs for updates, the official guide doesn‚Äôt specify, presumably assuming users already know:\ngit pull # Fetch the latest code ./install.sh # Re-run the installation script Fcitx 5 # Main reference: Install and Configure Fcitx 5 Chinese Input Method on Ubuntu. I initially considered using Sogou Input Method, but the official installation process seemed overly complicated, and it also required installing Fcitx 5. So, I figured I might as well just use Fcitx 5 directly.\nTo be honest, I was reluctant to use Fcitx at first ü•≤ because its interface is so \u0026ldquo;plain\u0026rdquo; that it‚Äôs hard to accept. I believe the author of the blog post above must have felt the same way üëÜ.\nUsing Windows Fonts # Approach: Copy font files from Windows to Ubuntu‚Äôs dedicated font directory, assign appropriate permissions, refresh Ubuntu‚Äôs font cache, and load the new fonts.\n# Windows font directory: C:/Windows/Fonts sudo cp /mnt/C/Windows/Fonts/LXGWWenKai-Regular.ttf /usr/share/fonts/custom/LXGWWenKai-Regular.ttf # Grant permissions sudo chmod u+rwx /usr/share/fonts/custom/* # Navigate to the font directory cd /usr/share/fonts/custom/ # Create font cache sudo mkfontscale # Refresh cache sudo fc-cache -fv Alternatively, you can download a new .ttf file from the web and copy it to the target directory. If you‚Äôre using a GUI-based Ubuntu system, you can simply double-click the font file to install it ü•∞.\nFonts may be installed redundantly, as the system doesn‚Äôt check for duplicates. If this happens, manually locate and delete the duplicate files in the relevant directory ü•≤. Ubuntu‚Äôs font tools can display all font information. Mounting Hard Drives # Since my Ubuntu system is installed on a portable hard drive, the main goal here is to access Windows partitions from Ubuntu. This section doesn‚Äôt cover detailed partition operations. For tasks like formatting partitions, refer to: How to Partition and Mount Disks in Ubuntu.\n# View disks and partitions (sudo privileges required) sudo fdisk -l # Create a mount point (essentially creating a folder) # The subfolder under /mnt is named \u0026#34;E\u0026#34; because it‚Äôs intended to mount the E drive sudo mkdir /mnt/E # Mount the new partition directly sudo mount /dev/vdb /mnt/E # Set auto-mount at boot # Check the partition‚Äôs UUID sudo blkid # Edit the specific file vim /etc/fstab # Append to the end of the file UUID=xxxxxxxx /mnt/E ntfs defaults 0 2 Replace the UUID above with the output from blkid. Replace ntfs with the appropriate filesystem type (common types include ntfs and ext 4). defaults: This is a combination of default mount options, such as rw (read-write) and relatime (reduces inode access time updates). 0 and 2: These values control backup and filesystem check order. Typically, the first value is 0 (no backup), and the second is 1 or 2 (1 for the root filesystem, 2 for others). Testing method:\n# If no errors occur, the configuration is correct sudo mount -a Creating Shortcuts # A common task: placing a quick link to a frequently used folder on the desktop for easy access.\n# Place a link to /target/dir in the Desktop folder # Replace with your target directory ln -s /target/dir ~/Desktop # Test‚Äîif you can cd into it, it works cd ~/Desktop/dir Configuring Git # One of the standout features of Linux is its extreme simplicity, which is why using the command line to manage Git is the preferred choice for Linux users üòÉ. Ubuntu comes with Git pre-installed, so there\u0026rsquo;s no need to install it separately. If you want to upgrade, follow these steps:\ngit --version # Check the Git version sudo add-apt-repository ppa:git-core/ppa # Add the official repository sudo apt update \u0026amp;\u0026amp; sudo apt upgrade # If possible, proceed with the upgrade Setting Up SSH for GitHub (Password-Free Configuration) # Of course, you can also use HTTPS directly, but the downside is that you‚Äôll need to enter your password every time. Moreover, with GitHub\u0026rsquo;s increasing security measures, the password isn‚Äôt necessarily your account password but rather a dedicated token ü•≤.\nSuch a cumbersome process is unbearable on Linux. I‚Äôd rather go through a tedious setup once than have to enter a long token every time.\nThis section is mainly referenced from: Configuring Git to Push by Default Without Entering Credentials (Ubuntu, SSH).\ngit config --global user.name \u0026#39;xx\u0026#39; # Configure the global username git config --global user.email \u0026#39;xxx@qq.com\u0026#39; # Configure the global email account # Generate an SSH key pair. Here, I choose to press Enter all the way through. ssh-keygen -t rsa -b 4096 -C \u0026#34;your_email@example.com\u0026#34; # Start the SSH agent and load the private key into the agent eval \u0026#34;$(ssh-agent -s)\u0026#34; ssh-add ~/.ssh/id_rsa # View and copy the public key content cat ~/.ssh/id_rsa.pub # Add this new SSH key to your GitHub account. # Change an existing HTTPS-linked repository to an SSH link git remote rm origin git remote add origin git@github.com:username/repository.git Installing and Managing Software # Software installation on Ubuntu generally falls into the following categories:\nVia the built-in Snap store Via apt Via .deb packages Via curl Different installation methods require different management approaches. curl installations are the most cumbersome to manage, while others can be handled easily with their respective package managers.\nSnap # Simply open the Snap store to install software effortlessly, though the packages are often outdated.\napt # # Software source operations can be performed graphically in the desktop version via \u0026#34;Software \u0026amp; Updates\u0026#34; # Add a software source sudo add-apt-repository ppa:libreoffice/ppa \u0026amp;\u0026amp; sudo apt update # Remove a software source sudo add-apt-repository --remove ppa:libreoffice/ppa # Install software sudo apt install xxx # Update packages sudo apt update # Sync package info from remote repositories without upgrading apt list --upgradable # View upgradable packages # Upgrade all available packages without handling dependency changes sudo apt upgrade sudo apt full-upgrade # Full upgrade sudo do-release-upgrade # Upgrade across major Ubuntu versions # Check software packages # Search for all packages containing \u0026#34;wps\u0026#34; and their description information sudo apt-cache search wps # View package names containing the keyword \u0026#34;wps\u0026#34; sudo apt-cache pkgnames | grep -i wps # Remove packages sudo apt remove xxx sudo apt autoremove # Clean up residuals deb # After downloading a .deb package from a browser, double-clicking it will install it directly. Internally, this uses apt, so management is the same as with apt.\n# Install via double-click # Uninstall via apt sudo apt remove xxx sudo apt autoremove # Clean up residuals AppImage # AppImage is a portable software packaging format for Linux systems, designed to simplify application distribution and execution. Its core philosophy is \u0026ldquo;one app = one file,\u0026rdquo; allowing users to run applications directly without installation or administrator privileges.\nIn newer versions of Ubuntu, attempting to run the file directly may result in an error. To resolve this, you need to install libfuse2 using the following command:\nsudo apt install libfuse2 Then grant executable permissions to the AppImage package:\nchmod +x file_name.AppImage After installation, you can simply double-click the package to launch the application üòÉ.\nNever install fuse directly, as this will automatically uninstall fuse3, causing the file system in newer Ubuntu versions to crash! If you accidentally install it, remove fuse and check the apt operation log to manually reinstall any automatically removed packages. If you want to uninstall the software, it‚Äôs very straightforward: just delete the package. However, if you‚Äôre a perfectionist like me, you can check the following directories to completely clean up any residual files:\nls ~/.config -a # Check configuration files ls ~/.local/share -a # Check shared configuration files ls ~/.cache -a # Check cache # Check disk usage of folders under .cache du -sh ~/.cache/* | sort -h -r curl # Download and execute installation scripts directly from a URL using curl. Software installed this way is harder to manage because the actual installation process is script-driven and difficult to monitor.\n# Example: Installing the Zed editor curl -f https://zed.dev/install.sh | sh # Uninstalling is usually messy # First, fetch the installation script curl -f https://zed.dev/install.sh -o install.sh # Have an AI parse the script # Then follow the AI‚Äôs instructions to manually uninstall Major Version Updates # Performing major version updates is completely unnecessary for server OSes, as the related software packages usually haven\u0026rsquo;t caught up yet. Chasing the \u0026ldquo;latest version\u0026rdquo; isn‚Äôt wise. However, for desktop users, it‚Äôs quite useful‚Äîafter all, updating allows them to experience the newest system features. In short, it‚Äôs just for fun. ü§ì\nThis section primarily references the WeChat public article: How to Upgrade from Ubuntu 24.04 to Ubuntu 25.04.\nData Backup # This step is essential. Although it might take up dozens of gigabytes of disk space, a major version update is still a risky operation. Better safe than sorry. üòÖ You can always delete the backup and free up space after a successful upgrade.\n# Install the backup tool sudo apt install deja-dup # Run directly deja-dup Update Software Packages # Ensuring the system is up to date minimizes compatibility issues. Execute the following commands one by one:\nsudo apt update sudo apt full-upgrade sudo apt autoremove sudo apt autoclean sudo reboot # Reboot the system Version Upgrade # The logic is straightforward: point the old version‚Äôs software sources to those of the new version. Below are some relevant files that may need modification:\nUpgrade policy file: /etc/update-manager/release-upgrades Software sources configuration file: /etc/apt/sources.list.d/ubuntu.sources If you want to upgrade from an LTS version to a non-LTS version, you\u0026rsquo;ll need to modify the policy file. The policy file actually contains just one line‚Äîchange it to the following:\nPrompt=normal Next, modify the software source configuration file by running the following command:\nsudo sed -i \u0026#39;s/noble/oracular/g\u0026#39; /etc/apt/sources.list.d/ubuntu.sources After modifying the file, execute:\n# Refresh the package index and perform a full upgrade, including the kernel, drivers, and all packages sudo apt update \u0026amp;\u0026amp; sudo apt full-upgrade -y After modifying the files:\n# Refresh the index and perform a full upgrade, including the kernel, drivers, and all packages sudo apt update \u0026amp;\u0026amp; sudo apt full-upgrade -y The -y option means \u0026ldquo;automatic confirmation.\u0026rdquo; If you prefer to manually type \u0026ldquo;yes,\u0026rdquo; you can omit it. üòÉ Personally, I‚Äôd rather not. After the upgrade completes:\nsudo reboot # Reboot to apply changes lsb_release -a # Verify the system version Office Suite # As we all know, Microsoft Office cannot run directly on Linux üòÖ. However, viewing and editing doc files is often unavoidable.\nTherefore, here‚Äôs a recommended Office alternative for Linux: LibreOffice. The installation steps are as follows:\nsudo add-apt-repository ppa:libreoffice/ppa sudo apt update sudo apt install libreoffice Before installing LibreOffice, I also tried using WPS to edit Office files, but for some reason, it kept causing system errors, so I eventually abandoned it.\nIf switching away from Office makes you feel lost, Wine might be your savior‚Äîit‚Äôs the sorcery that runs Windows apps on Linux! Storage Cleanup # # Remove orphaned dependencies sudo apt autoremove # Clear apt cache sudo du -sh /var/cache/apt # Check apt cache size sudo apt autoclean # Auto-clean sudo apt clean # Full clean # Clear system logs journalctl --disk-usage # Check system log size sudo journalctl --vacuum-time=3 d # Remove logs older than 3 days # Clear .cache du -sh ~/.cache/* | sort -h -r # Check cache size rm -r folder_name # delete the folder recursively # Clean up old Snap versions snap list --all # List all Snap packages # List all disabled packages (single-line command) echo -e \u0026#34;\\033[1 mDisabled Snap Packages and Their Sizes:\\033[0 m\u0026#34; \u0026amp;\u0026amp; snap list --all | awk \u0026#39;/disabled|Â∑≤Á¶ÅÁî®/{print $1}\u0026#39; | while read -r pkg; do size=$(snap info \u0026#34;$pkg\u0026#34; | awk \u0026#39;/installed:/ {print $4}\u0026#39;); printf \u0026#34;%-30 s %10 s\\n\u0026#34; \u0026#34;$pkg\u0026#34; \u0026#34;$size\u0026#34;; done | sort -k 2 -h # Remove all disabled Snap packages (single-line command) snap list --all | awk \u0026#39;/disabled|Â∑≤Á¶ÅÁî®/{print $1, $3}\u0026#39; | while read snapname revision; do sudo snap remove \u0026#34;$snapname\u0026#34; --revision=\u0026#34;$revision\u0026#34;; done # Clean up old kernels sudo dpkg --list | grep linux-image # List all kernels sudo apt autoremove --purge # Automatically remove unnecessary kernels Miscellaneous # This section includes some simple yet commonly used commands.\nSystem Control # Shut down immediately: shutdown now Restart immediately: sudo reboot Extracting Files # The command varies depending on the file format you need to extract.\n# Extract a .zip file unzip file.zip -d /target/directory # Extract a .tar file tar -xvf file.tar # Extract a .tar.gz file tar -xzvf file.tar.gz References # How to Partition and Mount Disks in Ubuntu LibreOffice Suite Âú® Ubuntu ÂÆâË£ÖÈÖçÁΩÆ Fcitx 5 ‰∏≠ÊñáËæìÂÖ•Ê≥ï Configuring Git to Push by Default Without Entering Credentials (Ubuntu, SSH) ","date":"1 May 2025","externalUrl":null,"permalink":"/en/blog/ubuntu-note/","section":"Blogs","summary":"\u003cdiv class=\"lead text-neutral-500 dark:text-neutral-400 !mb-9 text-xl\"\u003e\n  Summarizing and documenting the process of tinkering with Ubuntu for future reference.\n\u003c/div\u003e\n\n\n\n\u003ch2 class=\"relative group\"\u003eIntroduction \n    \u003cdiv id=\"introduction\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#introduction\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h2\u003e\n\u003cp\u003eUbuntu, as a popular Linux distribution, offers better ecosystem support compared to other Linux distros. The most notable advantage is that when you encounter issues, you\u0026rsquo;re more likely to find tutorials and solutions for Ubuntu.\u003c/p\u003e","title":"Ubuntu Tinkering Notes","type":"blog"},{"content":" Keywords: Mutual Reasoning; Small Models; Reasoning Capability Enhancement Research Background # Existing Work # The following content is generated by AI to quickly explain what RAP is.\nThe research foundation is a framework called RAP (Reasoning via Planning), which aims to enhance the reasoning capabilities of models in complex tasks by using language models (LLMs) as both world models and reasoning agents, combined with the Monte Carlo Tree Search (MCTS) algorithm for strategic exploration.\nProblem Background:\nCurrent large language models (LLMs) have limitations in reasoning tasks, mainly due to the lack of mental representations of the environment (world models), making it difficult to predict the outcomes and long-term effects of actions. Additionally, LLMs lack a reward mechanism to evaluate reasoning paths and cannot balance exploration and exploitation, leading to inefficient reasoning. RAP Framework:\nLanguage Model as World Model: Defines states and actions through natural language, models the reasoning process as a Markov Decision Process (MDP), and uses LLMs to predict the outcomes of each action. Reward Design: Uses the log probability of actions, confidence levels, and the LLM\u0026rsquo;s own evaluation results as rewards to guide reasoning towards an ideal state. Monte Carlo Tree Search (MCTS): Iteratively constructs a search tree through MCTS, balancing exploration and exploitation, and ultimately selects high-reward reasoning paths. Shortcomings # LLMs struggle to effectively explore the solution space, often generating low-quality reasoning paths. LLMs have difficulty accurately assessing the quality of reasoning paths. These issues are more pronounced in small models. Method Overview # Human-like Reasoning # The reasoning paths are still generated using MCTS, but with more actions that simulate human thinking processes: decomposing and searching a reasoning step, proposing sub-problems, problem transformation, etc.\nPath Evaluation # The mutual consistency principle is used to determine the quality of paths, i.e., introducing another small model of comparable capability as a \u0026ldquo;peer\u0026rdquo; to evaluate the quality of reasoning paths. The specific process is as follows:\nThe framework provides the \u0026ldquo;peer\u0026rdquo; small model with some local reasoning paths as prompts, and then asks this small model to complete the reasoning path. If the path generated by MCTS matches the path completed by the \u0026ldquo;peer\u0026rdquo; small model, then this reasoning path is considered high-quality and can be a candidate path. Using small models of comparable capability avoids distilling large models; using \u0026ldquo;peer models\u0026rdquo; to evaluate paths rather than directly guiding path generation; judging \u0026ldquo;path consistency\u0026rdquo; mainly relies on the final result. Detailed Methodology # Symbol Explanation Table:\nSymbol Meaning \\(x\\) Target Problem \\(M\\) Target Small Model \\(T\\) Search Tree Generated by Small Model Using MCTS \\(s\\) Intermediate Reasoning Step \\(t\\) Candidate Path, a Complete Reasoning Path in \\(T\\) \\(ans\\) Final Reasoning Path for \\(M\\) to Solve \\(x\\) \\(Score\\) Reasoning Path Evaluation Function \\(a\\) An Action Sampled from the Action Space \\(s_{d}\\) Termination Reasoning Step, Contains the Answer \\(\\hat{M}\\) \u0026ldquo;Peer\u0026rdquo; Small Model \\(T_{validate}\\) \\(T\\) Pruned by Path Evaluation Function \\(Estimate\\) Path Evaluation Function Problem Formalization # Formalize the abstract natural language description of \u0026ldquo;small model solving reasoning problems\u0026rdquo;:\n$$ t=x\\oplus s_1 \\oplus s_2 \\oplus ...\\oplus s_d $$$$ T=\\left \\{ t^1, t^2, ..., t^n \\right \\} $$$$ T_{validate}=Estimate(T) $$$$ ans = max(Score(T_{validate})) $$ Human-like Reasoning # Action Space # One-step Thinking: Given the existing reasoning path, let the model generate the next reasoning step. Quick Thinking: Directly let the model complete all reasoning steps until the final result is produced. Sub-problem + Answer: Let the model propose a sub-problem and answer it. Sub-problem Re-answer: The answer generated in the previous step may be inaccurate, so provide an additional action option to re-answer the sub-problem. Problem Restatement: Let the model reorganize the conditions of the problem. Note that action 4 can only occur after action 3, and action 5 can only occur after the problem itself (root node). Reward Function # Inspired by AlphaGo, the evaluation (reward) of intermediate steps is set as their contribution to the correct result. The specific implementation is as follows:\nInitialize \\(Q(s_{i},a_{i})=0\\); randomly generate the next step until a termination node \\(s_{d}\\) is encountered. Use consistency voting to calculate \\(Q(s_{d},a_{d})\\), which is also the confidence score of the termination node. Backpropagation: \\(Q(s_{i},a_{i})=Q(s_{i},a_{i})+Q(s_{d},a_{d})\\). MCTS # Generally, the classic MCTS is used: selection, expansion, simulation (Rollout), and backpropagation; however, to obtain more accurate reward values, multiple simulation evaluations are performed.\nThe exploration-exploitation balance still uses the classic UCT formula:\n$$ UCT(s,a) = \\frac{Q(s,a)}{N(s,a)}+c\\sqrt{ \\frac{\\ln N_{parent}(s)}{N(s,a)} } $$Where \\(N(s,a)\\) represents the number of times a node has been visited, \\(Q(s,a)\\) is the cumulative reward value, and \\(c\\) represents the balance rate.\nPath Evaluation # For a reasoning path \\(t=x\\oplus s_1 \\oplus s_2 \\oplus ...\\oplus s_d\\), randomly select a reasoning step \\(s_{i}\\). Inject the reasoning path before \\(s_{i}\\), \\(t_{1}=x\\oplus s_1 \\oplus s_2 \\oplus ...\\oplus s_i\\), as a prompt into the \u0026ldquo;peer\u0026rdquo; small model \\(\\hat{M}\\). \\(\\hat{M}\\) completes the path, generating a new path \\(t'=x\\oplus s_1 \\oplus s_2 \\oplus ...\\oplus s_i \\oplus s_{i+1}' \\oplus \\dots \\oplus s_{d}'\\). If the \u0026ldquo;path consistency\u0026rdquo; is achieved, i.e., the problem answers are consistent, then \\(t'\\) is considered a candidate path. The process of selecting candidate paths is the pruning process of \\(T\\), ultimately producing \\(T_{validate}\\). Final Selection # For each candidate path in the candidate path tree \\(t=x\\oplus s_1 \\oplus s_2 \\oplus ...\\oplus s_d\\):\n$$ Score(t)=\\prod_{i=1}^{d} Q(s_{i},a) $$Finally, select the reasoning path with the highest score:\n$$ ans = max(Score(T_{validate})) $$ Other Details # MCTS performs 32 Rollouts. When processing the MATH dataset, the maximum depth of MCTS is 8; in other cases, the maximum depth is 5. The reasoning of the \u0026ldquo;peer\u0026rdquo; small model and the target small model can be executed in parallel to improve computational efficiency. During path evaluation, the truncation point of the path should be between 20% and 80% of the total path. ","date":"20 March 2025","externalUrl":null,"permalink":"/en/blog/rstar-note/","section":"Blogs","summary":"\u003cp\u003e\n\n\u003cdiv class=\"lead text-neutral-500 dark:text-neutral-400 !mb-9 text-xl\"\u003e\n  Keywords: Mutual Reasoning; Small Models; Reasoning Capability Enhancement\n\u003c/div\u003e\n\u003c/p\u003e\n\n\n\u003ch2 class=\"relative group\"\u003eResearch Background \n    \u003cdiv id=\"research-background\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#research-background\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h2\u003e\n\n\n\u003ch3 class=\"relative group\"\u003eExisting Work \n    \u003cdiv id=\"existing-work\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#existing-work\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h3\u003e\n\u003cp\u003eThe following content is generated by AI to quickly explain what RAP is.\u003c/p\u003e","title":"rStar Reading Notes","type":"blog"},{"content":" Detailed documentation of the process of deploying an LLM application from scratch, focusing mainly on the initial setup of cloud servers, continuously updated\u0026hellip; Preface # Before deploying your LLM application, you should have already completed:\nA web front-end framework with core functionalities A relatively complete back-end code repository This document mainly records the setup process of cloud services for testing and development environments. Cloud servers for production environments are not covered here, so please refer to other articles. No practice, no sayü´°\nSolution Exploration # Although this article is about \u0026ldquo;Cloud Server Setup,\u0026rdquo; I still want to document some \u0026ldquo;non-cloud service\u0026rdquo; solutions, as not all testing and development environments require an expensive cloud server. If you have already decided to use a cloud server, then skip to: Cloud Service Operation Process\nThe following content assumes your main production environment is Windows, as I have not used Linux or MacOSüò¢\nLocal Services # General Idea # If you are a truly \u0026ldquo;independent\u0026rdquo; developer with no need for collaborative development, then you don\u0026rsquo;t need a cloud server at all. You just need Docker.\nAfter installing Docker Desktop, you need to perform container orchestration based on your existing back-end service code.\nThis step mainly involves:\nEstablishing API interfaces: Writing the main operational logic, which can be done in any programming language you prefer Creating necessary environment files: Such as .env, pyproject.toml, etc. Creating a DockerFile: Essentially a set of command-line instructions to initialize your container Creating a docker-compose.yml file: In this file, you need to orchestrate the images used by your app, specifying internal networks, ports, mounted volumes, and other necessary settings Then, run the command: docker-compose up --build\nAfter Docker initialization, you can directly access your service via the localhost domain, just like using a cloud server.\nDocker Path Issues # When orchestrating Docker containers, there are several necessary working path parameters to understand, otherwise, you might encounter \u0026ldquo;file not found\u0026rdquo; errorsüò¢\nbuild.context: This parameter exists in docker-compose.yml and refers to the \u0026ldquo;build context,\u0026rdquo; pointing to a real directory on your local machine WORKDIR: This parameter exists in the Dockerfile and is used to specify the \u0026ldquo;working directory\u0026rdquo;; subsequent RUN, COPY, and other commands will execute in this directory if relative paths are used, but absolute path parameters are not affected Port Forwarding # If you have built a Docker service locally but your team members need a unified testing environment, you can simply perform port forwarding on your local service.\nThere are many applications that support port forwarding, and I won\u0026rsquo;t list them all here. I personally use Sakura Frp\nAlthough port forwarding is very convenient, if you need front-end and back-end interaction, it\u0026rsquo;s best not to use port forwardingüò¢\nMy initial idea was: run the front-end web page locally, and also run the back-end service in a local Docker container, then use port forwarding to forward the front-end web page to users and the back-end to the front-end.\nIt seemed fine, but since your network service is not SSL verified, it cannot send https requests, causing the browser to block cross-origin insecure resource requests from the front-end to the back-endüò¢ The result is that only your computer can run the complete service process, and other devices will fail; even if you configure a self-signed certificate, it\u0026rsquo;s hard to pass the client browser\u0026rsquo;s security mechanismüò≠\nAfter unsuccessful attempts, I chose to use a cloud server.\nCloud Services # Now, the operation process of cloud servers is very simple, but there is one drawback: expensiveüò¢\nOf course, if it\u0026rsquo;s not for a production environment but a testing and development environment, you don\u0026rsquo;t need to choose a top-tier server. Choose a suitable one based on your financial situation and team size.\nI ultimately chose Tencent Cloud\u0026rsquo;s lightweight server: 2 cores + 2GB RAM + 4M bandwidth + 50GB system disk + 300GB monthly traffic\nNo other reason, mainly because it\u0026rsquo;s cheap, only Ôø•88 for the first year.\nCloud Service Operation Process # Register a Domain # There are plenty of tutorials online, so I won‚Äôt go into detail here. Just pick a domain registrar you trust, and you‚Äôre good to go.\nNot sure what a domain name is? Here‚Äôs my simple way of putting it: Do you remember your apartment‚Äôs full official address? Probably not. But you definitely remember the name of your apartment complex, right? A domain name is kind of like that‚Äîeasy to remember.\nOf course, if you‚Äôre a big company, it‚Äôs also about looking impressive üòÑ\nSet Up DNS Resolution # DNS resolution is usually a service provided by your domain registrar. When you buy a domain, they‚Äôll typically throw in a free DNS resolution service. It‚Äôs a basic version, sure‚Äîbut hey, it worksüëå\nSo what is DNS resolution exactly? Think of a domain name like the name of your apartment complex. It makes perfect sense to you, but outsiders probably have no idea where that is. DNS acts like a translator‚Äîit turns your apartment complex\u0026rsquo;s name into the full official address: Province XX, City XX, Street XX\u0026hellip;\nThat way, even people from out of town can find your place and drop by without getting lost.\nPurchase a Cloud Server # Just buy it from a cloud server provider‚Äîthis service system is already well-established. Nothing more to say. üòÖ\nConfigure the Server # The server operating system I use here is Ubuntu Server 24.04 LTS 64bit, and subsequent commands are based on this system.\nFile Transfer # To transfer files from the local machine to the server, I chose to use some more modern terminals, such as: Tabby, WindTerm, and Warp, etc.\nI randomly chose Tabby, which makes remote connections more convenient and has built-in SFTP for easy file transfer. If you have the energy to customize the terminal interface, Tabby can be very aesthetically pleasingüòÑ\nOf course, more traditional solutions include using FileZilla; and if you don\u0026rsquo;t mind the hassle, you can directly use terminal commands: use rsync or scp commands.\nIf you are using a Tencent Cloud server and are curious about what the lighthouse folder isü§®: this folder is the account for one-click password-free login. Install Docker # Execute the following commands one by one, each command has an explanation:\nsudo apt-get update # Upgrade sudo apt install -y apt-transport-https ca-certificates curl gnupg lsb-release # Install dependency tools, mainly https transmission and verification-related toolkits # Install Docker\u0026#39;s GPG key to ensure the Docker image you download has not been tampered with sudo curl -fsSL https://mirrors.cloud.tencent.com/docker-ce/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc # Add Docker\u0026#39;s official software repository to the system\u0026#39;s APT source list # I don\u0026#39;t think you want to know what this bunch meansŸ©(‚Ä¢Ã§ÃÄ·µï‚Ä¢Ã§ÃÅ‡πë) sudo install -m 0755 -d /etc/apt/keyrings sudo chmod a+r /etc/apt/keyrings/docker.asc echo \u0026#34;deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://mirrors.cloud.tencent.com/docker-ce/linux/ubuntu/ \\ $(. /etc/os-release \u0026amp;\u0026amp; echo \u0026#34;$VERSION_CODENAME\u0026#34;) stable\u0026#34; | sudo tee /etc/apt/sources.list.d/docker.list \u0026gt; /dev/null # Update to let apt recognize the newly added Docker software source sudo apt-get update # Install Docker engine sudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin # Start Docker sudo docker info # Verify installation information sudo systemctl start docker # Start service sudo systemctl enable docker # Enable auto-start on boot sudo systemctl status docker # Verify service status After running the last command, you will enter a paged log view mode, similar to a Vim editor. Enter :q to return to the regular command line. Use Docker # sudo systemctl start docker # Start service # Configure Tencent Cloud\u0026#39;s Docker source sudo vim /etc/docker/daemon.json # Create configuration file # Press the I key to switch to I mode and enter { \u0026#34;registry-mirrors\u0026#34;: [ \u0026#34;https://mirror.ccs.tencentyun.com\u0026#34; ] } # Press esc, then enter \u0026#39;:wq\u0026#39;, this is a Vim operation, meaning save and exit sudo systemctl restart docker # Restart Docker service sudo systemctl stop docker # Stop Docker service # Verify source information, you should see the Registry Mirrors value is the URL you set sudo docker info # Pull image sudo docker pull nginx # If you want to use docker-compose # On Ubuntu, use: docker compose sudo docker compose up --build # Start service without rebuilding sudo docker compose up -d # Stop all services sudo docker compose down Before using the docker compose command, please create docker-compose.yml and Dockerfile according to the official tutorial. Configure pip/poetry # Strangely, Tencent Cloud installs Python by default at the system level but does not install pipü§î So you need to manually install it. Before installation, please confirm whether Python is installed.\nIf you are not using pip at the system level but pip inside a Docker container, then changing the pip source at the system level will not have any effect. Change pip source at the system level:\n# Check Python version python3 -V # Check the path of python3 which python3 # Confirm Python is installed and pip is not installed sudo apt install python3-pip # If using Tencent\u0026#39;s server, you can use the intranet domain for faster speed pip config set global.index-url https://mirrors.tencentyun.com/pypi/simple # If not, use the external domain pip config set global.index-url https://mirrors.cloud.tencent.com/pypi/simple Change pip source inside Docker container: Directly write the following command in your Dockerfile\n# Configure pip source # Use Tencent Cloud mirror source, note this is the intranet domain RUN pip config set global.index-url https://mirrors.tencentyun.com/pypi/simple \\ \u0026amp;\u0026amp; pip config set global.trusted-host mirrors.tencentyun.com If you use poetry at the system level:\n# Install poetry using online script curl -sSL https://raw.githubusercontent.com/python-poetry/poetry/master/get-poetry.py | python # Install poetry using apt sudo apt install python3-poetry # Use Tencent Cloud mirror source, note this is the intranet domain sudo poetry config repositories.tencentyun https://mirrors.tencentyun.com/pypi/simple If your system does not have pip installed by default, it is not recommended to install pip first and then install poetry. SSL/TLS # SSL/TLS is a set of encrypted transmission protocols, carefully designed by cryptography experts. The widely used Hypertext Transfer Protocol Secure (HTTPS) is obtained by encrypting the plaintext transmission protocol HTTP using SSL/TLS.\nA core part of HTTPS is the handshake before data transmission, during which the password for data encryption is determined. During the handshake, the website sends an SSL certificate to the browser. The SSL certificate is similar to our daily ID card, serving as an identity proof for HTTPS websites. The SSL certificate contains the website\u0026rsquo;s domain name, certificate validity period, certificate issuing authority, and the public key used for encrypting the transmission password. Before generating the password, the browser needs to verify whether the currently accessed domain name matches the domain name bound to the certificate, and also verify the Certificate Authority (CA). If the verification fails, the browser will give a certificate error prompt.\nGenerally, SSL certificates need to be purchased from CA institutions, but some CA institutions provide free SSL certificate services, such as: Let‚Äôs Encrypt\nAfter long-term development, the certificates issued by Let‚Äôs Encrypt can now be automatically processed through Certbot, and the certificates can be automatically re-applied when they expire. Of course, such convenience comes at a costüò¢: You need to configure certbot, which is quite troublesome.\nHere, I chose to directly deploy a certbot inside a Docker container to apply for SSL certificates, meaning you can directly modify the content in docker-compose.yml to pull the official certbot image.\n# First, stop all Docker services sudo docker compose down # Start all services sudo docker compose up -d # Start part of the services sudo docker compose up nginx -d # View logs of a specific image sudo docker logs server-app-1 # View logs of a specific service sudo docker compose logs certbot-init # Use recursive deletion to clear cache rm -rf ./certbot/conf/* # Enter a container\u0026#39;s shell sudo docker exec -it server-app-1 /bin/sh # Remove a Docker image forcefully sudo docker rmi -f server-frontend # Restart a specific service in Docker Compose sudo docker compose restart app # Clear npm cache forcefully npm cache clean --force # Activate a Python virtual environment source your_venv_name/bin/activate Filing # Filing using a corporate entity is not difficult, so I won\u0026rsquo;t elaborate too much here. This section mainly discusses individual entity filing. Main reference: Tencent Cloud Server Filing Full Process 40 Days of Filing Blood and Tears - Blog Garden\ngraph LR; p1(Server Platform Real-name Authentication)--\u003ep2(Real-name Authentication Purchase Domain)--\u003ep3(Apply for Filing)--\u003ep4(Platform Review)--\u003ep5(Regulatory Authority Review)--\u003ep6(Public Security Filing) If you are a Tencent Cloud user, you can use the Tencent Cloud Website Filing Mini Program; after submitting the application, the platform will first review it, usually taking about 8 hours; then the platform will submit it to the regulatory authority for review, usually taking about 7 working days; after the regulatory authority review is passed, you need to complete the public security filing within 30 working days.\nMiscellaneous # This section mainly documents issues that may arise during the entire operation process.\nCertbot Access Denied? # First, of course, check the network reasons. Check whether the security group of the server has opened ports 443 and 80, and check whether the firewall of the server operating system has blocked port access.\n# Ubuntu installs ufw firewall by default # Check ufw status, inactive status means not started, which is the ideal state sudo ufw status If you\u0026rsquo;re based in China, here\u0026rsquo;s another issue: Certbot can\u0026rsquo;t reach your server if you don\u0026rsquo;t have ICPÂ§áÊ°à (ICP license). üò¢\nReferences # Using Third-party SSH Terminal to Log in to Linux Instance - Operation Guide - Tencent Cloud Using Local Built-in SSH Terminal to Log in to Linux Instance - Operation Guide - Tencent Cloud Installing Docker in Linux Environment Ubuntu 22.04 Install Docker What is Docker\u0026rsquo;s Official GPG Key For? Cloud Server Setup Docker - Practice Tutorial - Tencent Cloud Installing Docker and Configuring Mirror Acceleration Source - Practice Tutorial - Tencent Cloud Debian 12 / Ubuntu 24.04 Install Docker and Docker Compose Tutorial - Shaobing Blog Step-by-Step Guide to Python pip Source Change Operation in Linux System - Tencent Cloud pip Source Configuration - Tencent Cloud Developer Community - Tencent Cloud How to Change Domestic Source for Poetry - Data Science SourceResearch Linux Stop Docker Container: Single, Multiple, or All Where to View Docker Logs? How to View Logs in Linux Server - CSDN Blog Troubleshooting and Solving Linux Firewall Closed but Still Unable to Access Web About Ubuntu Console Opened All Ports, but External Hosts Still Cannot Access Corresponding Port Services - CSDN Blog Three Minutes to Explain SSL Authentication and Encryption Technology - CSDN Blog One Article to Thoroughly Understand SSL/TLS Protocol - Zhihu HTTPS and SSL Certificate Overview | Rookie Tutorial Four Common Free Certificate Application Methods - CSDN Blog Using Docker to Deploy Nginx and Configure HTTPS - TandK - Blog Garden Using Docker + Nginx + Certbot to Automate SSL Certificate Management - CSDN Blog The Certificate Authority Failed to Download the Temporary Challenge Files Created by Certbot \u0026ndash; Connection Refused - Help - Let\u0026rsquo;s Encrypt Community Support ICP Filing First Filing - Tencent Cloud ICP Filing Requirements by Province - Tencent Cloud Complete Tencent Cloud Server Filing Process: 40 Days of Painful Experience - Zheng Weizhong - CNBlogs ICP Filing and Public Security Filing Process - Tencent Cloud ICP Filing Video Verification - Tencent Cloud In-Depth Analysis of npm Cache: Understanding, Usage, and Clearing Guide_npm Clear Cache - CSDN Blog Deploying a Web Application with Docker - Zhihu Why Do HTTPS Requests Get Downgraded to HTTP? - CSDN Blog HTTPS Redirected to HTTP‚ÄîRisk Control-CSDN Blog How to Fill Out the Public Security Network Filing ‚Äî Guide to Public Security Network Filing Information_Filing Service - Alibaba Cloud Help Center ","date":"6 March 2025","externalUrl":null,"permalink":"/en/blog/cloud-server-build/","section":"Blogs","summary":"\u003cdiv class=\"lead text-neutral-500 dark:text-neutral-400 !mb-9 text-xl\"\u003e\n  Detailed documentation of the process of deploying an LLM application from scratch, focusing mainly on the initial setup of cloud servers, continuously updated\u0026hellip;\n\u003c/div\u003e\n\n\n\n\u003ch2 class=\"relative group\"\u003ePreface \n    \u003cdiv id=\"preface\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#preface\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h2\u003e\n\u003cp\u003eBefore deploying your LLM application, you should have already completed:\u003c/p\u003e","title":"Cloud Service Deployment","type":"blog"},{"content":" A detailed record of the core features of the Qdrant vector database, helping developers design better databases in practical applications üòã Preface # Vector database-based RAG is the most fundamental form of RAG and is widely used in production. As the name suggests, vector databases primarily store vectors. Through vector operations, we can achieve more precise relevance searches, which serve as the cornerstone for many application scenarios.\nThere are many vector database providers: Weaviate, Qdrant, Milvus, Chroma, and more. However, they are generally similar. For a detailed comparative analysis of mainstream vector databases, see: Vector Database Comparison Report: A Detailed Comparison of 12 Mainstream Vector Databases.\nThis article focuses solely on Qdrant. All content is based on the Qdrant Official Documentation (as of March 2025).\nThis article focuses less on specific syntax and more on features and principles, helping developers quickly design product workflows without being constrained by cumbersome syntax, thereby improving imagination and efficiency.\nBasic Data Types # Qdrant defines some abstract data types to better handle vector data. Understanding these types is fundamental to flexible usage.\nPoint # Points are the core data type in a vector database. All operations revolve around points.\nA very basic point contains only its vector, but typically, points are tagged with additional metadata to provide more information beyond the vector data. These tags are called Payload üëá.\nThus: Point = Vector + Payload Tags\n// A simple point { \u0026#34;id\u0026#34;: 129, \u0026#34;vector\u0026#34;: [0.1, 0.2, 0.3, 0.4], \u0026#34;payload\u0026#34;: {\u0026#34;color\u0026#34;: \u0026#34;red\u0026#34;}, } Qdrant points can be configured with various types of vectors: Dense Vectors, Sparse Vectors, Multi-Vectors, and Named Vectors.\nVector Type Description Dense Vectors Standard vectors; most embedding models generate this type. Sparse Vectors Variable-length vectors with few non-zero elements; typically used for exact token matching and collaborative filtering. Multi-Vectors Matrices composed of multiple dense vectors; dimensions must match across points, but the number of vectors can vary. Used to store different vector descriptions of the same target. Named Vectors A hybrid of the above types, allowing different vector types to coexist in a single point. These vectors are abstracted as named vectors. Basic CRUD operations are straightforward and won\u0026rsquo;t be elaborated here.\nPayload # Additional metadata attached to vectors, described and stored using JSON syntax. Here\u0026rsquo;s an example:\n{ \u0026#34;name\u0026#34;: \u0026#34;jacket\u0026#34;, \u0026#34;colors\u0026#34;: [\u0026#34;red\u0026#34;, \u0026#34;blue\u0026#34;], \u0026#34;count\u0026#34;: 10, \u0026#34;price\u0026#34;: 11.99, \u0026#34;locations\u0026#34;: [ { \u0026#34;lon\u0026#34;: 52.5200, \u0026#34;lat\u0026#34;: 13.4050 } ], \u0026#34;reviews\u0026#34;: [ { \u0026#34;user\u0026#34;: \u0026#34;alice\u0026#34;, \u0026#34;score\u0026#34;: 4 }, { \u0026#34;user\u0026#34;: \u0026#34;bob\u0026#34;, \u0026#34;score\u0026#34;: 5 } ] } Since this data is stored in the database, it serves a purpose: vector databases primarily rely on semantic similarity matching, and these tags allow you to add additional logical filtering conditions on top of that.\nFor more on filtering, see: Filtering\nCollection # A collection is simply a group of points. At this level, you can define:\nSimilarity algorithms between points Vector dimensions Optimizer configurations HNSW algorithm configurations: The key parameter to adjust is ef, which determines the number of neighboring nodes the algorithm visits. Higher values yield more accurate but slower queries. WAL configurations Quantization configurations Common Operations # These are the most fundamental and important operations when using a vector database.\nSearch # In the context of vector databases, \u0026ldquo;search\u0026rdquo; primarily refers to similarity search. The theoretical premise is that objects with higher similarity in the real world are closer in vector space.\nThe term \u0026ldquo;closer\u0026rdquo; implies a similarity metric. Qdrant supports several popular similarity algorithms:\nDot Product Similarity Cosine Similarity Euclidean Distance Manhattan Distance To improve performance, all vectors are normalized before storage. This means dot product similarity and cosine similarity are equivalent in Qdrant. For a better user experience, Qdrant provides a complete API for easy invocation: Search API - Qdrant.\nIn summary, the functionalities you can invoke include:\nBasic Operations: Input a vector to perform similarity matching in the database. If your points store \u0026ldquo;named vectors,\u0026rdquo; you need to specify which vector to use for matching. Search Algorithm Control: You can enable exact search, which performs similarity matching on every point, taking longer (more parameters are adjustable but rarely used). Result Filtering: Filter results before the actual search using payload tags to narrow the scope, or after the search using similarity score thresholds. Result Count: The limit parameter controls the number of results returned (the top limit most similar results). Batch Search: Input multiple vectors for search in one go. Search Grouping: Group search results by certain tags. The group_size parameter sets the number of results per group. Search Planning: Based on optional indexes, filter complexity, and the total number of points, a heuristic method selects an appropriate search approach (improving performance ü§î). If both group_size and limit are set, limit represents the number of groups. Additionally, sparse and dense vector searches in Qdrant have key differences:\nComparison Sparse Vectors Dense Vectors Similarity Algorithm Defaults to dot product; no need to specify You can specify supported algorithms Search Method Only exact search Can use HNSW for approximate search Search Results Returns only vectors with shared non-zero elements Returns the top limit vectors Explore # Explore operations are more flexible searches: they can query based on similarity as well as dissimilarity.\nRecommendation # \u0026ldquo;Recommendation\u0026rdquo; allows you to provide both positive and negative vectors for search. Here\u0026rsquo;s an official example:\nimport { QdrantClient } from \u0026#34;@qdrant/js-client-rest\u0026#34;; const client = new QdrantClient({ host: \u0026#34;localhost\u0026#34;, port: 6333 }); client.query(\u0026#34;{collection_name}\u0026#34;, { query: { recommend: { positive: [100, 231], negative: [718, [0.2, 0.3, 0.4, 0.5]], strategy: \u0026#34;average_vector\u0026#34; } }, filter: { must: [ { key: \u0026#34;city\u0026#34;, match: { value: \u0026#34;London\u0026#34;, }, }, ], }, limit: 3 }); In the official example, 100 and 231 are vector IDs, each corresponding to a 4-dimensional vector. The strategy parameter controls the search algorithm. Here are the details:\nAverage Algorithm: Averages positive and negative examples separately, then combines them into a final search vector.\nBest Score Algorithm: Each candidate point is matched against all positive and negative examples to compute scores. The highest scores are selected, and the final score is calculated as:\nlet score = if best_positive_score \u0026gt; best_negative_score { best_positive_score } else { -(best_negative_score * best_negative_score) }; Negative-Only Algorithm: Uses the best score algorithm üëÜ without positive examples, yielding a reverse scoring algorithm to find the least relevant points. Multi-vectors and other special vectors can also be processed with similar logic, though the syntax differs. Discovery # \u0026ldquo;Discovery\u0026rdquo; operations partition the sample space. You provide pairs of positive and negative vectors, each dividing the space into positive and negative regions. The search returns points that lie more in positive regions or less in negative regions.\nSimilar to Recommendation, but here you pair positive and negative vectors together as input.\nDue to hard partitioning, consider increasing the HNSW ef parameter to compensate for precision loss. Discovery operations enable Qdrant to handle two new search requirements:\nDiscovery Search: Given a target vector and a set of positive-negative vector pairs as contextual constraints.\nRegion Partition Search: A special case of discovery search üëÜ where no target vector is provided. The database partitions the space directly and returns points lying most in positive regions.\nIn discovery search, contextual constraints are enforced with higher priority. In other words, discovery search first performs region partitioning, then standard similarity search. Distance Matrix # This operation resembles batch search. Batch search inputs multiple vectors and searches for similar vectors for each. Distance matrix randomly selects a subset of vectors, then searches for similar vectors within this subset for each vector.\nFor example, if sample=100 vectors are selected to form a subset, and limit=10 similar vectors are searched for each, the returned distance matrix will be a \\(100 \\times 10\\) matrix, where each row represents the top 10 similar vectors for one point.\nThis operation is typically used for data visualization or dimensionality reduction.\nFiltering # Official English guide: A Complete Guide to Filtering in Vector Search. The guide explains how Qdrant internally performs filtering, which helps design efficient systems.\nThe guide briefly lists some functionalities. For complete details, see: Filtering.\nFilter Conditions # These refer to individual filter conditions, the basic units of filtering. Here are the types:\nType Function Match The condition is a specific value; the attribute must exactly match it. Match Any The condition is a set of options; the attribute must match any of them. Match Except The condition is a set of options; the attribute must not match any of them. Range The condition is a range; the attribute must lie within it. Values Count The attribute is an array; filtering is based on the number of elements. Is Empty Filters based on whether the attribute exists. These are the basic filter types. Syntax varies for different payload types:\nJSON Payload: A point\u0026rsquo;s payload can be a JSON object, and any field can participate in filtering. See Nested Key. Date Range: Similar to numeric ranges, date ranges can also filter. Geofiltering: Geographic locations can be filtered. Named Vectors: Named vectors contain vectors of different dimensions. Filtering can check for the presence of specific vectors (e.g., filtering points with image embeddings). These basic conditions can be nested using logical keywords üëá to form complex filters.\nLogical Keywords # Similar to SQL\u0026rsquo;s AND, OR, NOT, Qdrant uses must, should, must_not to express similar logic. These keywords build complex filters.\nmust: Returns true only if all listed conditions are met. should: Returns true if any listed condition is met. must_not: Returns true if none of the listed conditions are met. Advanced Operations # Hybrid Queries # Optimization # Storage # Indexing # Snapshots # References # Vector Database Comparison Report: A Detailed Comparison of 12 Mainstream Vector Databases | SynDataWorks Qdrant Official Documentation ","date":"5 March 2025","externalUrl":null,"permalink":"/en/blog/qdrant-feature-guide/","section":"Blogs","summary":"\u003cp\u003e\n\n\u003cdiv class=\"lead text-neutral-500 dark:text-neutral-400 !mb-9 text-xl\"\u003e\n  A detailed record of the core features of the Qdrant vector database, helping developers design better databases in practical applications üòã\n\u003c/div\u003e\n\u003c/p\u003e\n\n\n\u003ch2 class=\"relative group\"\u003ePreface \n    \u003cdiv id=\"preface\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#preface\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h2\u003e\n\u003cp\u003eVector database-based RAG is the most fundamental form of RAG and is widely used in production. As the name suggests, vector databases primarily store vectors. Through vector operations, we can achieve more precise \u003cstrong\u003erelevance searches\u003c/strong\u003e, which serve as the \u003cstrong\u003ecornerstone\u003c/strong\u003e for many application scenarios.\u003c/p\u003e","title":"Qdrant Feature Guide","type":"blog"},{"content":" Practical Issues # The emergence of any technology is driven by real-world problems. RAG (Retrieval-Augmented Generation) technology was developed primarily to address the following limitations of generative pre-trained language models:\nKnowledge Limitations: Once a large language model (LLM) is trained, its weight parameters are fixed, meaning its performance and capabilities rely solely on what was learned during the training phase, resulting in limited timeliness. High Cost of Knowledge Updates: Traditional full-parameter training methods are extremely expensive, making it difficult to quickly integrate newly generated data into the model\u0026rsquo;s parameters. Hallucination Issues: LLM outputs are probabilistic, which can lead to factually incorrect or \u0026ldquo;hallucinated\u0026rdquo; responses. Therefore, in real-world production environments, LLMs require a technology that can efficiently, accurately, and rapidly update domain-specific knowledge.\nSolutions # To address these issues, the academic community has proposed several solutions. These approaches focus on different stages of the LLM development pipeline:\nData-Level Enhancement: Expanding the model\u0026rsquo;s knowledge sources. Architecture-Level Improvement: Modifying the Transformer architecture to enable real-time dynamic adjustments to its internal parameters. Post-Training Optimization: Incorporating fine-tuning techniques. Query-Level Augmentation: Retrieving relevant knowledge from external databases based on user queries to enhance responses. The original LLM workflow:\ngraph LR; p1(Data)--\u0026gt;p2(Transformer)--\u0026gt;p4(Generation); p3(Query)--\u0026gt;p2; The improved workflow:\ngraph LR; datatype1(File);datatype2(Image);datatype3(Audio);datatype4(HTML);data(data); datatype1--\u0026gt;data;datatype2--\u0026gt;data;datatype3--\u0026gt;data;datatype4--\u0026gt;data; architecture1(Transformer);architecture2(Dynamic Transformer);architecture3(Fine-tuned Transformer); data--\u0026gt;architecture1;data--\u0026gt;architecture2; query(Query);retrieval(Retrival); query--\u0026gt;retrieval--\u0026gt;architecture3; retrieval--\u0026gt;architecture2; finetuning(Fine tuning);data2(Specific data); architecture1--\u0026gt;finetuning--\u0026gt;architecture3; data2--\u0026gt;finetuning; generation1(Generation);generation2(Generation); architecture3--\u0026gt;generation1; architecture2--\u0026gt;generation2; Academic Definition # RAG (Retrieval-Augmented Generation) is an artificial intelligence technology that combines information retrieval with language generation models. It enhances the performance of large language models (LLMs) in knowledge-intensive tasks‚Äîsuch as question answering, text summarization, and content generation‚Äîby retrieving relevant information from external knowledge bases and incorporating it as contextual prompts. The RAG framework was first introduced by Facebook AI Research (FAIR) in 2020 and has since become a widely adopted solution in LLM applications. Original paper: Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.\n","date":"4 March 2025","externalUrl":null,"permalink":"/en/blog/rag-report/","section":"Blogs","summary":"\u003ch2 class=\"relative group\"\u003ePractical Issues \n    \u003cdiv id=\"practical-issues\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#practical-issues\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h2\u003e\n\u003cp\u003eThe emergence of any technology is driven by real-world problems. RAG (Retrieval-Augmented Generation) technology was developed primarily to address the following limitations of generative pre-trained language models:\u003c/p\u003e","title":"RAG Technology Research Report","type":"blog"},{"content":" This blog post is the zeroth article in the \u0026ldquo;AI Engineering\u0026rdquo; series. Besides serving as an index for the entire series, it also summarizes relevant knowledge points from practical experiences with large models. Introduction # Foundation of Large Models # This section is essentially a review of large model technologies, so if you have a solid foundation, you might not need to read it.\nOf course, what does having a solid foundation mean? Below are some self-assessment questions, which are also commonly asked in interviews:\nWrite out the forward propagation process of the Transformer model on scratch paper. What do Encoder and Decoder mean? How do you estimate the expressive power of a model? What is the attention mechanism? What is layer normalization? How do language models sample based on probability? Related articles:\nLarge Model Foundation Reading Notes Model Training # This part involves some unavoidable issues in the training process of large models:\nWhy do most models today adopt a Decoder-only architecture? How to estimate the amount of data a model needs? What are the requirements for the data? How to estimate the training duration? How to evaluate the computational power requirements for the training and deployment phases? How to choose among different model fine-tuning methods? How to estimate the computational power needed for model fine-tuning? Related articles:\nPractical LLM Training Model Deployment # Application Development # This section mainly documents the practical experiences of large models oriented towards application development based on foundational models.\nSome Reflections # When we have become accustomed to casually opening an AI chat assistant: DouBao, Kimi, DeepSeek\u0026hellip; something that appears so simple, it might seem like creating one ourselves wouldn\u0026rsquo;t be too difficult, right?\nThis was my \u0026ldquo;dream starting point\u0026rdquo;üò¢ It sounds ridiculous, but I really thought that at the time. And then came the long journey of development.\nIf you have similar thoughts, the content below may be helpful to youü§ó\nAfter staying up so many nights, I must admit that \u0026ldquo;building an LLM application\u0026rdquo; is a very extensive topic, especially for an independent developer. If you choose to do independent development or form a small team, it also means you\u0026rsquo;ve chosen a tough path: there\u0026rsquo;s no \u0026ldquo;senior\u0026rdquo; to help you explore the technical route; everything has to be figured out by yourself.\nHowever, this is where the fun of independent development lies: you can fully control the direction of your product and watch it gradually iterate and improve.\nYou might ask: \u0026ldquo;What if the product doesn\u0026rsquo;t perform well?\u0026rdquo;; \u0026ldquo;What if the development gets stuck?\u0026rdquo;; \u0026ldquo;If no one uses it, isn\u0026rsquo;t my time wasted?\u0026quot;ü§î\nIt must be said that the investment in independent development is truly substantial, and these questions are unavoidable, even inevitable for most independent developers.\nBut if you really take \u0026ldquo;independent development\u0026rdquo; seriously, regardless of the outcome, I believe you will gain something: if you succeed, congratulations to you hereü•≥ Your hard work has paid off; if you fail or give up, I understand your frustration and even anger.\nBut no one can find happiness or success in completely negating their past. Forget those pains for now and move on to the next step in life.\nThere are no wrong turns in life; every step countsü´°\nIdentifying Needs # Draft draft draft\u0026hellip;\nExploring Implementation Solutions # Draft draft draft\u0026hellip;\nProject Construction # Draft draft draft\u0026hellip;\nService Release # Cloud Service Deployment ","date":"28 May 2025","externalUrl":null,"permalink":"/en/blog/llm-engineering-note/","section":"Blogs","summary":"\u003cdiv class=\"lead text-neutral-500 dark:text-neutral-400 !mb-9 text-xl\"\u003e\n  This blog post is the zeroth article in the \u0026ldquo;AI Engineering\u0026rdquo; series. Besides serving as an index for the entire series, it also summarizes relevant knowledge points from practical experiences with large models.\n\u003c/div\u003e\n\n\n\n\u003ch2 class=\"relative group\"\u003eIntroduction \n    \u003cdiv id=\"introduction\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#introduction\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h2\u003e\n\n\n\u003ch2 class=\"relative group\"\u003eFoundation of Large Models \n    \u003cdiv id=\"foundation-of-large-models\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#foundation-of-large-models\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h2\u003e\n\u003cp\u003eThis section is essentially a review of large model technologies, so if you have a solid foundation, you might not need to read it.\u003c/p\u003e","title":"Notes on LLM Engineering Practices","type":"blog"},{"content":" The basic info and operations of Neo4j Graph Database. Introduction # Neo4j is a high-performance graph database that stores data and the relationships between data in the form of graphs.\nThe specific form of the graph is a labeled property graph, and the query language used is Cypher.\nLabeled Property Graph # A labeled property graph is a specific type of graph:\nA node has one or more labels to define its type.\nRelationships and nodes are treated equally, holding the same level of importance.\nEach node and relationship possesses accessible properties.\nCypher # Cypher is a¬†declarative¬†query language that allows you to identify¬†patterns¬†in your data using an¬†ASCII-art style syntax¬†consisting of¬†brackets,¬†dashes¬†and¬†arrows.\nPatterns # A pattern in a graph database is a specific combination of nodes and relationships. For example, a person (Person) acting in a movie (Movie) is a pattern, represented in code as:\n(p:Person)-[r:ACT_IN]-\u0026gt;(m:Movie) Here, the parts enclosed in parentheses represent nodes, while the parts enclosed in square brackets represent relationships. In the node section, p and m are variables referring to the respective nodes, with Person and Movie being the labels of the nodes, connected by a colon. In the relationship section, r is the variable referring to the relationship, and ACT_IN is the specific type of relationship. Translated into natural language, this means: use p to refer to a node labeled Person, use m to refer to a node labeled Movie, and represent the relationship between them with r, which is of type ACT_IN.\nData Reading # Data reading operations rely on pattern matching, using the keyword MATCH. This is equivalent to sending an instruction to the database to filter out only the node-relationship pairs that match the specified pattern, i.e., a collection of triples (p, r, m).\nMATCH (p:Person)-[r:ACT_IN]-\u0026gt;(m:Movie) Similar to SQL, you can further filter using the WHERE keyword:\nMATCH (p:Person)-[r:ACT_IN]-\u0026gt;(m:Movie) WHERE p.name = \u0026#39;Tom Hanks\u0026#39; RETURN p, r, m Here, name is a property of the node p, accessed using the . operator. The RETURN keyword marks the values to be returned.\nBelow is a more complex pattern matching command, where the AS keyword is used to set aliases:\nMATCH (p:Person)-[:ACTED_IN]-\u0026gt;(m:Movie)\u0026lt;-[r:ACTED_IN]-(p2:Person) WHERE p.name = \u0026#39;Tom Hanks\u0026#39; RETURN p2.name AS actor, m.title AS movie, r.role AS role This command filters out all actors who have acted in the same movie as Tom Hanks, returning their names, the movies they acted in, and the roles they played.\nSorting by a specific property and pagination are also supported:\nMATCH (m:Movie) WHERE m.released IS NOT NULL RETURN m.title AS title, m.url AS url, m.released AS released ORDER BY released DESC LIMIT 5 This will filter out the 5 most recent movies.\nCypher keywords are case-insensitive; property names, variable names, and other identifiers are case-sensitive.\nData Writing # Data writing uses the MERGE keyword, which means merging a node or relationship into the data graph.\nMerging nodes: MERGE (m:Movie {title: \u0026#34;Arthur the King\u0026#34;}) SET m.year = 2024 RETURN m This command creates a new Movie node with the title property set to \u0026quot;Arthur the King\u0026quot; and the year property set to 2024.\nYou might wonder why not write it like this:\nMERGE (m:Movie) SET m.year = 2024, m.title = \u0026#34;Arthur the King\u0026#34; RETURN m The reason is that the content within the curly braces is used to check whether the pattern you want to create already exists, avoiding duplicate patterns.\nMerging relationships: MERGE (m:Movie {title: \u0026#34;Arthur the King\u0026#34;}) MERGE (u:User {name: \u0026#34;Adam\u0026#34;}) MERGE (u)-[r:RATED {rating: 5}]-\u0026gt;(m) RETURN u, r, m Installation # As of February 15, 2025, after extensive testing by the developer community, it has been found that Neo4j Desktop is blocked for users in mainland China, causing the software interface to fail to display properly. While bypassing the block through methods such as disabling the network is possible, this removes the key advantage of the desktop version‚Äîeasy deployment.\nThus, I recommend using the Docker deployment method.\nDocker Desktop # Simply install Docker Desktop and run it in the background.\nPulling the Image # Normally, you can simply pull the latest image with the command: docker pull neo4j.\nHowever, if your project requires the APOC plugin, you should consider the version of APOC, as the image may be ahead of the APOC version. The community version of APOC is released on Releases ¬∑ neo4j/apoc.\nFor APOC compatibility, use the following command: docker pull neo4j:5.26.2.\nBuilding the Container # docker run -d -p 7474:7474 -p 7687:7687 -v E:/neo4j/data:/data -v E:/neo4j/logs:/logs -v E:/neo4j/conf:/var/lib/neo4j/conf -v E:/neo4j/import:/var/lib/neo4j/import -v E:/neo4j/plugins:/var/lib/neo4j/plugins -e NEO4J_dbms_security_procedures_unrestricted=\u0026#34;apoc.*\u0026#34; -e NEO4J_dbms_security_procedures_allowlist=\u0026#34;apoc.*\u0026#34; -e NEO4JLABS_PLUGINS=\u0026#39;[\u0026#34;apoc\u0026#34;]\u0026#39; -e NEO4J_AUTH=neo4j/mo123456789 --name neo4j neo4j:5.26.2 Explanation of the parameters:\nThe -p option exposes ports; here, we open two ports. The -v option mounts directories from the host machine (this means specifying where the Docker application will store its data). The -e option configures environment variables. The apoc-related configurations are for the APOC plugin; NEO4J_AUTH sets the username to neo4j and the password to mo123456789. If you use Neo4j for language model-enhanced generation (RAG), be sure to include the APOC-related configurations. Otherwise, you can omit these settings. After running the command above, check the host machine\u0026rsquo;s mounted directory to confirm if the APOC plugin is installed:\nManual Installation of APOC # Visit Releases ¬∑ neo4j/apoc, download the latest apoc-5.26.2-core from the \u0026ldquo;Assets\u0026rdquo; section, and paste it into the host machine\u0026rsquo;s specified directory. Then restart the Docker container.\nBrowser UI # By default, the 7474 port is used for the browser UI, and the 7687 port is for other backend applications.\nWhen the container is running in the background, access the UI at: http://localhost:7474/browser/preview.\nConnect to the database using: neo4j://localhost:7687.\nYou should now be able to access the browser UI successfully. üòÑ\nReferences # Docker: Docker Deployment of Neo4j Graph Database - Angry Radish - Blog ","date":"5 February 2025","externalUrl":null,"permalink":"/en/blog/neo4j-basics/","section":"Blogs","summary":"\u003cdiv class=\"lead text-neutral-500 dark:text-neutral-400 !mb-9 text-xl\"\u003e\n  The basic info and operations of Neo4j Graph Database.\n\u003c/div\u003e\n\n\n\n\u003ch2 class=\"relative group\"\u003eIntroduction \n    \u003cdiv id=\"introduction\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#introduction\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/neo4j/neo4j\" target=\"_blank\"\u003eNeo4j\u003c/a\u003e is a high-performance graph database that stores data and the relationships between data in the form of graphs.\u003c/p\u003e","title":"Neo4j Basics","type":"blog"},{"content":"TODO_UPDATE Analysis Report on the Current State of Schedule Management Software Functionality Preface # The term \u0026ldquo;schedule management\u0026rdquo; frequently appears in our daily lives, yet it remains elusive: What exactly is schedule management? Why do we need it? What purpose does it serve? Who engages in schedule management, and how effective is it?\nWhat is Schedule Management? # On the surface, \u0026ldquo;schedule management\u0026rdquo; seems like a common concept, but in reality, it encompasses complex and multifaceted content.\nIn summary, schedule management is a method of clearly recording, planning, organizing, and optimizing personal or team time and tasks to achieve set goals.\nDepending on different needs, it can be categorized into the following types:\nPersonal Growth and Self-Improvement:\nCharacteristics: Entirely dependent on personal will, often with a broad goal but no specific events or timelines, possibly with a quantitative metric. Primary Purpose: Facilitate long-term personal growth, improve quality of life, and enhance satisfaction. Professional and Academic Management:\nCharacteristics: Highly specific events and timelines, not driven by personal preference, often requiring collaboration with others, usually with one or more quantitative metrics. Primary Purpose: Achieve concrete career goals, enhance professional skills, and improve efficiency. Social and Relationship Management:\nCharacteristics: Involves complex interpersonal dynamics, lacks quantitative metrics, is unpredictable, and has low controllability. Primary Purpose: Maintain, expand, and optimize social networks, increase social capital. Financial and Daily Life Management:\nCharacteristics: Tasks are trivial, frequently repetitive, flexible yet follow periodic patterns, and generally short in duration. Primary Purpose: Ensure a sense of order and stability in life. In reality, schedules are a mix of the above aspects. Thus, when we casually refer to \u0026ldquo;schedule management,\u0026rdquo; we vaguely mean \u0026ldquo;managing all aspects of life,\u0026rdquo; leading to a very ambiguous understanding of the concept.\nWhat Constitutes \u0026ldquo;Good\u0026rdquo; Schedule Management? # Generally, \u0026ldquo;good schedule management\u0026rdquo; is a clear, realistic, flexible system that effectively balances long-term goals with short-term actions, incorporates regular feedback and continuous adjustment mechanisms, significantly enhances personal control, reduces stress, and is easy to record and execute. Specifically, it should meet the following criteria:\nGoal Clarity Realistic and Executable Flexibility \u0026amp; Resilience Balance between Long-term and Short-term Feedback and Adjustment Mechanisms Reduce Stress and Enhance Control Ease of Use \u0026amp; Clear Recording From a user experience perspective, \u0026ldquo;good schedule management\u0026rdquo; simply needs to answer the following questions:\nHow to collect necessary information: User schedules, preferences, weather data, online information, etc. How to process the collected information: Different workflows for different types of schedules. How to enhance user experience: Streamlined and warm interactions, intuitive and concise information presentation. Industry Status # Below is a list of existing schedule management solutions. However, to be honest, there are few domestic applications in this niche, while international offerings, though numerous, suffer from severe homogenization.\nTickTick # TickTick offers comprehensive functionality with a clean design, representing a classic static schedule management app. Its logic for schedule management is roughly as follows:\ngraph LR; id1(\u0026#34;Schedule Collection\u0026#34;)--\u0026gt;id2(\u0026#34;Manual Schedule Arrangement\u0026#34;)--\u0026gt;id3(\u0026#34;Execute Schedule on Time\u0026#34;); id2(\u0026#34;Manual Schedule Arrangement\u0026#34;)--\u0026gt;id4(\u0026#34;Schedule Not Executed on Time\u0026#34;)--\u0026gt;id5(\u0026#34;Manual Adjustment\u0026#34;); Dola # Dola is a futuristic AI-powered schedule management app: it has no interface, and all interactions with the AI occur via messaging platforms like WhatsApp or Apple Messages. However, it does not support mainstream Chinese apps like QQ or WeChat.\nBased on official descriptions, here‚Äôs an approximate user flow:\ngraph LR; id1(\u0026#34;Send Message via Platform\u0026#34;)--\u0026gt;id2(\u0026#34;AI Analyzes and Generates Schedule\u0026#34;)--\u0026gt;id3(\u0026#34;Store Schedule Internally\u0026#34;); id1(\u0026#34;Send Message via Platform\u0026#34;)--\u0026gt;id4(\u0026#34;AI Analyzes and Modifies Schedule\u0026#34;); id1(\u0026#34;Send Message via Platform\u0026#34;)--\u0026gt;id5(\u0026#34;AI Analyzes and Returns Query Results\u0026#34;) Motion # Motion is an AI-driven work management app primarily targeting entrepreneurial teams, with many features designed for collaboration. During testing, we focused on its schedule arrangement capabilities but found the results underwhelming.\nThe core logic is: divide time into work and personal blocks, then let the AI insert tasks into available slots. The outcome essentially devolves into simple task insertion: if there‚Äôs free time, a task is added‚Äîeven if the deadline is a month away or the day is already packed.\nSummary # Static Schedule Management # Manual adjustments are unavoidable in static schedule management apps and represent the most labor-intensive part of the process. While these apps can create time blocks for scheduling, they lack dynamic adjustment capabilities. This isn‚Äôt an issue for fixed, external schedules with clear start and end times.\nHowever, for internal, flexible, or trivial tasks‚Äîlike memorizing 15 words, doing laundry, or reading a magazine‚Äîthese tasks often lack specific start times and only need completion by the end of the day. They also lack clear end times; for example, memorizing 15 words might take 30 minutes, but on a good day, it could take only 20.\nThese trivial tasks pose a significant challenge to static scheduling. While a single task‚Äôs delay or early completion may not disrupt the schedule, their cumulative effect can be highly disruptive.\nAdditionally, if these scattered tasks aren‚Äôt aligned, they can create time vacuums, leaving users unsure of what to do next. For instance, if a user finishes memorizing words early and has 10 minutes to spare, how should they use it? Without clear guidance, they might default to scrolling through short videos, leading to further delays and more time vacuums.\nTo break this vicious cycle, three strategies exist:\nManual Adjustment by the User: Highly cumbersome, wasting precious time on trivial arrangements, and potentially causing further delays. Ignore Adjustments and Time Control: This renders schedule management meaningless, reducing it to a simple to-do list without time control‚Äîdefeating its purpose. Strict Adherence to the Schedule: Theoretically perfect, but practically ineffective. Life is unpredictable, and delays or early completions disrupt subsequent tasks, leading to rushed work or wasted time. Time vacuums and their cascading effects are the Achilles\u0026rsquo; heel of static schedule management apps, severely limiting their adoption. Consequently, schedule management is often seen as a niche activity for highly disciplined individuals, and the software is perceived as catering only to this small group.\nI‚Äôve used several static schedule management apps but struggled to stick with them: either the learning curve was too steep, or rigid schedules were impractical to follow. Thus, a truly meaningful schedule management tool shouldn‚Äôt demand a life free of surprises or strict adherence to plans. Instead, it should provide solutions for when things go off track.\nDynamic Schedule Management # Existing dynamic schedule management apps partially address the inflexibility of static tools but remain constrained by traditional paradigms, requiring extensive manual input. This creates a paradox: if users are inputting so much manually, why use AI at all? Why not rely on faster, more precise algorithms?\nBolder innovations are needed to leverage the full potential of large language models for generalized processing.\nUnderstanding the Current Landscape # Understanding public perceptions and needs regarding schedule management is crucial for defining the app‚Äôs form.\nWe conducted a small-scale survey, revealing:\nMost respondents only occasionally plan their schedules. Dedicated schedule management apps are less popular than built-in phone notes. Over half reported that most planned schedules weren‚Äôt completed as intended. Among desired features, \u0026ldquo;ease of use\u0026rdquo; ranked highest, followed by \u0026ldquo;personalization.\u0026rdquo; Though the sample size was small, we can infer:\nPublic awareness of schedule management is low. Simplicity and lowering barriers to use should be foundational. Personalization is another key area for optimization. Technical Proposal # Based on the above, we propose the following goals:\nBreak the illusion that mental scheduling is efficient, reduce reliance on deadlines, address the high cost and inflexibility of traditional tools, and solve AI‚Äôs inability to capture implicit preferences or balance long- and short-term goals. Make schedule management a practical part of daily life, genuinely improving efficiency and quality of life‚Äîtruly enabling: Easy Schedule, Simple Life.\nFor feasibility, we exclude: social/relationship management, financial tracking, and team collaboration, focusing solely on individual users‚Äô needs for personal growth and daily task management.\nInterface Design # To keep the interface intuitive, we draw inspiration from life: a boss communicates simply with a secretary to arrange schedules.\nThus, the interface adopts the simplest form: a chat dialog, akin to messaging a personal assistant on QQ or WeChat.\nEssential Data Collection # Since user interaction occurs via chat, all user-related data must be extracted from messages. Key data includes:\nSchedule Data: Tasks the user needs to complete, e.g., \u0026ldquo;Submit academic English homework by next Monday.\u0026rdquo; User Preferences: Unconscious habits, e.g., procrastinating on English homework. Execution Data: Whether scheduled tasks were completed. External Data: Weather, news, etc., fetched from the web. Schedule data comes from user messages, preferences are inferred from adjustments to \u0026ldquo;unreasonable\u0026rdquo; schedules, and external data is retrieved as needed.\nProcessed data is stored long-term in a \u0026ldquo;memory system,\u0026rdquo; technically implemented as RAG: vector databases for schedules/preferences, relational databases for arranged tasks, with AI granted direct access for \u0026ldquo;memory updates.\u0026rdquo;\nData Processing # The \u0026ldquo;memory system\u0026rdquo; uses semantic and temporal searches to populate prompts, guiding the AI‚Äôs scheduling decisions.\nA pre-built knowledge base provides scientific principles for scheduling, ensuring plans are both personalized and scientifically sound.\nData Presentation # To maintain simplicity, only two elements are displayed:\nChat Dialog: For input and scheduling. TODO List: For tracking task completion. System Implementation # The system architecture is shown above, with the following workflow:\nUser input is saved to an internal message history database for global access. Parameter Extraction: The LLM extracts parameters from input and handles unsupported messages. Memory Query: Relevant data is fetched from schedules and vector databases, formatted into XML for the LLM to decide the next action and extract parameters. Action Execution: Database APIs update schedules and vector stores based on the LLM‚Äôs decision. Response Generation: Update logs are formatted into XML prompts, and the LLM generates a user-friendly response. Schedule Storage Implementation # Vector Schedule Database # Schedule Table # Parameter Extraction Implementation # Memory Query Implementation # Action Decision Implementation # Database Interface Implementation # Response Generation Implementation # Testing Analysis # ","date":"27 January 2025","externalUrl":null,"permalink":"/en/blog/schedule-management-report/","section":"Blogs","summary":"\u003cp\u003eTODO_UPDATE\n\u003cdiv class=\"lead text-neutral-500 dark:text-neutral-400 !mb-9 text-xl\"\u003e\n  Analysis Report on the Current State of Schedule Management Software Functionality\n\u003c/div\u003e\n\u003c/p\u003e\n\n\n\u003ch2 class=\"relative group\"\u003ePreface \n    \u003cdiv id=\"preface\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#preface\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h2\u003e\n\u003cp\u003eThe term \u0026ldquo;schedule management\u0026rdquo; frequently appears in our daily lives, yet it remains elusive: What exactly is schedule management? Why do we need it? What purpose does it serve? Who engages in schedule management, and how effective is it?\u003c/p\u003e","title":"Schedule Management Report","type":"blog"},{"content":" Summary of CUMCM 2024 competition experience, focusing on the A problem and the collaborative approach with MATLAB code improvement Process Overview # graph LR; id1(\"Task Allocation\")--\u003eid2(\"VS Code Collaboration\")--\u003eid3(\"MATLAB Code Execution\"); Task Allocation # There are two key points:\nTasks are divided into two parts: a main part and a secondary part. Tasks should be independent of each other, meaning no dependencies between them. Code Collaboration # Software Tools # VS Code Editor, along with the Live Server plugin, MATLAB plugin\nMATLAB.\nNaming Conventions # Main function should be uniformly named main: The main function is the one that directly computes the final result, while others should be called auxiliary functions. Data processing code: This refers to code that does not return any value but generates data tables. It should start with data, e.g., converting solar altitude angle \\(\\phi\\) to cosine value, dataCosPhi. Internal data conversion code: This should start with to, e.g., converting coordinates from a natural coordinate system to a Cartesian coordinate system StoXY. Plotting code: Code related to plotting graphs should start with fig. Testing code: Should start with test. For other special types of functions, they should be named based on their functionality. Functions that return boolean values should start with is, such as a collision detection function isCollided; functions that return other data should start with get. File Documentation Comments # Except for the common types of files mentioned in the naming conventions (data, test, fig, main), all other files should include documentation comments.\nThe documentation should be concise yet clear. It generally includes a brief description of the function, the input parameters, and the output parameters.\nIf you\u0026rsquo;re unsure how to write documentation, you can use AI assistance.\nInternal Variable Naming # Fixed Parameters # All fixed parameters should be placed in a config.m file located in the root directory for centralized management. Each parameter should be followed by a comment explaining its purpose, for example:\nlearningRate = 0.001; % Learning rate batchSize = 32; % Batch size numEpochs = 100; % Number of epochs To use this configuration file in other code files, simply include the following line:\nrun(\u0026#39;config.m\u0026#39;); Even the parameters are transformed into another file, the VS Code can also recognizes it and provide complement suggestion. Function-Internal Parameters # All variables used within a function should be explicitly defined at the beginning of the function and should include brief Chinese comments.\nIf similar parameters are used across multiple files, make sure to use consistent naming, especially in AI-generated code. Use F2 in VS Code to rename them. Code Formatting # Code formatting is mainly handled using the official MATLAB plugin. After you activate the plugin, press Shift+Alt+F to format your code.\nGit Version Control # A GitHub repository needs to be created to store the entire project files. Although the Live Server plugin enables faster real-time code collaboration, it is still necessary to commit and save at key development milestones to maintain the ability to roll back code.\nAll Git version control operations are performed on the lead developer\u0026rsquo;s computer, while other supporting developers use the Live Server plugin for more immediate code collaboration.\nLocal commit to save small improvements push operations are done in major increments Code Execution # Thanks to the latest function of the plugin MATLAB, we can now directly run and debug in the VS Code IDE. Though there are some limitations, it deserves a standing ovation.\nOther # AI Instruction Set # Since generative AI\u0026rsquo;s code style might differ from the project\u0026rsquo;s standards, if you need to generate an entire file, please include the following code standard instructions at the beginning of your request:\nYou are a mature and standardized MATLAB programmer. While correctly implementing the user\u0026#39;s goal, you should follow these code standards: 1. Clear and concise file documentation comments: The documentation should include a brief description of the function, input parameters, and output parameters. 2. Function-internal variable names should be concise and easy to read. 3. All variables used within a function should be explicitly defined at the beginning of the function, with brief Chinese comments added next to them. User\u0026#39;s instruction: Example Project # In order to develop a project as fast and well-defined as possible, I have created an Example Project which contains all the regulations mentioned before. You can directly change the files in the example project without mnemonic cost. üòÑ\nCode Tips # Parallel Execution\nMATLAB supports multi-threaded computing. You can convert a typical for loop to a parfor loop for parallel execution. The execution of parfor functions is subject to strict requirements. Please refer to the official documentation for more details. Huge Table Process # The comment output of the Mathematical Model is a Huge table in .mat file, which is usually hard to abstract the target data.\nHere I directly write a simply Python program, Data extractor, to automatically operate the huge table data. With tiny modification, you can get any data you want from the .mat file.\nUtilizing the online LaTeX table editor, we can get the capacity to quickly insert the table data into your thesis.\n","date":"16 January 2025","externalUrl":null,"permalink":"/en/blog/code-collaboration-scheme/","section":"Blogs","summary":"\u003cp\u003e\n\n\u003cdiv class=\"lead text-neutral-500 dark:text-neutral-400 !mb-9 text-xl\"\u003e\n  Summary of CUMCM 2024 competition experience, focusing on the A problem and the collaborative approach with MATLAB code improvement\n\u003c/div\u003e\n\u003c/p\u003e\n\n\n\u003ch2 class=\"relative group\"\u003eProcess Overview \n    \u003cdiv id=\"process-overview\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#process-overview\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h2\u003e\n\u003cdiv class=\"mermaid\" align=\"center\"\u003e\n  \u003cpre\u003e\ngraph LR;\nid1(\"Task Allocation\")--\u003eid2(\"VS Code Collaboration\")--\u003eid3(\"MATLAB Code Execution\");\n\u003c/pre\u003e\n\u003c/div\u003e\n\n\n\n\u003ch2 class=\"relative group\"\u003eTask Allocation \n    \u003cdiv id=\"task-allocation\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#task-allocation\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h2\u003e\n\u003cp\u003eThere are two key points:\u003c/p\u003e","title":"Code Collaboration Scheme","type":"blog"},{"content":" MySQL Installation and Deployment Process + Quick Syntax CookBook + Study Notes Information Source # SQL Tutorial - Liao Xuefeng\u0026rsquo;s Official Website\nThis is an extremely user-friendly MySQL tutorial website with an embedded web-based database, making it easy for beginners to get a hands-on understanding of MySQL operations. It also provides concise yet essential explanations of the background of SQL. MySQL Installation # The simplest way to install MySQL is through Docker Desktop. It takes only two steps to complete:\nRun the following command in your terminal:\ndocker pull mysql Run MySQL With Shell # Then initialize and run the SQL container:\ndocker run -d --name mysql -p 3306:3306 -e MYSQL_ROOT_PASSWORD=password -v /Users/liaoxuefeng/mysql-data:/var/lib/mysql mysql Parameter Explanation:\nParameter Description -d Run the container in the background --name Assign a name to the container (if not specified, Docker will choose one automatically) -p 3306:3306 Map the container\u0026rsquo;s port 3306 to the local machine, allowing you to connect to MySQL through port 3306 -e MYSQL_ROOT_PASSWORD=password Set the root password for MySQL (in this case, the password is password). If not set, Docker will generate a password, which you‚Äôll need to check the logs to retrieve. It‚Äôs recommended to set a password. -v /path:/var/lib/mysql Mount a local directory to /var/lib/mysql in the container, which will store MySQL data. Replace /path with the actual directory path on your machine mysql Specifies the name of the Docker image you want to run When using Docker to run MySQL, you can always delete the MySQL container and rerun it. If you delete the locally mounted directory, rerunning the container is equivalent to starting with a fresh MySQL instance. Run MySQL With docker-compose # Create docker-compose.ymlÔºö\nservices: mysql: image: mysql ports: - \u0026#34;3306:3306\u0026#34; environment: MYSQL_ROOT_PASSWORD: password volumes: - ../data:/var/lib/mysql restart: unless-stopped Run docker-compose build up -d to start service.\nMySQL Basic Syntax # Querying # Comment Syntax:\n-- This is a comment Basic Query Syntax:\nSELECT * FROM \u0026lt;table_name\u0026gt;; Conditional Query:\nSELECT * FROM \u0026lt;table_name\u0026gt; WHERE \u0026lt;condition\u0026gt;; In the condition expression, you can use various logical operators such as: AND, NOT, OR.\nProjection Query:\nSELECT \u0026lt;column1\u0026gt;, \u0026lt;column2\u0026gt;, \u0026lt;column3\u0026gt; FROM \u0026lt;table_name\u0026gt;; SELECT \u0026lt;column1\u0026gt; alias1, \u0026lt;column2\u0026gt; alias2, \u0026lt;column3\u0026gt; alias3 FROM \u0026lt;table_name\u0026gt;; Sorting:\n-- Sort by score in ascending order: SELECT id, name, gender, score FROM students ORDER BY score; -- Sort by score in descending order: SELECT id, name, gender, score FROM students ORDER BY score DESC; -- Sort by score, gender: SELECT id, name, gender, score FROM students ORDER BY score DESC, gender; -- Sorting with a WHERE condition: SELECT id, name, gender, score FROM students WHERE class_id = 1 ORDER BY score DESC; Pagination Query:\n-- Query the first page: SELECT id, name, gender, score FROM students ORDER BY score DESC LIMIT 3 OFFSET 0; -- Start from the 0th record, fetch up to 3 records, which may be less than 3. Aggregation Query, using MySQL‚Äôs aggregation functions:\n-- Count the total number of records: SELECT COUNT(*) FROM students; -- Set the alias for the count result: SELECT COUNT(*) num FROM students; -- Conditional aggregation: SELECT COUNT(*) boys FROM students WHERE gender = \u0026#39;M\u0026#39;; Even though COUNT(*) returns a scalar value, the result is still a two-dimensional table, but with only one row and one column. Other commonly used aggregation functions: MAX(), MIN(), AVG(), SUM(), etc., similar to COUNT().\nGrouped Aggregation Query:\n-- Group by class_id and count records, similar to a for loop: SELECT COUNT(*) num FROM students GROUP BY class_id; -- Include class_id in the result: SELECT class_id, COUNT(*) num FROM students GROUP BY class_id; -- Group by multiple fields, e.g., class_id and gender: SELECT class_id, gender, COUNT(*) num FROM students GROUP BY class_id, gender; Multi-table Query (Cartesian Product):\n-- Querying from students and classes: SELECT * FROM students, classes; -- Set aliases and distinguish columns with table names: SELECT students.id sid, students.name, students.gender, students.score, classes.id cid, classes.name cname FROM students, classes; -- Using aliases for both tables to make it cleaner: SELECT s.id sid, s.name, s.gender, s.score, c.id cid, c.name cname FROM students s, classes c; The result of a multi-table query is still a two-dimensional table, but this table is organized using a Cartesian product, which is why it‚Äôs also known as a Cartesian Query.\nJoin Queries, unlike multi-table queries where the tables are combined using Cartesian products, join queries select one table as the main table and combine it with an auxiliary table based on a relationship:\nInner Join (using INNER): -- Select all students and their corresponding class names: SELECT s.id, s.name, s.class_id, c.name class_name, s.gender, s.score FROM students s INNER JOIN classes c ON s.class_id = c.id; Outer Join, with three variations: RIGHT, LEFT, FULL: -- Using RIGHT OUTER JOIN: SELECT s.id, s.name, s.class_id, c.name class_name, s.gender, s.score FROM students s RIGHT OUTER JOIN classes c ON s.class_id = c.id; Understanding method: You can think of it as sets, where tableA is the main table (also known as the left table) and tableB is the related table (the right table).\n-- Here tableA is the main table, also known as the left table, and tableB is the related table. SELECT ... FROM tableA ??? JOIN tableB ON tableA.column1 = tableB.column2; Modifying Data # Inserting Data: -- Insert a new record: INSERT INTO students (class_id, name, gender, score) VALUES (2, \u0026#39;Daniu\u0026#39;, \u0026#39;M\u0026#39;, 80); -- Insert multiple records at once: INSERT INTO students (class_id, name, gender, score) VALUES (1, \u0026#39;Dabao\u0026#39;, \u0026#39;M\u0026#39;, 87), (2, \u0026#39;Erbao\u0026#39;, \u0026#39;M\u0026#39;, 81), (3, \u0026#39;Sanbao\u0026#39;, \u0026#39;M\u0026#39;, 83); Updating Data: -- Update the record with id=1: UPDATE students SET name=\u0026#39;Daniu\u0026#39;, score=66 WHERE id=1; -- Update records with score \u0026lt; 80: UPDATE students SET score=score+10 WHERE score\u0026lt;80; -- Update record with id=999, but no matching records will be found: UPDATE students SET score=100 WHERE id=999; -- Without a WHERE clause, the update will affect all records in the table: UPDATE students SET score=60; Deleting Data: -- Delete the record with id=1: DELETE FROM students WHERE id=1; -- Deleting without a WHERE clause will remove all records from the table: DELETE FROM students; Here\u0026rsquo;s a fluent and natural English translation:\nCreating Database and Tables # Creating the Database: CREATE DATABASE your_db_name -- name of the database CHARACTER SET utf8mb4 -- character set of the database COLLATE utf8mb4_unicode_ci; -- collation rule Creating Tables: CREATE TABLE table_name ( column1 datatype constraints, column2 datatype constraints, column3 datatype constraints, ... PRIMARY KEY (primary_key_column) ); -- Example of a user table CREATE TABLE users ( id INT AUTO_INCREMENT, username VARCHAR(50) NOT NULL UNIQUE, email VARCHAR(100) NOT NULL UNIQUE, password VARCHAR(255) NOT NULL, created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP, PRIMARY KEY (id) ); Creating TriggersÔºö CREATE TRIGGER trigger_name BEFORE INSERT ON orders FOR EACH ROW BEGIN SET NEW.order_time = NOW(); END; ","date":"15 January 2025","externalUrl":null,"permalink":"/en/blog/mysql-basics/","section":"Blogs","summary":"\u003cdiv class=\"lead text-neutral-500 dark:text-neutral-400 !mb-9 text-xl\"\u003e\n  MySQL Installation and Deployment Process + Quick Syntax CookBook + Study Notes\n\u003c/div\u003e\n\n\n\n\u003ch3 class=\"relative group\"\u003eInformation Source \n    \u003cdiv id=\"information-source\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#information-source\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://liaoxuefeng.com/books/sql/introduction/index.html\" target=\"_blank\"\u003eSQL Tutorial - Liao Xuefeng\u0026rsquo;s Official Website\u003c/a\u003e\u003cbr\u003e\nThis is an extremely user-friendly MySQL tutorial website with an embedded web-based database, making it easy for beginners to get a hands-on understanding of MySQL operations. It also provides concise yet essential explanations of the background of SQL.\u003c/li\u003e\n\u003c/ul\u003e\n\n\n\u003ch3 class=\"relative group\"\u003eMySQL Installation \n    \u003cdiv id=\"mysql-installation\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#mysql-installation\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h3\u003e\n\u003cp\u003eThe simplest way to install MySQL is through Docker Desktop. It takes only two steps to complete:\u003c/p\u003e","title":"MySQL Basics","type":"blog"},{"content":" Familiarizing oneself with the project analysis process, this is a practical analysis report. After I had completed my small plugin, I realized that there doesn\u0026rsquo;t seem to be a one-click solution in the note-to-blog field. So, I wrote this article to analyze whether it would make sense to create a new project to fill this gap.\nThere is no such thing as a \u0026ldquo;better\u0026rdquo; or \u0026ldquo;worse\u0026rdquo; project; there is only whether it is suitable or not. The evaluation criteria in this article are based on whether they meet the requirements of a blog website. Therefore, some projects may not be suitable for creating a blog site, but this does not diminish their value.\nField Definition # Before we begin the analysis, it\u0026rsquo;s important to have a clear understanding of the note-to-blog field.\nNotes: In this context, \u0026ldquo;notes\u0026rdquo; specifically refers to the notes in Obsidian, which use Obsidian Flavored Markdown. This adds certain complexities to the process of converting notes to a webpage.\nBlog: A blog, by definition, is a means to gain traffic. Creators spend time writing notes, and then using conversion software, generate beautifully formatted, feature-rich blog websites.\nEvaluation criteria are as follows:\nPrivacy:\nDoes it run locally? Is it open-source? Usability:\nHow well does it adapt to Obsidian\u0026rsquo;s syntax? How complex is the service deployment? How detailed is the documentation? How easy is it to customize settings? Web Functionality:\nDoes the default web template include all essential functions (search, day/night mode, etc.)? How visually appealing is the default webpage? Does it support SEO? How smoothly does it convert Obsidian\u0026rsquo;s native syntax (for example, are there untranslatable code blocks in internal links, or does it discard some Obsidian syntax features)? Project Overview # In my vision, this project would be an Obsidian plugin that seamlessly exports Obsidian notes into Hugo blog webpages, supporting all of Obsidian\u0026rsquo;s basic core features.\nResult: It greatly reduces the cost of creating a blog webpage, allowing anyone who can use Obsidian to have their own blog.\nMarket and User Feasibility Analysis # Market Demand Analysis # Overview # Basic Needs: The demand to build a personal website and continuously produce content, including for self-improvement, self-expression, and creating a unique and comprehensive personal skill showcase (for corporate recruitment), etc.\nTarget User Group: Heavy Obsidian users who want to share notes; knowledge creators who want to build a personal blog but have abandoned the idea due to technical difficulty.\nRelevant Data # Flowershow: As of October 2024, the plugin had been downloaded 3,355 times; by January 2025, this number had risen to 4,594, while the most downloaded plugin had reached 3,211,992 downloads. Quartz: As of January 2025, Quartz had accumulated 7.7k stars on GitHub. Existing Solutions # Quartz (Hugo) # Recommendation: ‚ù§Ô∏è‚Äçüî•‚ù§Ô∏è‚Äçüî•‚ù§Ô∏è‚Äçüî•‚ù§Ô∏è‚Äçüî•‚ù§Ô∏è‚Äçüî•\nIntroduction # Quartz is a toolset that converts Obsidian notes into web pages. The latest version, v4, has undergone a complete rewrite compared to v3, removing its dependency on Hugo and optimizing the user customization experience. The v4 version is now primarily built with TypeScript, and the original Hugo templates have been replaced with JSX.\nAs a result, Quartz in its current form is almost entirely disconnected from Hugo. However, much of the information available in Internet still advertises Quartz as being built on top of Hugo.\nOfficial example website: Welcome to Quartz 4\nReview # Pros:\nExtremely complete functionality The only toolset that successfully handles display wiki links Detailed documentation Cons:\nVirtually no drawbacks, but one notable limitation is the lack of Chinese documentation. Summary: An excellent project, where the styles displayed in Obsidian are the same as those displayed on the webpage. It has garnered the most stars on GitHub among all available solutions.\nFlowershow # Recommendation: ‚ù§Ô∏è‚Äçüî•‚ù§Ô∏è‚Äçüî•‚ù§Ô∏è‚Äçüî•‚ù§Ô∏è‚Äçüî•ü©∂\nIntroduction # Flowershow is an overall publishing service based on Obsidian, which can convert your Obsidian notes into an online digital garden website with directory structure. Vercel is a cloud service for front-end deployment, enabling serverless front-end deployment via GitHub. Each content submission triggers automatic deployment. For domestic users, Netlify is an alternative.\nSubjectively, the development team behind Flowershow is very passionate and mission-driven. Their core philosophies are detailed in their About page.\nObjectively, Flowershow\u0026rsquo;s positioning as a blog webpage generation platform based on Obsidian is spot-on, and the final result is very good both from a front-end and back-end perspective.\nReview # Pros:\nClear positioning and a straightforward workflow Comprehensive feature support Professional team behind maintenance and operations Highly customizable, suitable for creators who enjoy personalizing their setup Cons (as of January 2025):\nSome Obsidian features are not handled, such as display wiki links. At least this section is omitted in the documentation. A reverse link feature is mentioned on the homepage, but it‚Äôs unclear in the site‚Äôs details. Summary: Overall, the project is well done, but some details still need improvement. This solution is suitable for creators who don\u0026rsquo;t require high support for Obsidian syntax. Official Publish # Recommendation: ‚ù§Ô∏è‚Äçüî•‚ù§Ô∏è‚Äçüî•‚ù§Ô∏è‚Äçüî•ü©∂ü©∂\nIntroduction # Examples of websites using Obsidian\u0026rsquo;s official publishing service:\nObsidian Chinese Tutorial - A Chinese tutorial website, using the official Obsidian publishing service.\nDigital 3D Garden - Deep front-end customizations.\nMister Chad - A simple, neat site with rich content.\nDiscrete Structures for Computer Science - Official simple style.\nReview # Pros:\nThe official publish service offers top-notch support for Obsidian‚Äôs internal representations, ensuring all Obsidian features are correctly displayed on the webpage. Continuous maintenance ensures quick adaptation to updates from Obsidian. Highly customizable settings for users with coding experience, and a wide range of themes from other developers. Privacy settings, password protection, and access control for internal document management. SEO support and mobile platform adaptation for greater potential traffic. Cons:\nCosts $8 per month. Since personal websites typically have very little traffic initially, this can become a significant expense over time. This is the major drawback of the official service. If you stop paying, the website becomes inaccessible. Limited support in certain regions, with traffic constraints in China. Summary: The official service is suitable for users with sufficient funds and moderate customization needs. Digital Garden # Recommendation: ‚ù§Ô∏è‚Äçüî•‚ù§Ô∏è‚Äçüî•‚ù§Ô∏è‚Äçüî•ü©∂ü©∂\nIntroduction # Digital Garden is an Obsidian plugin that exports notes as webpages and hosts them on GitHub, with deployment via Vercel or Netlify.\nExample sites:\nDigital Garden - Official example\nAaron Youn - Created by an individual user\nJohn\u0026rsquo;s Digital Galaxy - Rich content showcasing Digital Garden‚Äôs features, including display links.\nReview # Pros:\nComprehensive feature support Supports Obsidian theme migration Cons:\nNot friendly with Chinese paths Web interface customization requires direct handling of source code (HTML, JavaScript, CSS), and the default interface is not very visually appealing. Summary: The workflow is simple, and the feature support is extensive. However, the interface requires effort to improve, and creators who care less about aesthetics can jump straight into using it. Perlite # Recommendation: ‚ù§Ô∏è‚Äçüî•‚ù§Ô∏è‚Äçüî•ü©∂ü©∂ü©∂\nIntroduction # Perlite is an open-source alternative to Obsidian\u0026rsquo;s official publishing service, providing a browser\n-based file reader with an interface nearly identical to Obsidian‚Äôs.\nReview # Pros:\nSupports almost all Obsidian features. Classic native interface, offering a familiar experience for users. Cons:\nIt is not a blog page but a \u0026ldquo;file reader\u0026rdquo; instead. Requires Docker, which can result in slower startup times compared to the simplicity of a plugin experience. Summary: Perlite is best suited for those who need a browser-based Obsidian experience, rather than as a public-facing blog. Jekyll + Netlify + GitHub Pages # Recommendation: ‚ù§Ô∏è‚Äçüî•‚ù§Ô∏è‚Äçüî•ü©∂ü©∂ü©∂\nIntroduction # This method is derived from obsidian\u0026rsquo;s most perfect free publishing solution.\nExample website by the author: oldwinter‚Äôs Digital Garden\nReview # Pros:\nSimple configuration Highly customizable Cons:\nDoes not support certain Obsidian features like callout syntax No dark mode support No search functionality Summary: A good solution for converting Obsidian to a blog, but missing some core features, making it unsuitable for creators seeking a complete web experience. TiddlyWiki # Recommendation: ‚ù§Ô∏è‚Äçüî•ü©∂ü©∂ü©∂ü©∂\nIntroduction # TiddlyWiki is an older note-taking framework that remains very active today, with developers continuously enhancing it.\nReview # Pros:\nExtremely simple and lightweight\nWidely used with a strong user base\nDomestic services available with no need for VPNs\nCons:\nThe simplicity might result in a somewhat primitive interface. Not a full-fledged personal blog; lacks SEO and is difficult to access via search engines, limiting traffic potential. Summary: TiddlyWiki is ideal for personal note storage but not for creators seeking a blog that attracts traffic. Conclusion # Before conducting a thorough analysis, I was unaware of the actual landscape in the note-to-blog field, which led me to consider creating a simplified plugin. üí° However, after systematic research, I must admit that Quartz stands out as the best project in this space. Whether it\u0026rsquo;s the adaptation to Obsidian\u0026rsquo;s syntax, ease of configuration, front-end aesthetics, customization options, or backend blog creation, there is very little room for improvement. Thus, there is no need for me to initiate a project to duplicate what‚Äôs already been done. I salute all the teams involved in the note-to-blog field, whether mentioned in this article or not. ü´°\nThere are no \u0026ldquo;better\u0026rdquo; or \u0026ldquo;worse\u0026rdquo; projects, only those that are suitable or not. The evaluation criteria in this article focus on whether the solution meets the requirements for a blog webpage, and thus, some projects may not be ideal for blogging but still offer great value.\nSaluting open-source pioneers! ü´°ü´°ü´°\nÂú®ÁªèËøáÂÆåÊï¥ÁöÑÂàÜÊûê‰πãÂâçÔºåÊàëÂπ∂‰∏ç‰∫ÜËß£Á¨îËÆ∞ËΩ¨ÂçöÂÆ¢Ëøô‰∏™È¢ÜÂüüÂÜÖÈÉ®ÁöÑÂÆûÈôÖÊÉÖÂÜµÔºåÂõ†Ê≠§ËêåÁîü‰∫ÜÊÉ≥ÂÅö‰∏Ä‰∏™ÁÆÄÂåñÊèí‰ª∂È°πÁõÆÁöÑÊÉ≥Ê≥ïüí°\n‰ΩÜÊòØÁªèËøáÁ≥ªÁªüÊÄßÁöÑË∞ÉÊü•Á†îÁ©∂ÔºåÊàëÂøÖÈ°ªÊâøËÆ§ Quartz Á°ÆÂÆûÊòØËøô‰∏™È¢ÜÂüüÂÜÖÂá∫Á±ªÊãîËêÉÁöÑÈ°πÁõÆÔºåÊó†ËÆ∫‰ªé Obsidian ËØ≠Ê≥ïÈÄÇÈÖçÊÄß„ÄÅÈÖçÁΩÆÊµÅÁ®ã‰æøÊç∑Á®ãÂ∫¶„ÄÅÂâçÁ´ØÈÖçÂ•óÁïåÈù¢ÁæéËßÇÁ®ãÂ∫¶„ÄÅÂâçÁ´ØÁïåÈù¢Ëá™ÂÆö‰πâ‰æøÊç∑Á®ãÂ∫¶„ÄÅÂêéÁ´ØÊí∞ÂÜôÂçöÂÆ¢‰æøÊç∑Á®ãÂ∫¶Á≠âÁ≠âÂêÑÁßçÊñπÈù¢ÔºåÂá†‰πéÈÉΩÊ≤°Êúâ‰∏äÂçáÁ©∫Èó¥‰∫Ü„ÄÇ\nÂõ†Ê≠§‰πüÂ∞±‰∏çÈúÄË¶ÅÊàëÂÜçÂéªÂêØÂä®‰∏Ä‰∏™È°πÁõÆÊù•ÂÅöÈáçÂ§çÁöÑ‰∫ãÊÉÖ‰∫Ü„ÄÇÂú®ËøôÈáåÂêëÊâÄÊúâÁ¨îËÆ∞ËΩ¨ÂçöÂÆ¢È¢ÜÂüüÂÜÖÁöÑÁõ∏ÂÖ≥È°πÁõÆÁöÑÂºÄÂèëÂõ¢ÈòüËá¥Êï¨ü´°‰∏çÁÆ°ÊòØÂê¶Âú®ÊñáÁ´†‰∏≠ÊèêÂèä„ÄÇ\nÈ°πÁõÆÊ≤°Êúâ‰ºòÂä£‰πãÂàÜÔºåÂè™ÊúâÂêàÈÄÇ‰∏éÂê¶ÔºåËøôÁØáÊñáÁ´†‰∏≠Ê∂âÂèäÁöÑËØÑ‰ª∑Ê†áÂáÜÈÉΩÊòØÂõ¥ÁªïÊòØÂê¶Á¨¶ÂêàÂçöÂÆ¢ÁΩëÈ°µË¶ÅÊ±ÇËøõË°åÁöÑÔºåÂõ†Ê≠§ÂèØËÉΩÊúâ‰∫õÈ°πÁõÆÂπ∂‰∏çÈÄÇÂêàÂÅöÂçöÂÆ¢ÁΩëÈ°µÔºå‰ΩÜÊòØËøô‰∏ùÊØ´‰∏çÂΩ±ÂìçÈ°πÁõÆÊú¨Ë∫´ÁöÑ‰ª∑ÂÄº„ÄÇ\nÂêëÂºÄÊ∫êÂÖàÈîãËá¥Êï¨ü´°ü´°ü´°\n","date":"10 January 2025","externalUrl":null,"permalink":"/en/blog/note-to-blog-report/","section":"Blogs","summary":"\u003cdiv class=\"lead text-neutral-500 dark:text-neutral-400 !mb-9 text-xl\"\u003e\n  Familiarizing oneself with the project analysis process, this is a practical analysis report.\n\u003c/div\u003e\n\n\u003cp\u003eAfter I had completed my \u003ca href=\"https://morethan987.github.io/en/blog/plugin-writing-experience/\"\u003esmall plugin\u003c/a\u003e, I realized that there doesn\u0026rsquo;t seem to be a one-click solution in the \u003cstrong\u003enote-to-blog\u003c/strong\u003e field. So, I wrote this article to analyze whether it would make sense to create a new project to fill this gap.\u003c/p\u003e","title":"Analysis Report on the Note-to-Blog Project","type":"blog"},{"content":" A blog website journey, from hand-coding to Hugo, a story of twists and turns. Why Hugo? # It all started with hearing that Hugo could generate webpages and that it was incredibly efficient at compiling static pages. I decided to dive into the research‚ÄîHugo is said to be the fastest static site generator in the world, as claimed on its official website.\nOf course, words are just words, so here‚Äôs the output I got when I compiled and ran Hugo locally without the public folder at the beginning:\nZH-CN EN Pages 53 51 Paginator pages 0 0 Non-page files 13 13 Static files 7 7 Processed images 3 0 Aliases 18 17 Cleaned 0 0 Built in 872 ms\nIn total, compiling 104 pages (both Chinese and English) took just 0.872 seconds, including the time to build the local server. That speed is hard to criticize. And the local server can listen for changes to the source code in real time and do incremental refactoring, depending on the size of the change, usually around 0.03 seconds.\nI haven\u0026rsquo;t used other page generators for setting up blogs, so I can\u0026rsquo;t compare Hugo\u0026rsquo;s speed with others. References # Here are all the resources I used during the blog setup process:\nËé±ÁâπÈõ∑-letere This is a blogger‚Äôs site also built with Hugo. It contains a lot of tutorials on other web tools as well. The series is also available on Bilibili video tutorial. Blowfish This is the Hugo theme I used, and the documentation is excellent. I‚Äôve never seen such a patient author. Official Hugo Website Hugo Themes Full Deployment Process # Setting Up Hugo # This part is covered in detail in the webpage and video tutorial by the blogger. If you don‚Äôt like reading text, you can follow the video tutorial. üòù\nHonestly, setting up Hugo is one of the easiest setups I\u0026rsquo;ve ever seen, no exaggeration. You simply download Hugo from the official website, place it in a folder, and unzip it. You‚Äôll find just one file, hugo.exe‚Äîit‚Äôs that simple.\nHugo is really convenient. I tried Hexo before, but the Node.js setup turned me away. Even now, I have no idea why it failed to compile. üò¢ The only slight difficulty is adding the directory containing hugo.exe to your environment variables.\nCreating a Template System # Open the terminal in the folder where hugo.exe is located and run the command hugo new site your-site-name. You‚Äôll see a new folder appear in the current directory.\nThe template system sounds advanced, but it‚Äôs just a special folder structure created in the same directory as hugo.exe. You can‚Äôt arbitrarily modify its contents because each folder has a specific purpose.\nName Purpose asset Stores images, icons, and other assets used by the website config Website configuration folder (may not exist initially; some themes require it) hugo.toml One of the website configuration files content All content goes here public The folder containing the fully compiled website (empty initially) themes Stores your website‚Äôs themes Basic Theme Configuration # Hugo has a lot of themes, and you can browse them on Hugo Themes. You can download the theme you like and place it in the themes folder. The process might sound abstract, but you can check out the video tutorial for more guidance.\nHere‚Äôs one important tip: most themes come with a sample site located in the exampleSite folder. If you don‚Äôt want to configure everything from scratch, you can just use the sample configuration.\nAfter configuring the theme, you‚Äôll need to customize it. I highly recommend the Blowfish theme, which is fantastic, and I truly respect the author ü´°.\nBlowfish Theme # The Blowfish official documentation is incredibly detailed, so I won‚Äôt repeat it here. Any additional words would be disrespectful to such a comprehensive guide ü´°.\nHowever, there are some issues you might encounter, and I‚Äôll briefly mention them below. You should carefully read the official documentation to fully understand these points ü§î.\nWhy does the \u0026ldquo;Recent Articles\u0026rdquo; section still show up even if params.homepage.showRecent = false is set? If you face this issue, it‚Äôs likely because, like me, you lazily used the exampleSite configuration. This is because the homepage layout is controlled by more than one interface, and another one is located in the layouts\\partials\\home\\custom.html file.\nIf you don‚Äôt mind, just ignore it. But if you care (like I did ü§™), you can comment out the following code in the file:\n\u0026lt;section\u0026gt; {{ partial \u0026#34;recent-articles-demo.html\u0026#34; . }} \u0026lt;/section\u0026gt; Why doesn‚Äôt the logo change between day and night modes when I use an svg logo? This is a bug I discovered, and I‚Äôve already submitted an improvement to the theme author. See code improvement or SVG support\nWhy does the small icon in the browser window still show the blowfish logo even after I change the site‚Äôs logo? This is mentioned in the official documentation, but it‚Äôs buried deep. You can find it under Partials Documentation for Blowfish.\nTo be honest, the official documentation is excellent. üëç After going through the entire process, I only encountered a few minor issues that were not easy to understand üòã.\nPlugins I Use # I prefer using Obsidian for writing articles. However, the format used by Obsidian and the one used by Blowfish is quite different, so converting between the two can be a hassle ü§î.\nAfter searching around, I found that there weren‚Äôt any suitable plugins! So, I developed my own plugin: Hugo-Blowfish-Exporter.\nWhile the plugin is simple, it covers almost all of my needs, including:\n- Callouts (supports all official callout names, with additional icons)\n- Inline math formulas (Blowfish supports block-level formulas)\n- Mermaid (supports Mermaid diagrams)\n- Image embedding (automatically exports images)\n- Wiki-style links (only support the none-displayed link üò¢)\nThe none-displayed link is simply exported as normal hyperlink in the HTML file;\nThe displayed link is more complex: change(override) the source code of Blowfish to support the file injection through the shortcode, mdimporter; every Obsidian file should includes a meta data slug to tag the folder that contains the target markdown file in your website repository.\nThe overriding of the theme\u0026rsquo;s source code can be found in the mdimporter and the stripFrontMatter used to remove metadata from the injected files\u0026rsquo; headers. For overriding the directory, refer to the configuration on GitHub.\nI put a lot of effort into this plugin, even though it only took a few days ü§î. But those few days were quite exhausting üòµ‚Äçüí´.\nIf this plugin helps you, feel free to share it! If you‚Äôre not happy with the functionality, you can submit an issue on GitHub ü´°. If you\u0026rsquo;re familiar with the code, you can modify it directly; the code is well-commented and quite standard ü§ó.\nAnd if you modify and upgrade the code, I‚Äôd be very grateful if you share your changes with me (via a pull request on GitHub)! ‚ò∫Ô∏è\nFinal Thoughts # Setting up a blog site is just the first step in a long journey; the real challenge is filling it with content.\nAs I mentioned in An experience of writing plugins, many personal blogs fade into obscurity in as little as a year, from the initial burst of excitement to the eventual silence.\nIn this fast-paced world, most meaningless and inefficient things are eventually replaced by efficiency, and the original enthusiasm and dreams often compromise with reality. I too no longer have the passion I once had, and my actions have become more like those of a real adult.\nBut there\u0026rsquo;s still a bit of unwillingness in me. This website is a form of resistance, and I‚Äôll do my best to maintain it. That‚Äôs also why I developed the plugin‚Äîto make updating the blog easier.\nI hope this tutorial helps anyone planning to set up their own blog. Let‚Äôs keep moving forward, together ü´°.\nÂõ†Ê≠§Â∏åÊúõËøôÁØáÊïôÁ®ãÊâÄÊèê‰æõÁöÑÂÜÖÂÆπËÉΩÂ§üÂ∏ÆÂä©Âà∞Ê≠£Âú®ÂáÜÂ§áÊê≠Âª∫Ëá™Â∑±ÁöÑÂçöÂÆ¢ÁΩëÁ´ôÁöÑ‰Ω†Ôºå‰Ω†ÊàëÂÖ±Âãâü´°\n","date":"7 January 2025","externalUrl":null,"permalink":"/en/blog/hugo-blog/","section":"Blogs","summary":"\u003cdiv class=\"lead text-neutral-500 dark:text-neutral-400 !mb-9 text-xl\"\u003e\n  A blog website journey, from hand-coding to Hugo, a story of twists and turns.\n\u003c/div\u003e\n\n\n\n\u003ch2 class=\"relative group\"\u003eWhy Hugo? \n    \u003cdiv id=\"why-hugo\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#why-hugo\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h2\u003e\n\u003cp\u003eIt all started with hearing that Hugo could generate webpages and that it was incredibly efficient at compiling static pages. I decided to dive into the research‚ÄîHugo is said to be the \u003cstrong\u003efastest static site generator\u003c/strong\u003e in the world, as claimed on its official website.\u003c/p\u003e","title":"Hugo Blog Setup","type":"blog"},{"content":" A Reflection on Writing a Plugin and What I Learned from the Experience. The Beginning # It all started with my blog website. I stumbled upon an article on WeChat about building a blog with Hugo, and since I wanted to revamp my old, simple site, I decided to give it a try. My original site was extremely rudimentary, and the whole writing process involved jumping between HTML, JS, and CSS in a rather awkward manner. On top of that, I had always admired the blog of a great tech guru, Lilian Weng, which was also built with Hugo. This further strengthened my resolve to change my site\u0026rsquo;s underlying platform.\nSo, I quickly started diving into Hugo.\nTo my surprise, the results were extraordinary! My old webpage took me nearly a month to build, but with Hugo, I was able to finish everything in less than half a day. What shocked me even more was that Hugo, a program written in Go, didn‚Äôt require users to set up a Go environment! üòÆ\nAt the same time, I discovered an incredibly well-documented Hugo theme‚ÄîBlowfish. This was by far the most detailed documentation I had ever seen for any project, bar none (‡πë‚Ä¢ÃÄ„ÖÇ‚Ä¢ÃÅ)Ÿà‚úß.\nWith Hugo and Blowfish working in tandem, my small site quickly took shape. Of course, I‚Äôm not great at designing, so I just used the default layout from Blowfish, as I felt any changes would ruin the beauty of the page.\nTo be honest, after all this work, I didn‚Äôt have any strong emotional reactions, except for deep respect for the coding skills of the authors of Hugo and Blowfish.\nThat was until I wanted to upload the massive amount of notes I had in Obsidian to my new blog.\nThe Bitter Taste of Originality # I soon realized that there wasn‚Äôt a plugin available to directly convert the format of my Obsidian notes to fit the Blowfish theme. Fueled by the earlier \u0026ldquo;pleasant experience,\u0026rdquo; I decided to write a plugin myself! (üòÑ Although, I would soon stop laughing üò¢)\nThe rest of the experience wasn‚Äôt anything particularly exciting‚Äîjust endless switching between webpages, searching through API documentation, and never-ending conversations with AI bots. After countless revisions, I finally ended up with something exceedingly simple: a plugin that identifies specific patterns in documents and performs content replacement.\nIt was quite laughable. Compared to the few hours it took to set up the website, the nearly forty hours I spent writing that plugin felt almost negligible. At one point, I seriously considered just deleting my few hundred lines of code.\nYes, such a simple plugin drained me mentally and physically. I truly tasted the bitterness of originality.\nNow, looking back at Hugo and Blowfish, I feel deeply shocked by their complexity and the effort required to implement all of those features. If they were getting paid for this work, I could at least understand the level of effort involved. But they were both open-source, relying entirely on user goodwill and appreciation.\nI saw the last update of the Blowfish author‚Äôs blog, which was in March 2024, and I fell into deep thought.\nSentiments and Idealism # I imagine that the author of Blowfish must have paused the development of the theme for some reason‚Äîperhaps due to life circumstances. After all, this project didn‚Äôt bring in much real income.\nSuddenly, I remembered the changes I had noticed before‚Äîthose GitHub profiles, once full of green squares, gradually becoming sparse, and eventually disappearing. Beneath this peaceful change, there might be a shift in someone\u0026rsquo;s life. Whether it\u0026rsquo;s because of busy work or the gradual fading of motivation, the original passionate drive eventually drowns in silence. I can\u0026rsquo;t stop this from happening, but I understand the reasons behind it.\nOpen-source is driven by passion, but passion doesn‚Äôt pay the bills. People need to live in the present.\nI recalled a tech YouTuber, Ma Nong Gao Tian, a core Python developer who humorously complained about the harsh realities of open-source life. His prematurely graying hair made me feel a pang of empathy‚Äîhe had spent most of his life writing code and yet found himself out of work, surviving on a few extra bucks from his videos.\nIn Conclusion # Life is rarely as we wish. Once again, I looked at my forty-plus hours of work and couldn‚Äôt help but laugh and shake my head.\nAfter writing this, I‚Äôm off to bed. It‚Äôs now 1:48 AM on January 6, 2025, and I still haven‚Äôt reviewed for my English final exam tomorrow.\nLooking at this blog again, I laughed and shook my head.\nSuch is life.\n","date":"6 January 2025","externalUrl":null,"permalink":"/en/blog/plugin-writing-experience/","section":"Blogs","summary":"\u003cdiv class=\"lead text-neutral-500 dark:text-neutral-400 !mb-9 text-xl\"\u003e\n  A Reflection on Writing a Plugin and What I Learned from the Experience.\n\u003c/div\u003e\n\n\n\n\u003ch3 class=\"relative group\"\u003eThe Beginning \n    \u003cdiv id=\"the-beginning\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#the-beginning\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h3\u003e\n\u003cp\u003eIt all started with my blog website. I stumbled upon an article on WeChat about building a blog with \u003ccode\u003eHugo\u003c/code\u003e, and since I wanted to revamp my old, simple site, I decided to give it a try. My original site was extremely rudimentary, and the whole writing process involved jumping between \u003ccode\u003eHTML\u003c/code\u003e, \u003ccode\u003eJS\u003c/code\u003e, and \u003ccode\u003eCSS\u003c/code\u003e in a rather awkward manner. On top of that, I had always admired the \u003ca href=\"https://lilianweng.github.io/\" target=\"_blank\"\u003eblog\u003c/a\u003e of a great tech guru, \u003ccode\u003eLilian Weng\u003c/code\u003e, which was also built with \u003ccode\u003eHugo\u003c/code\u003e. This further strengthened my resolve to change my site\u0026rsquo;s underlying platform.\u003c/p\u003e","title":"An experience of writing plugin","type":"blog"},{"content":"Morethan\u0026rsquo;s dummy blog page~\n","date":"3 January 2025","externalUrl":null,"permalink":"/en/blog/moravecs-paradox/","section":"Blogs","summary":"\u003cp\u003eMorethan\u0026rsquo;s dummy blog page~\u003c/p\u003e","title":"Reflections on Moravec's Paradox","type":"blog"},{"content":" Preface # This article is primarily a review and summary of the entire process of CUMCM 2024.\nOur team was formed in the winter of 2023, and CUMCM 2024 was our first participation in the \u0026ldquo;Mathematical Modeling\u0026rdquo; competition. After numerous mock contests, we finally made it to the national competition. After submitting the final paper, we won the first prize at the provincial level and were recommended for the first prize at the national level, ultimately receiving the second prize at the national level.\nThere were moments of excitement and surprise, as well as disappointment; we must have done some things right in the competition, which is why we won a national award in our first attempt; but there are definitely shortcomings, after all, there must be a reason for going from \u0026ldquo;recommended for the first prize at the national level\u0026rdquo; to \u0026ldquo;second prize at the national level\u0026rdquo;.\nIn short, this experience is truly unforgettable, and it is even more worth summarizing and learning from the experience to prepare for next year\u0026rsquo;s competition.\nCUMCM stands for Chinese Undergraduate Mathematical Contest in Modeling; it is commonly referred to as the \u0026ldquo;National Mathematical Modeling Competition\u0026rdquo;. Terminology Explanation # Term Explanation Computational System The traditional modeling process, encapsulating a large function Optimization System A system used to optimize the adjustable parameters in the computational system to generate the best parameter configuration Computational Flow The process of handling input data in the computational system Computational Flow Node A key intermediate step in the workflow Optimization Flow The main logic of the optimization system Main Body of the Paper Includes the abstract, restatement, descriptions of computational and optimization flows, results presentation and analysis, that is, all content before the conclusion of the paper Conclusion of the Paper Includes sensitivity analysis and model extension Objective Conditions # Task Division # Although there were many topics to choose from for the competition, our group chose to focus on optimization problems, which is Topic A.\nMe: Modeling + Coding + Part of Paper Writing CL: Modeling + Paper Writing + Part of Coding HWJ: Paper Beautification Workflow # The coding part of the entire Topic A can be roughly divided into two systems:\nComputational System: Function: Accept input data and parameters, return the required results Nature: Directly determined by the problem, different topics have different computational systems, which need to be constructed temporarily Optimization System: Function: Accept the computational system as the target function to be optimized, execute its own optimization logic, and finally return the computational results Nature: The method system is relatively mature and can be prepared in advance of the competition with various optimization systems The paper writing part is divided into:\nOverall Framework: Determined by the LaTeX template Main Content Filling: Clear description of the workflow and optimization flow Typesetting and Beautification: Adjust the details of each part, with illustrative images (flowcharts, schematics) Concluding Content Pre-Modeling # Objective: Under the premise of accurately understanding the problem, quickly carry out preliminary modeling, basically determine the direction of modeling and calculation methods;\nEstimated Time: 3 hours\nWork: All team members conduct a web search to see if there are any literature materials that basically hit the topic.\nHit Successful: The most ideal situation, at this time, you can directly study the papers and collect ideas; Hit Unsuccessful: Although there are no ready-made materials for reference, some ideas have been accumulated in the process of literature review. Early Modeling # Overall Objective: Construct a precise and optimization method adaptable computational system\nModeling: Clarify the operations between input data and each computational flow node Coding: Implement the computational flow with code and achieve data visualization Paper: Fill in the content of the first question and initially typeset Estimated Time: 30 hours\nWork:\nAll team members model together, first clarify the modeling ideas, and provide a complete mathematical derivation process Me and CL: Code implementation and paper content filling are carried out simultaneously HWJ: Draw more vivid schematic diagrams that cannot be generated by code Mid-Modeling # Overall Objective: Construct a suitable optimization system\nModeling: According to the particularity of the computational system, choose the most matching optimization system Coding: Make minor changes in the implementation of the optimization system to match the computational system Paper: Complete the main part of the paper and start local detail fine-tuning Estimated Time: 20 hours\nWork: Similar to the previous, but the focus of work has shifted from code writing to paper writing\nSimplify the paper, at this time, the paper is very bloated Fine-tune the logic of the paper to make the context more closely related Beautify the typesetting, reduce text, increase images Late Modeling # The basic modeling is completed, and all members check for loopholes: Conventional checks such as typos, inaccurate expressions, formula spelling errors, etc. Optimize code comments to make them more readable Focus on checking personal information Personal information must not be retained in the competition paper, including file paths in the code, such as C:\\Users\\Morethan; retaining personal information is a very serious mistake! Actual Combat Effectiveness # When we applied the above strategies to the actual combat process, that is, the formal competition of CUMCM 2024, the results were as follows:\nEffective Time: The total duration of the competition is three days, a total of 72 hours The team works from seven in the morning to eight in the evening, excluding meal times, with an effective time of 12 hours a day Time utilization rate is \\(50\\%\\) (quite low in comparisonü§î) Completed Work: The main body of the paper is 28 A4 pages The code part is 35 A4 pages, excluding the reused code between each sub-question, there should be about 20 pages A total of 25 illustrations in the paper The above data is after the paper has been streamlined, with the initial draft of the paper being nearly 50 pages Uncompleted Work: The final result calculation, due to the large amount of calculation (the code efficiency is not high), the code was finished two hours in advance, but there was not enough time to calculate the resultsüò≠üò≠ The calculation accuracy of the model is not enough, the accuracy is 1s which does not meet the standard answer\u0026rsquo;s precision The conclusion part of the paper was not actually completed Strengths # Topic Selection # Focused on Topic A, accumulated sufficient experience in mock contests, and polished a set of efficient workflows\nThe methodology for Topic A is relatively well-constructed\nWorkflow # The workflow is relatively clear, and the efficiency is high\nGuided by the final paper, modeling, paper, and code are carried out simultaneously, ensuring sufficient content in the paper\nDivision of Labor # Adopted a blurred division of labor, each team member has a main job and a secondary job, can work independently on their main job, and can also complete some work on the secondary job, greatly improving time utilization\nThe team members are very capable, as handling two division tasks means more learning costs\nWeaknesses # Workflow # The plan is perfect, but some necessary links were not well done in practice\nEffective time ratio: finishing work at eight in the evening is too early! More time should be taken to model trial and error to ensure the correctness and accuracy of the model\nDivision of Labor # The code writing, code debugging, code visualization, result calculation, and result visualization involve too much code, which is difficult for one person to handle;\nTask overlap caused by blurred division of labor increases collaboration costs\nModeling # Topic understanding accuracy: This time, there was a significant deviation in our understanding of the topic, which led to wasting a lot of time on model correction; Code # Code efficiency: Due to no time limit before, there was insufficient preparation for \u0026ldquo;very long\u0026rdquo; code, no experience with code parallelism;\nResult precision: The initial modeling was too rough, and a bad characteristic was used: setting the time step to 1, and using it as an array index, which made it difficult to reduce the time step later, resulting in insufficient precision of the final results\nImprovement Plans # Carefully select the venue, increasing effective time‚ú®is the most important‚ú® Division of Labor # Slightly change the division of labor, increase the investment of human resources in coding\nIncrease learning input in each main and secondary division to increase work efficiency\nModeling # Focus more on understanding the topic, don\u0026rsquo;t rush; correcting modeling errors is not worth the loss Code # Build a set of effective code collaboration plans to enhance code writing speed\nStart building code writing standards:\nVariable naming Documentation at the beginning of the file Code writing process standards Code parallelization: Add some parallelizable code to the code to increase running speed\nAll code improvements must be implemented in a document! Not just slogans! The final output: Code Collaboration Scheme Paper # Study excellent papers\nPay attention to its paper framework Pay attention to its language style, text readability, detail, illustration logic, and image readability Improve ourselves\nOptimize the paper\u0026rsquo;s main logic framework, refine the content of each section Improvements in language style, text readability, detail, illustration logic, and image readability, etc. The results are fixed in the form of comments in the LaTeX template! Summary # A test paper without full marks is more rewarding than one with full marks!\nAccumulating knowledge of applied mathematics, enhancing paper writing skills, and improving the ability to discover problems are more meaningful than the competition itself. ü´°\nCUMCM, every MathModeler can benefit from it. ü§ó\n","date":"12 September 2024","externalUrl":null,"permalink":"/en/blog/cumcm2024/","section":"Blogs","summary":"\u003ch2 class=\"relative group\"\u003ePreface \n    \u003cdiv id=\"preface\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#preface\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h2\u003e\n\u003cp\u003eThis article is primarily a review and summary of the entire process of CUMCM 2024.\u003c/p\u003e","title":"CUMCM 2024 Summary","type":"blog"},{"content":" Creating Virtual Environments # Standard Python Operations # Creation # Here are some common code examples üëá\n# Create a virtual environment python -m venv your_env_name # Create a virtual environment with a specific Python version (if Python is installed in the default location) python -m venv your_env_name --python=python3.11 # If Python is installed in a custom location D:\\Python\\Python311\\python.exe -m venv your_env_name Below are some optional parameters for creating customized virtual environments:\nParameter Description --system-site-packages Includes packages from the global Python environment to avoid redundant installations --clear Clears the target directory if it already exists before creating the virtual environment --version Confirms the Python version in the virtual environment All parameter descriptions can be obtained by running python -m venv -h‚Äîno need to search elsewhere for documentation~üòÜ Activation # By default, the virtual environment is inactive. In the \u0026ldquo;your_env_name/Scripts/\u0026rdquo; directory, there will be a file named \u0026ldquo;activate,\u0026rdquo; which can be executed via the command line.\n# Activate the virtual environment your_env_name/Scripts/activate Poetry # Poetry is a Python package management tool. According to its changelog, it has been in development since February 2018, making it not a new tool but one that has gained significant popularity in recent years üí´.\nIts advertised highlights include:\nMore comprehensive third-party dependency analysis Automated virtual environment creation More intuitive commands ü§î Installation # Straightforward: pip install poetry for a global installation.\nGlobal Configuration # Poetry has some unique mechanisms that may differ from tools like pip. You might need to configure certain settings as follows:\n# List all configuration items poetry config --list # By default, Poetry creates virtual environments in a dedicated folder rather than the project directory # Use the following command to create the environment folder in the project directory poetry config virtualenvs.in-project true # Poetry can automatically create virtual environments when none exist in the project # For reassurance, you can run this command poetry config virtualenvs.create true # Switch to a domestic mirror poetry config repositories.tencentyun https://mirrors.tencentyun.com/pypi/simple # Other settings can generally be left as defaults Initializing a Project # If you\u0026rsquo;re starting a new project from scratch, you\u0026rsquo;ll need to: create a project folder, navigate to it in the command line, and simply run:\npoetry init An interactive command-line interface will guide you through creating a new project, generating a pyproject.toml file to record dependencies, and creating Poetry\u0026rsquo;s signature poetry.lock file to lock package versions. Those familiar with Node will find this familiar üòù.\nIf you\u0026rsquo;re using someone else\u0026rsquo;s project, you\u0026rsquo;ll typically only find the pyproject.toml file, which is compatible across different package management tools. Some authors may also include the poetry.lock file. In such cases, you can choose to: 1) delete the lock file and use another package manager, or 2) keep the lock file and use Poetry to manage dependencies.\nIf you need a simple directory structure, you can use:\npoetry new my-package This will automatically create a directory structure suitable for most projects:\nmy-package ‚îú‚îÄ‚îÄ pyproject.toml ‚îú‚îÄ‚îÄ README.md ‚îú‚îÄ‚îÄ my_package ‚îÇ ‚îî‚îÄ‚îÄ __init__.py ‚îî‚îÄ‚îÄ tests ‚îî‚îÄ‚îÄ __init__.py For more complex needs: new Command.\nCreating a Virtual Environment # Here are a few scenarios to consider.\nYou\u0026rsquo;re building your project from scratch: poetry env use python # Remember how you installed Poetry? This uses the default Python version. poetry env use python3.7 # Uses Python 3.7 from your system. poetry env use 3.7 # For the lazy. poetry env use /full/path/to/python # If the shorthand doesn\u0026#39;t work, use this. You\u0026rsquo;re using someone else\u0026rsquo;s project: There are two very similar commands, and the comments below highlight their differences. The official recommendation is the sync command, as it avoids installing packages not tracked by poetry.lock, which might have been accidentally added by the original developer.\nHowever, in practice, install is often more reliable, as sync can sometimes produce strange errors, like removing itself and crashing the command line ü§î.\npoetry install # Automatically creates a virtual environment and installs all dependencies listed in pyproject.toml. # Ensure virtualenvs.create is set to true. # Automatically creates a virtual environment and installs all dependencies listed in poetry.lock, removing any extra packages from pyproject.toml. poetry sync Then, enter the virtual environment with:\npoetry shell Adding Packages # Poetry allows developers to distinguish between production and development dependencies.\n# Defaults to the production group. poetry add your-package # Adds to the development group. poetry add your-package -D Updating Packages # poetry update # Automatically analyzes dependencies and updates all possible packages. poetry update your-package1 your-package2 # Updates only the listed packages. Removing Packages # This is one of Poetry\u0026rsquo;s standout features: pip installs a package and its dependencies but only removes the specified package, leaving its dependencies behind.\nPoetry, however, can safely and completely remove a package and its dependencies without affecting other packages.\n# Removes a production dependency. poetry remove your-package # Removes a development dependency. poetry remove your-package -D Displaying Dependencies # poetry show --tree # Displays the dependency tree from pyproject.toml. poetry show your-package --tree # Displays the dependency tree for a specific package. UV # A package management tool written in Rust, with a command structure very similar to poetry.\nInstallation # First, configure environment variables:\ncd ~ # Ensure you\u0026#39;re in the default directory. ls -a # Check if the .bashrc file exists. vim .bashrc # Edit the file with vim. # Append these three lines to the end of the file. # Domestic mirror for the installer. export UV_INSTALLER_GHE_BASE_URL=https://ghproxy.cn/https://github.com # Domestic mirror for Python installation. export UV_PYTHON_INSTALL_MIRROR=https://ghproxy.cn/https://github.com/indygreg/python-build-standalone/releases/download # Domestic mirror for Python packages. export UV_DEFAULT_INDEX=https://mirrors.tuna.tsinghua.edu.cn/pypi/web/simple Installation via curl (recommended): # Replace \u0026#34;/custom/path\u0026#34; with your desired installation location. curl -LsSf https://astral.sh/uv/install.sh | env UV_INSTALL_DIR=\u0026#34;/custom/path\u0026#34; sh Installation via pip/pipx (recommended): pipx install uv # Install uv using pipx. pip install uv # Install uv using pip. Usage # Usage is very similar to poetry.\nConda # I personally don\u0026rsquo;t use Conda much for environment management, but since many lab servers default to using Conda, I‚Äôll document the basics here for reference.\nUsage # The following commands are fundamental for checking environment information. They‚Äôre typically used when accessing a new server environment or troubleshooting environment-related issues.\n########## Checking Environment Information ########## arch # View the system architecture conda --version # Get the Conda version python --version # Get the Python version conda env list # List all available environments conda activate xxx # Activate the virtual environment named \u0026#34;xxx\u0026#34; conda deactivate # Exit the current virtual environment whereis python # Check the location of Python in the currently activated environment nvidia-smi # Check CUDA status (not a Conda command but frequently used) conda config --show channels # View Conda\u0026#39;s download sources conda remove -n xxx # Delete the virtual environment named \u0026#34;xxx\u0026#34; conda create -n env_name # Create a virtual environment with the specified name, using the default Python version Adding domestic (Chinese) download sources:\nconda config --show channels # View Conda\u0026#39;s download sources # Add Tsinghua\u0026#39;s mirror source conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/msys2 Managing Python packages:\n# Install a specific package conda install package_name # Remove a specific package conda remove package_name # Update a specific package conda update package_name # Search for a specific package conda search package_name # List all installed packages and their versions conda list # Clean unused packages and cache to save disk space conda clean --all # Install packages from a requirements.txt file conda install -f requirements.txt Manual installation of dependency packages for handling extreme offline situations:\n# Directly use pip download command to download all related whl files pip download scikit-image --dest ./scikit_image_files --only-binary :all: --python-version 3.13 --platform manylinux_2_17_x86_64 --implementation cp --abi cp313 # Manually transfer to the server, switch to the target folder, and run (note the whl files) # --find-links specifies the folder containing the whl files pip install scikit_image-0.25.2-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl --no-index --find-links=./ Do not rename the whl files üò¢ Passwordless Login via VSCode # For students without a local GPU, editing and running files directly on a server is very convenient. The VSCode extension remote-ssh can also display a VSCode interface directly on the server. To avoid the hassle of repeatedly entering passwords, you can set up passwordless login as follows üòÑ.\nFor a local Windows machine, SSH-related configuration files are usually stored in the C:\\Users\\\u0026lt;User_name\u0026gt;\\.ssh folder. Passwordless login can be achieved by modifying files in this directory.\nConfigure the config file. A sample configuration is as follows: # Replace \u0026lt;User_name\u0026gt; with your local machine\u0026#39;s username Host 3090 HostName xx.xxx.xx.xx User morethan IdentityFile \u0026#34;C:\\Users\\\u0026lt;User_name\u0026gt;\\.ssh\\id_rsa\u0026#34; Generate the authentication file id_rsa.pub.\nLocally create an authorized_keys file and copy the contents of id_rsa.pub into it.\nOn the server, create a .ssh folder in the default directory and copy the authorized_keys file from your local machine into it.\nNow, when logging into the server via VSCode, you‚Äôll find that passwordless login is enabled.\nThe steps above only need to be performed once initially. When configuring passwordless login for another server, you only need to modify the config file and copy the authorized_keys file to the server‚Äîno further file modifications are required üòÑ. Packaging Applications # We often need to share Python programs we\u0026rsquo;ve written. However, sharing only the source code can be frustrating for users unfamiliar with coding, as running the source requires setting up a local environment. Hence, packaging tools come into play.\npyinstaller # Installation is straightforward, just like any other Python package: pip install pyinstaller. To use it, navigate to the directory containing the target file in the terminal and run the command.\nFeatures: Fast packaging speed; larger resulting executable size.\nCommon commands:\n# Package main.py into a standalone executable; disable the console window when running. pyinstaller -F -w main.py # Package main.py into a project folder; enable the console window when running. pyinstaller -D main.py Parameter Description -h, --help Show help message and exit. -v, --version Show program version and exit. -F, --onefile Package everything into a single executable. -D, --onedir Package everything into a directory (default). -w, --windowed, --noconsole Disable the console window (Windows only). -c, --console, --nowindowed Run the program with a console window (default, Windows only). -a, --ascii Exclude Unicode character set support (included by default). -d, --debug Generate a debug version of the executable. -n NAME, --name=NAME Specify the name of the generated executable or directory (default: script name). -o DIR, --out=DIR Specify the directory for the spec file (default: current directory). -p DIR, --path=DIR Set the Python import path (similar to PYTHONPATH). -i \u0026lt;FILE\u0026gt;, --icon \u0026lt;FILE\u0026gt; Set the executable\u0026rsquo;s icon (supports .ico or .icns formats). --distpath DIR Specify the output directory for the executable (default: ./dist). --workpath WORKPATH Specify the directory for temporary files (default: ./build). --add-data \u0026lt;SRC;DEST or SRC:DEST\u0026gt; Add additional data files or directories (Windows uses semicolons; Linux/OSX uses colons). --add-binary \u0026lt;SRC;DEST or SRC:DEST\u0026gt; Add additional binary files. --hidden-import MODULENAME Add modules not automatically detected. --exclude-module EXCLUDES Exclude specified modules. --clean Clean PyInstaller cache and temporary files. --log-level LEVEL Set verbosity for console messages (options: TRACE, DEBUG, INFO, WARN, ERROR, FATAL). Nuitka # Converts Python code into an exe executable by first translating Python to C and then compiling the C code.\nFeatures: Very slow packaging speed; requires a C compiler (though installation can be automated, it may not suit users with strict memory constraints); resulting executable is very small (about one-tenth the size of pyinstaller\u0026rsquo;s output).\nInstallation command:\npip install -U nuitka Common usage commands:\n# Package main.py into a single exe file with link-time optimization and clean up temporary files afterward. python -m nuitka --lto=yes --remove-output --onefile main.py Parameter Description --standalone Creates a standalone executable with all dependencies. --onefile Packages everything into a single .exe file. --optimize=N Sets optimization level (0, 1, or 2); higher numbers mean more optimization. --lto Enables Link Time Optimization (options: no, yes, or thin). --enable-plugin=\u0026lt;plugin_name\u0026gt; Enables specified plugins (e.g., tk-inter, numpy, anti-bloat). --output-dir=\u0026lt;dir\u0026gt; Specifies the output directory for compilation. --remove-output Deletes intermediate .c files and other temporary files after compilation. --nofollow-imports Does not recursively process any imported modules. --include-package=\u0026lt;package_name\u0026gt; Explicitly includes an entire package and its submodules. --include-module=\u0026lt;module_name\u0026gt; Explicitly includes a single module. --follow-import-to=\u0026lt;module/package\u0026gt; Specifies modules or packages to process recursively. --nofollow-import-to=\u0026lt;module/package\u0026gt; Specifies modules or packages to exclude from recursive processing. --include-data-files=\u0026lt;source\u0026gt;=\u0026lt;dest\u0026gt; Includes specified data files. --include-data-dir=\u0026lt;directory\u0026gt; Includes all data files in a directory. --noinclude-data-files=\u0026lt;pattern\u0026gt; Excludes data files matching a pattern. --windows-icon-from-ico=\u0026lt;path\u0026gt; Sets the icon for the Windows executable. --company-name, --product-name, --file-version, --product-version, --file-description Sets properties for the Windows executable. References # Poetry-related:\nPoetry Beginner\u0026rsquo;s Guide_Using Poetry-CSDN Blog Very detailed resource. Poetry How to Change Poetry\u0026rsquo;s Domestic Mirror - Data Science SourceResearch UV-related:\nInstalling and Using Python Project and Package Manager UV - Deep Sea Xiao Tao One of the few blogs that includes the domestic mirror address for uv python install. Conda-relatedÔºö\nSummary of Commonly Used Conda Commands - Zhihu ","date":"10 August 2024","externalUrl":null,"permalink":"/en/blog/pytips/","section":"Blogs","summary":"\u003ch2 class=\"relative group\"\u003eCreating Virtual Environments \n    \u003cdiv id=\"creating-virtual-environments\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#creating-virtual-environments\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h2\u003e\n\n\n\u003ch3 class=\"relative group\"\u003eStandard Python Operations \n    \u003cdiv id=\"standard-python-operations\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#standard-python-operations\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h3\u003e\n\n\n\u003ch4 class=\"relative group\"\u003eCreation \n    \u003cdiv id=\"creation\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#creation\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h4\u003e\n\u003cp\u003eHere are some common code examples üëá\u003c/p\u003e","title":"Python Tips and Tricks","type":"blog"},{"content":" References # My understanding of Bayesian optimization is also limited, mainly referencing the following contentüëá\n„ÄêMachine Learning„ÄëUnderstanding Bayesian Optimization in One Article\nA Detailed Explanation of Bayesian Optimization Principles\nBayesian Optimization\nHyperparameter Optimization‚ÄîBayesian Optimization and Its Improvement (PBT Optimization)\nBayesian Optimization\nMATLAB Official Documentation\nExploring Bayesian Optimization\nAdvantages and Algorithm Principles # This section focuses on the advantages of Bayesian optimization and its algorithmic principles. If you are only interested in \u0026ldquo;how to use it,\u0026rdquo; you can first learn about the advantages of Bayesian optimization and then skip to MATLAB Usage.\nAdvantages # Algorithm Principles # MATLAB Usage # Code Overview # % Define the objective function function y = objectiveFcn(x) y = (1 - x.x1)^2 + 100 * (x.x2 - x.x1^2)^2; end % Define optimization variables vars = [optimizableVariable(\u0026#39;x1\u0026#39;, [-2, 2]) optimizableVariable(\u0026#39;x2\u0026#39;, [-2, 2])]; % Perform Bayesian optimization results = bayesopt(@objectiveFcn, vars, ... \u0026#39;AcquisitionFunctionName\u0026#39;, \u0026#39;expected-improvement-plus\u0026#39;, ... \u0026#39;MaxObjectiveEvaluations\u0026#39;, 30, ... \u0026#39;IsObjectiveDeterministic\u0026#39;, true, ... \u0026#39;Verbose\u0026#39;, 1); % View results bestPoint = results.XAtMinObjective; bestObjective = results.MinObjective; fprintf(\u0026#39;Optimal solution x1: %.4f, x2: %.4f\\n\u0026#39;, bestPoint.x1, bestPoint.x2); fprintf(\u0026#39;Optimal objective value: %.4f\\n\u0026#39;, bestObjective); Parameter Explanation # Params Meaning AcquisitionFunctionName Selects the acquisition function, which determines how the algorithm chooses the next sampling point after each iteration. MaxObjectiveEvaluations Maximum number of iterations. IsObjectiveDeterministic Set to true if the objective function is deterministic (no noise); otherwise, set to false. Verbose Controls the verbosity of the output, which may include multiple charts. For specific options for each parameter, refer to the official documentation: bayesopt. The official documentation is very detailed and includes many examples.\nOne of the essential skills for mathematical modelers is reading documentation üòù ","date":"5 August 2024","externalUrl":null,"permalink":"/en/blog/bayesianopt/","section":"Blogs","summary":"\u003ch2 class=\"relative group\"\u003eReferences \n    \u003cdiv id=\"references\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#references\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h2\u003e\n\u003cp\u003eMy understanding of Bayesian optimization is also limited, mainly referencing the following contentüëá\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca href=\"https://blog.csdn.net/qq_27590277/article/details/115451660\" target=\"_blank\"\u003e„ÄêMachine Learning„ÄëUnderstanding Bayesian Optimization in One Article\u003c/a\u003e\u003c/p\u003e","title":"Bayesian Optimization","type":"blog"},{"content":" Background Overview # You should know how to interact with the computer via the command line, including but not limited to: how to open the command line/terminal in Windows, when a running command ends, etc.\nA little knowledge of bypassing internet censorship is helpful. OverLeaf is foreign software, and its related LaTeX projects are also hosted abroad. Therefore, directly accessing foreign traffic when downloading dependencies can save a lot of trouble. If you don‚Äôt have a VPN, you will need to specify a domestic source for each package manager, though sometimes the updates from domestic sources are not timely.\nBasic familiarity with Vim operations is useful, such as: how to enter insert mode, how to save and exit, how to exit without saving, etc.\nFull Deployment Process # Installing Linux # Search for a Linux distribution in the Windows App Store and download it. The author chose Kali. After installation, you can open it directly from the Start menu. Upon opening, a command-line window will pop up, and you will need to register with a username and password.\nAt this point, your command line should display a warning. This is because you haven‚Äôt installed WSL (Windows Subsystem for Linux). Also, when entering the password, your input will not be displayed in the command line, but it has been recorded. Why do you need a Linux system? Because OverLeaf\u0026rsquo;s ShareLaTeX model requires a Linux environment. It is said that OverLeaf runs more smoothly on Linux systems.\nInstalling WSL # To install WSL2, run the following in the Windows command line:\nwsl --install After installation, you can open it directly. Another warning will appear. At this point, you need to create a text file in the C:\\Users\\ASUS directory and rename it to .wslconfig.\nEnter the following content:\n[experimental] autoMemoryReclaim=gradual # gradual | dropcache | disabled networkingMode=mirrored dnsTunneling=true firewall=true autoProxy=true Installing Docker # Go to the Docker website to download Docker, which will be the container for the ShareLaTeX model. Docker is an open-source application container engine that includes images, containers, and repositories. Its purpose is to manage the lifecycle of application components, such as encapsulation, distribution, deployment, and operation, allowing users to \u0026ldquo;package once, run anywhere,\u0026rdquo; much like a container, developed and encapsulated by programmers, which users can directly move around.\nOnce Docker is installed, you can double-click to start it in the background. We will interact with Docker later via the command line.\nPulling the Image # Open Kali, and run the following command:\ngit clone https://github.com/overleaf/toolkit.git ./overleaf-toolkit Then run:\ncd ./overleaf-toolkit bin/init vim ./config/variables.env At this point, you should be in the document interface of the Vim text editor. Vim has many shortcuts, and pressing the \u0026quot;I\u0026quot; key will enter insert mode for text editing. Press \u0026quot;esc\u0026quot; to return to normal mode. In insert mode, type: OVERLEAF_SITE_LANGUAGE=zh-CN.\nAfter typing, press \u0026quot;esc\u0026quot; to return to normal mode, then type :wq to \u0026ldquo;save and quit.\u0026rdquo; If you make a mistake, type :e! to discard all changes and start over. This step will set your OverLeaf interface to Chinese.\nAfter successfully saving and quitting, return to the familiar Kali command-line interface and run bin/up. This will pull the ShareLaTeX image and related network tools. There will be a large amount of data transfer, so ensure that your network is stable (your VPN should be reliable!).\nConfiguring the User # Once the previous command finishes, run bin/start. At this point, open Docker and enter the ShareLaTeX container. You should see code \u0026ldquo;flashing.\u0026rdquo; If there are no red messages, everything is running normally.\nNow open a browser and visit http://localhost/launchpad.\nAfter registering an Administrator Account, you will be redirected to http://localhost/project. The basic OverLeaf webpage should now be displayed.\nIf you compile now, it will most likely report an error ·ïï( ·êõ )·ïó. This is because ShareLaTeX is missing many required packagesüôÉ Installing Extension Packages # Open Kali, navigate to the appropriate directory, and run bin/shell. Then execute the following one by one:\ncd /usr/local/texlive # Download and run the upgrade script wget http://mirror.ctan.org/systems/texlive/tlnet/update-tlmgr-latest.sh sh update-tlmgr-latest.sh -- --upgrade # Change the TeX Live download source tlmgr option repository https://mirrors.sustech.edu.cn/CTAN/systems/texlive/tlnet/ # Upgrade tlmgr tlmgr update --self --all # Install the full TeX Live package (this will take time, so don‚Äôt let the shell disconnect) tlmgr install scheme-full # Exit the ShareLaTeX command-line interface exit # Restart the ShareLaTeX container docker restart sharelatex After restarting, enter the shell again and run:\napt update # Install fonts apt install --no-install-recommends ttf-mscorefonts-installer fonts-noto texlive-fonts-recommended tex-gyre fonts-wqy-microhei fonts-wqy-zenhei fonts-noto-cjk fonts-noto-cjk-extra fonts-noto-color-emoji fonts-noto-extra fonts-noto-ui-core fonts-noto-ui-extra fonts-noto-unhinted fonts-texgyre # Install pygments apt install python3-pygments # Install Beamer and others apt install texlive-latex-recommended apt install texlive-latex-extra # Install English fonts echo \u0026#34;yes\u0026#34; | apt install -y --reinstall ttf-mscorefonts-installer # Install Chinese fonts apt install -y latex-cjk-all texlive-lang-chinese texlive-lang-english cp fonts/* /usr/share/fonts/zh-cn/ cd /usr/share/fonts fc-cache -fv # Update font cache fc-list :lang=zh-cn fc-match Arial Finally, in the shell directory, run:\nvim /usr/local/texlive/2023/texmf.cnf Open the configuration file and add shell_escape = t at the bottom.\nI‚Äôm not sure what this does, but it was passed down by the predecessors ü§î Note, if the TeX Live version (the official name for extension packages) differs, the directory path may also change. You will need to adjust the path based on the actual version, for example, change 2023 to 2024.\nYou can use ls -l in the Linux command line to view all files in the current directory. Successful Deployment # Now you can happily use your local OverLeaf version without worrying about compilation timeouts~\nIf you\u0026rsquo;re lucky and happen to be a CQUer, here‚Äôs a graduation thesis template from Chongqing University, super user-friendly: CQUThesis\n","date":"12 July 2024","externalUrl":null,"permalink":"/en/blog/localoverleaf/","section":"Blogs","summary":"\u003ch2 class=\"relative group\"\u003eBackground Overview \n    \u003cdiv id=\"background-overview\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#background-overview\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eYou should know how to interact with the computer via the command line, including but not limited to: how to open the command line/terminal in Windows, when a running command ends, etc.\u003c/p\u003e","title":"Local OverLeaf Deployment","type":"blog"},{"content":"","date":"28 May 2025","externalUrl":null,"permalink":"/en/series/ai-engineering/","section":"Seires","summary":"","title":"AI Engineering","type":"series"},{"content":"","date":"28 May 2025","externalUrl":null,"permalink":"/en/tags/architecture/","section":"Tags","summary":"","title":"Architecture","type":"tags"},{"content":"","date":"28 May 2025","externalUrl":null,"permalink":"/en/authors/","section":"Authors List","summary":"","title":"Authors List","type":"authors"},{"content":"","date":"28 May 2025","externalUrl":null,"permalink":"/en/blog/","section":"Blogs","summary":"","title":"Blogs","type":"blog"},{"content":"","date":"28 May 2025","externalUrl":null,"permalink":"/en/tags/engineeringpractice/","section":"Tags","summary":"","title":"EngineeringPractice","type":"tags"},{"content":"","date":"28 May 2025","externalUrl":null,"permalink":"/en/tags/engineeringpractices/","section":"Tags","summary":"","title":"EngineeringPractices","type":"tags"},{"content":"","date":"28 May 2025","externalUrl":null,"permalink":"/en/tags/llm/","section":"Tags","summary":"","title":"LLM","type":"tags"},{"content":" Hi, welcome to my info page. üëã # Basic Info # My casual English name is Morethan because it resembles my Chinese name. üôÉ\nNow I\u0026rsquo;m a university student in China. ·ïï( ·êõ )·ïó Nothing more to say. ü´†\nBlog Focus # Personal Knowledge Base: to store frequently-used operations and valuable experience.\nMicro Paper Stack: to store inspirations for my Graduation Thesis, usually serious and logical, attempt to follow the standard thesis working stream.\nKnowledge Outlet: to put what I leant into practice.\nFinal # If you find the content is useful, click a like please at the beginning of that page. ü§ó\nIf you want to share the content, cite this website please. ü´°\nIf you find some bug, push an issue on the GitHub please. ü•∞\n","date":"28 May 2025","externalUrl":null,"permalink":"/en/authors/morethan/","section":"Authors List","summary":"\u003ch1 class=\"relative group\"\u003eHi, welcome to my info page. üëã \n    \u003cdiv id=\"hi-welcome-to-my-info-page-\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#hi-welcome-to-my-info-page-\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h1\u003e\n\n\n\u003ch2 class=\"relative group\"\u003eBasic Info \n    \u003cdiv id=\"basic-info\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#basic-info\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h2\u003e\n\u003cp\u003eMy casual English name is Morethan because it resembles my Chinese name. üôÉ\u003c/p\u003e","title":"Morethan","type":"authors"},{"content":"","date":"28 May 2025","externalUrl":null,"permalink":"/en/tags/notes/","section":"Tags","summary":"","title":"Notes","type":"tags"},{"content":" ","date":"28 May 2025","externalUrl":null,"permalink":"/en/series/","section":"Seires","summary":"\u003chr\u003e","title":"Seires","type":"series"},{"content":" ","date":"28 May 2025","externalUrl":null,"permalink":"/en/tags/","section":"Tags","summary":"\u003chr\u003e","title":"Tags","type":"tags"},{"content":"Here are notes and micro-thesises to reinforce knowledge through writing.\nHope my casual words can provide you something useful.ü§ó\n","date":"28 May 2025","externalUrl":null,"permalink":"/en/","section":"Welcome to Morethan's website","summary":"\u003cp\u003eHere are notes and micro-thesises to reinforce knowledge through writing.\u003c/p\u003e\n\u003cp\u003eHope my casual words can provide you something useful.ü§ó\u003c/p\u003e","title":"Welcome to Morethan's website","type":"page"},{"content":"","date":"2025-05-28","externalUrl":null,"permalink":"/tags/%E5%B7%A5%E7%A8%8B%E5%AE%9E%E8%B7%B5/","section":"Ê†áÁ≠æ","summary":"","title":"Â∑•Á®ãÂÆûË∑µ","type":"tags"},{"content":"","date":"2025-05-28","externalUrl":null,"permalink":"/tags/%E6%9E%B6%E6%9E%84/","section":"Ê†áÁ≠æ","summary":"","title":"Êû∂ÊûÑ","type":"tags"},{"content":"","date":"2025-05-28","externalUrl":null,"permalink":"/tags/%E7%AC%94%E8%AE%B0/","section":"Ê†áÁ≠æ","summary":"","title":"Á¨îËÆ∞","type":"tags"},{"content":"","date":"14 May 2025","externalUrl":null,"permalink":"/en/tags/paper/","section":"Tags","summary":"","title":"Paper","type":"tags"},{"content":"","date":"14 May 2025","externalUrl":null,"permalink":"/en/series/paper-notes/","section":"Seires","summary":"","title":"Paper Notes","type":"series"},{"content":"","date":"2025-05-14","externalUrl":null,"permalink":"/tags/%E8%AE%BA%E6%96%87/","section":"Ê†áÁ≠æ","summary":"","title":"ËÆ∫Êñá","type":"tags"},{"content":"","date":"2025-05-14","externalUrl":null,"permalink":"/series/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/","section":"Á≥ªÂàó","summary":"","title":"ËÆ∫ÊñáÁ¨îËÆ∞","type":"series"},{"content":"","date":"4 May 2025","externalUrl":null,"permalink":"/en/series/ai-musings/","section":"Seires","summary":"","title":"AI Musings","type":"series"},{"content":"","date":"2025-05-04","externalUrl":null,"permalink":"/series/ai%E9%81%90%E6%83%B3/","section":"Á≥ªÂàó","summary":"","title":"AIÈÅêÊÉ≥","type":"series"},{"content":"","date":"4 May 2025","externalUrl":null,"permalink":"/en/tags/blog/","section":"Tags","summary":"","title":"Blog","type":"tags"},{"content":"","date":"2025-05-04","externalUrl":null,"permalink":"/tags/%E5%8D%9A%E5%AE%A2/","section":"Ê†áÁ≠æ","summary":"","title":"ÂçöÂÆ¢","type":"tags"},{"content":"","date":"1 May 2025","externalUrl":null,"permalink":"/en/tags/linux/","section":"Tags","summary":"","title":"Linux","type":"tags"},{"content":"","date":"1 May 2025","externalUrl":null,"permalink":"/en/series/technical-miscellany/","section":"Seires","summary":"","title":"Technical Miscellany","type":"series"},{"content":"","date":"1 May 2025","externalUrl":null,"permalink":"/en/tags/ubuntu/","section":"Tags","summary":"","title":"Ubuntu","type":"tags"},{"content":"","date":"2025-05-01","externalUrl":null,"permalink":"/series/%E6%8A%80%E6%9C%AF%E6%9D%82%E9%A1%B9/","section":"Á≥ªÂàó","summary":"","title":"ÊäÄÊúØÊùÇÈ°π","type":"series"},{"content":"","date":"20 March 2025","externalUrl":null,"permalink":"/en/tags/mcts/","section":"Tags","summary":"","title":"MCTS","type":"tags"},{"content":"","date":"20 March 2025","externalUrl":null,"permalink":"/en/tags/reasoningenhance/","section":"Tags","summary":"","title":"ReasoningEnhance","type":"tags"},{"content":"","date":"2025-03-20","externalUrl":null,"permalink":"/tags/%E6%8E%A8%E7%90%86%E5%A2%9E%E5%BC%BA/","section":"Ê†áÁ≠æ","summary":"","title":"Êé®ÁêÜÂ¢ûÂº∫","type":"tags"},{"content":"","date":"6 March 2025","externalUrl":null,"permalink":"/en/tags/cloud-service/","section":"Tags","summary":"","title":"Cloud-Service","type":"tags"},{"content":"","date":"2025-03-06","externalUrl":null,"permalink":"/tags/%E4%BA%91%E6%9C%8D%E5%8A%A1/","section":"Ê†áÁ≠æ","summary":"","title":"‰∫ëÊúçÂä°","type":"tags"},{"content":"","date":"5 March 2025","externalUrl":null,"permalink":"/en/tags/qdrant/","section":"Tags","summary":"","title":"Qdrant","type":"tags"},{"content":"","date":"4 March 2025","externalUrl":null,"permalink":"/en/tags/ai/","section":"Tags","summary":"","title":"AI","type":"tags"},{"content":"","date":"4 March 2025","externalUrl":null,"permalink":"/en/series/project-reports/","section":"Seires","summary":"","title":"Project Reports","type":"series"},{"content":"","date":"4 March 2025","externalUrl":null,"permalink":"/en/tags/web/","section":"Tags","summary":"","title":"Web","type":"tags"},{"content":"","date":"2025-03-04","externalUrl":null,"permalink":"/series/%E9%A1%B9%E7%9B%AE%E6%8A%A5%E5%91%8A/","section":"Á≥ªÂàó","summary":"","title":"È°πÁõÆÊä•Âëä","type":"series"},{"content":"","date":"5 February 2025","externalUrl":null,"permalink":"/en/tags/neo4j/","section":"Tags","summary":"","title":"Neo4j","type":"tags"},{"content":"","date":"27 January 2025","externalUrl":null,"permalink":"/en/tags/management/","section":"Tags","summary":"","title":"Management","type":"tags"},{"content":"","date":"27 January 2025","externalUrl":null,"permalink":"/en/tags/report/","section":"Tags","summary":"","title":"Report","type":"tags"},{"content":"","date":"27 January 2025","externalUrl":null,"permalink":"/en/tags/schedule/","section":"Tags","summary":"","title":"Schedule","type":"tags"},{"content":"","date":"2025-01-27","externalUrl":null,"permalink":"/tags/%E6%8A%A5%E5%91%8A/","section":"Ê†áÁ≠æ","summary":"","title":"Êä•Âëä","type":"tags"},{"content":"","date":"2025-01-27","externalUrl":null,"permalink":"/tags/%E6%97%A5%E7%A8%8B%E7%AE%A1%E7%90%86/","section":"Ê†áÁ≠æ","summary":"","title":"Êó•Á®ãÁÆ°ÁêÜ","type":"tags"},{"content":"The CUMCM (Chinese Undergraduate Mathematical Contest in Modeling) is a competition where participants do not directly enter the national finals.\nInstead, they must go through a selection process involving the University-level competition (Ê†°Ëµõ), Provincial-level competition (ÁúÅËµõ), and only after that will their papers be submitted for evaluation by national experts. Thus, the competition is informally divided into three stages: University-level competition, Provincial-level competition, and National-level competition.\n","date":"16 January 2025","externalUrl":null,"permalink":"/en/tags/cumcm/","section":"Tags","summary":"\u003cp\u003eThe \u003cstrong\u003eCUMCM\u003c/strong\u003e (Chinese Undergraduate Mathematical Contest in Modeling) is a competition where participants do not directly enter the national finals.\u003c/p\u003e\n\u003cp\u003eInstead, they must go through a selection process involving the \u003cstrong\u003eUniversity-level competition\u003c/strong\u003e (Ê†°Ëµõ), \u003cstrong\u003eProvincial-level competition\u003c/strong\u003e (ÁúÅËµõ), and only after that will their papers be submitted for evaluation by national experts. Thus, the competition is informally divided into three stages: \u003cstrong\u003eUniversity-level competition\u003c/strong\u003e, \u003cstrong\u003eProvincial-level competition\u003c/strong\u003e, and \u003cstrong\u003eNational-level competition\u003c/strong\u003e.\u003c/p\u003e","title":"CUMCM","type":"tags"},{"content":"","date":"16 January 2025","externalUrl":null,"permalink":"/en/series/mathmodel/","section":"Seires","summary":"","title":"MathModel","type":"series"},{"content":"","date":"16 January 2025","externalUrl":null,"permalink":"/en/tags/matlab/","section":"Tags","summary":"","title":"MATLAB","type":"tags"},{"content":"","date":"15 January 2025","externalUrl":null,"permalink":"/en/tags/mysql/","section":"Tags","summary":"","title":"MySQL","type":"tags"},{"content":"","date":"10 January 2025","externalUrl":null,"permalink":"/en/tags/hugo/","section":"Tags","summary":"","title":"Hugo","type":"tags"},{"content":"","date":"6 January 2025","externalUrl":null,"permalink":"/en/series/casual-essay/","section":"Seires","summary":"","title":"Casual Essay","type":"series"},{"content":"","date":"6 January 2025","externalUrl":null,"permalink":"/en/tags/experience/","section":"Tags","summary":"","title":"Experience","type":"tags"},{"content":"","date":"2025-01-06","externalUrl":null,"permalink":"/tags/%E7%BB%8F%E5%8E%86/","section":"Ê†áÁ≠æ","summary":"","title":"ÁªèÂéÜ","type":"tags"},{"content":"","date":"2025-01-06","externalUrl":null,"permalink":"/series/%E9%9A%8F%E7%AC%94/","section":"Á≥ªÂàó","summary":"","title":"ÈöèÁ¨î","type":"series"},{"content":"","date":"3 January 2025","externalUrl":null,"permalink":"/en/tags/imagination/","section":"Tags","summary":"","title":"Imagination","type":"tags"},{"content":"","date":"2025-01-03","externalUrl":null,"permalink":"/tags/%E9%81%90%E6%83%B3/","section":"Ê†áÁ≠æ","summary":"","title":"ÈÅêÊÉ≥","type":"tags"},{"content":"","date":"12 September 2024","externalUrl":null,"permalink":"/en/tags/math/","section":"Tags","summary":"","title":"Math","type":"tags"},{"content":"","date":"10 August 2024","externalUrl":null,"permalink":"/en/tags/python/","section":"Tags","summary":"","title":"Python","type":"tags"},{"content":"","date":"12 July 2024","externalUrl":null,"permalink":"/en/tags/latex/","section":"Tags","summary":"","title":"LaTeX","type":"tags"},{"content":"","date":"12 July 2024","externalUrl":null,"permalink":"/en/tags/overleaf/","section":"Tags","summary":"","title":"Overleaf","type":"tags"},{"content":"","externalUrl":null,"permalink":"/en/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"Mathematical Modeling is the use of mathematical models to precisely and systematically describe objects in life, and it is an important combination of mathematics and practice.\n","externalUrl":null,"permalink":"/en/series/ai%E5%B7%A5%E7%A8%8B/","section":"Seires","summary":"\u003cp\u003e\u003ccode\u003eMathematical Modeling\u003c/code\u003e is the use of mathematical models to \u003cstrong\u003eprecisely and systematically\u003c/strong\u003e describe objects in life, and it is an important combination of mathematics and practice.\u003c/p\u003e\n\u003chr\u003e","title":"Mathematical Modeling","type":"series"},{"content":"Mathematical Modeling is the use of mathematical models to precisely and systematically describe objects in life, and it is an important combination of mathematics and practice.\n","externalUrl":null,"permalink":"/en/series/%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1/","section":"Seires","summary":"\u003cp\u003e\u003ccode\u003eMathematical Modeling\u003c/code\u003e is the use of mathematical models to \u003cstrong\u003eprecisely and systematically\u003c/strong\u003e describe objects in life, and it is an important combination of mathematics and practice.\u003c/p\u003e\n\u003chr\u003e","title":"Mathematical Modeling","type":"series"}]