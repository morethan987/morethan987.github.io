


[{"content":" Keywords: Mutual Reasoning; Small Models; Reasoning Capability Enhancement Research Background # Existing Work # The following content is generated by AI to quickly explain what RAP is.\nThe research foundation is a framework called RAP (Reasoning via Planning), which aims to enhance the reasoning capabilities of models in complex tasks by using language models (LLMs) as both world models and reasoning agents, combined with the Monte Carlo Tree Search (MCTS) algorithm for strategic exploration.\nProblem Background:\nCurrent large language models (LLMs) have limitations in reasoning tasks, mainly due to the lack of mental representations of the environment (world models), making it difficult to predict the outcomes and long-term effects of actions. Additionally, LLMs lack a reward mechanism to evaluate reasoning paths and cannot balance exploration and exploitation, leading to inefficient reasoning. RAP Framework:\nLanguage Model as World Model: Defines states and actions through natural language, models the reasoning process as a Markov Decision Process (MDP), and uses LLMs to predict the outcomes of each action. Reward Design: Uses the log probability of actions, confidence levels, and the LLM\u0026rsquo;s own evaluation results as rewards to guide reasoning towards an ideal state. Monte Carlo Tree Search (MCTS): Iteratively constructs a search tree through MCTS, balancing exploration and exploitation, and ultimately selects high-reward reasoning paths. Shortcomings # LLMs struggle to effectively explore the solution space, often generating low-quality reasoning paths. LLMs have difficulty accurately assessing the quality of reasoning paths. These issues are more pronounced in small models. Method Overview # Human-like Reasoning # The reasoning paths are still generated using MCTS, but with more actions that simulate human thinking processes: decomposing and searching a reasoning step, proposing sub-problems, problem transformation, etc.\nPath Evaluation # The mutual consistency principle is used to determine the quality of paths, i.e., introducing another small model of comparable capability as a \u0026ldquo;peer\u0026rdquo; to evaluate the quality of reasoning paths. The specific process is as follows:\nThe framework provides the \u0026ldquo;peer\u0026rdquo; small model with some local reasoning paths as prompts, and then asks this small model to complete the reasoning path. If the path generated by MCTS matches the path completed by the \u0026ldquo;peer\u0026rdquo; small model, then this reasoning path is considered high-quality and can be a candidate path. Using small models of comparable capability avoids distilling large models; using \u0026ldquo;peer models\u0026rdquo; to evaluate paths rather than directly guiding path generation; judging \u0026ldquo;path consistency\u0026rdquo; mainly relies on the final result. Detailed Methodology # Symbol Explanation Table:\nSymbol Meaning \\(x\\) Target Problem \\(M\\) Target Small Model \\(T\\) Search Tree Generated by Small Model Using MCTS \\(s\\) Intermediate Reasoning Step \\(t\\) Candidate Path, a Complete Reasoning Path in \\(T\\) \\(ans\\) Final Reasoning Path for \\(M\\) to Solve \\(x\\) \\(Score\\) Reasoning Path Evaluation Function \\(a\\) An Action Sampled from the Action Space \\(s_{d}\\) Termination Reasoning Step, Contains the Answer \\(\\hat{M}\\) \u0026ldquo;Peer\u0026rdquo; Small Model \\(T_{validate}\\) \\(T\\) Pruned by Path Evaluation Function \\(Estimate\\) Path Evaluation Function Problem Formalization # Formalize the abstract natural language description of \u0026ldquo;small model solving reasoning problems\u0026rdquo;:\n$$ t=x\\oplus s_1 \\oplus s_2 \\oplus \u0026hellip;\\oplus s_d $$\n$$ T=\\left { t^1, t^2, \u0026hellip;, t^n \\right } $$\n$$ T_{validate}=Estimate(T) $$\n$$ ans = max(Score(T_{validate})) $$\nHuman-like Reasoning # Action Space # One-step Thinking: Given the existing reasoning path, let the model generate the next reasoning step. Quick Thinking: Directly let the model complete all reasoning steps until the final result is produced. Sub-problem + Answer: Let the model propose a sub-problem and answer it. Sub-problem Re-answer: The answer generated in the previous step may be inaccurate, so provide an additional action option to re-answer the sub-problem. Problem Restatement: Let the model reorganize the conditions of the problem. Note that action 4 can only occur after action 3, and action 5 can only occur after the problem itself (root node). Reward Function # Inspired by AlphaGo, the evaluation (reward) of intermediate steps is set as their contribution to the correct result. The specific implementation is as follows:\nInitialize \\(Q(s_{i},a_{i})=0\\); randomly generate the next step until a termination node \\(s_{d}\\) is encountered. Use consistency voting to calculate \\(Q(s_{d},a_{d})\\), which is also the confidence score of the termination node. Backpropagation: \\(Q(s_{i},a_{i})=Q(s_{i},a_{i})+Q(s_{d},a_{d})\\). MCTS # Generally, the classic MCTS is used: selection, expansion, simulation (Rollout), and backpropagation; however, to obtain more accurate reward values, multiple simulation evaluations are performed.\nThe exploration-exploitation balance still uses the classic UCT formula:\n$$ UCT(s,a) = \\frac{Q(s,a)}{N(s,a)}+c\\sqrt{ \\frac{\\ln N_{parent}(s)}{N(s,a)} } $$\nWhere \\(N(s,a)\\) represents the number of times a node has been visited, \\(Q(s,a)\\) is the cumulative reward value, and \\(c\\) represents the balance rate.\nPath Evaluation # For a reasoning path \\(t=x\\oplus s_1 \\oplus s_2 \\oplus \u0026hellip;\\oplus s_d\\), randomly select a reasoning step \\(s_{i}\\). Inject the reasoning path before \\(s_{i}\\), \\(t_{1}=x\\oplus s_1 \\oplus s_2 \\oplus \u0026hellip;\\oplus s_i\\), as a prompt into the \u0026ldquo;peer\u0026rdquo; small model \\(\\hat{M}\\). \\(\\hat{M}\\) completes the path, generating a new path \\(t\u0026rsquo;=x\\oplus s_1 \\oplus s_2 \\oplus \u0026hellip;\\oplus s_i \\oplus s_{i+1}\u0026rsquo; \\oplus \\dots \\oplus s_{d}\u0026rsquo;\\). If the \u0026ldquo;path consistency\u0026rdquo; is achieved, i.e., the problem answers are consistent, then \\(t\u0026rsquo;\\) is considered a candidate path. The process of selecting candidate paths is the pruning process of \\(T\\), ultimately producing \\(T_{validate}\\). Final Selection # For each candidate path in the candidate path tree \\(t=x\\oplus s_1 \\oplus s_2 \\oplus \u0026hellip;\\oplus s_d\\):\n$$ Score(t)=\\prod_{i=1}^{d} Q(s_{i},a) $$\nFinally, select the reasoning path with the highest score:\n$$ ans = max(Score(T_{validate})) $$\nOther Details # MCTS performs 32 Rollouts. When processing the MATH dataset, the maximum depth of MCTS is 8; in other cases, the maximum depth is 5. The reasoning of the \u0026ldquo;peer\u0026rdquo; small model and the target small model can be executed in parallel to improve computational efficiency. During path evaluation, the truncation point of the path should be between 20% and 80% of the total path. ","date":"20 March 2025","externalUrl":null,"permalink":"/en/blog/rstar-note/","section":"Blogs","summary":"\u003cdiv class=\"lead text-neutral-500 dark:text-neutral-400 !mb-9 text-xl\"\u003e\n  Keywords: Mutual Reasoning; Small Models; Reasoning Capability Enhancement\n\u003c/div\u003e\n\n\n\n\u003ch2 class=\"relative group\"\u003eResearch Background \n    \u003cdiv id=\"research-background\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#research-background\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h2\u003e\n\n\n\u003ch3 class=\"relative group\"\u003eExisting Work \n    \u003cdiv id=\"existing-work\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#existing-work\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h3\u003e\n\u003cp\u003eThe following content is generated by AI to quickly explain what RAP is.\u003c/p\u003e","title":"rStar Reading Notes","type":"blog"},{"content":" Detailed documentation of the process of deploying an LLM application from scratch, focusing mainly on the initial setup of cloud servers, continuously updated\u0026hellip; Preface # Before deploying your LLM application, you should have already completed:\nA web front-end framework with core functionalities A relatively complete back-end code repository This document mainly records the setup process of cloud services for testing and development environments. Cloud servers for production environments are not covered here, so please refer to other articles. No practice, no say🫡\nSolution Exploration # Although this article is about \u0026ldquo;Cloud Server Setup,\u0026rdquo; I still want to document some \u0026ldquo;non-cloud service\u0026rdquo; solutions, as not all testing and development environments require an expensive cloud server. If you have already decided to use a cloud server, then skip to: Cloud Service Operation Process\nThe following content assumes your main production environment is Windows, as I have not used Linux or MacOS😢\nLocal Services # General Idea # If you are a truly \u0026ldquo;independent\u0026rdquo; developer with no need for collaborative development, then you don\u0026rsquo;t need a cloud server at all. You just need Docker.\nAfter installing Docker Desktop, you need to perform container orchestration based on your existing back-end service code.\nThis step mainly involves:\nEstablishing API interfaces: Writing the main operational logic, which can be done in any programming language you prefer Creating necessary environment files: Such as .env, pyproject.toml, etc. Creating a DockerFile: Essentially a set of command-line instructions to initialize your container Creating a docker-compose.yml file: In this file, you need to orchestrate the images used by your app, specifying internal networks, ports, mounted volumes, and other necessary settings Then, run the command: docker-compose up --build\nAfter Docker initialization, you can directly access your service via the localhost domain, just like using a cloud server.\nDocker Path Issues # When orchestrating Docker containers, there are several necessary working path parameters to understand, otherwise, you might encounter \u0026ldquo;file not found\u0026rdquo; errors😢\nbuild.context: This parameter exists in docker-compose.yml and refers to the \u0026ldquo;build context,\u0026rdquo; pointing to a real directory on your local machine WORKDIR: This parameter exists in the Dockerfile and is used to specify the \u0026ldquo;working directory\u0026rdquo;; subsequent RUN, COPY, and other commands will execute in this directory if relative paths are used, but absolute path parameters are not affected Port Forwarding # If you have built a Docker service locally but your team members need a unified testing environment, you can simply perform port forwarding on your local service.\nThere are many applications that support port forwarding, and I won\u0026rsquo;t list them all here. I personally use Sakura Frp\nAlthough port forwarding is very convenient, if you need front-end and back-end interaction, it\u0026rsquo;s best not to use port forwarding😢\nMy initial idea was: run the front-end web page locally, and also run the back-end service in a local Docker container, then use port forwarding to forward the front-end web page to users and the back-end to the front-end.\nIt seemed fine, but since your network service is not SSL verified, it cannot send https requests, causing the browser to block cross-origin insecure resource requests from the front-end to the back-end😢 The result is that only your computer can run the complete service process, and other devices will fail; even if you configure a self-signed certificate, it\u0026rsquo;s hard to pass the client browser\u0026rsquo;s security mechanism😭\nAfter unsuccessful attempts, I chose to use a cloud server.\nCloud Services # Now, the operation process of cloud servers is very simple, but there is one drawback: expensive😢\nOf course, if it\u0026rsquo;s not for a production environment but a testing and development environment, you don\u0026rsquo;t need to choose a top-tier server. Choose a suitable one based on your financial situation and team size.\nI ultimately chose Tencent Cloud\u0026rsquo;s lightweight server: 2 cores + 2GB RAM + 4M bandwidth + 50GB system disk + 300GB monthly traffic\nNo other reason, mainly because it\u0026rsquo;s cheap, only ￥88 for the first year.\nCloud Service Operation Process # Register a Domain # Set Up DNS Resolution # Purchase a Cloud Server # Configure the Server # The server operating system I use here is Ubuntu Server 24.04 LTS 64bit, and subsequent commands are based on this system.\nFile Transfer # To transfer files from the local machine to the server, I chose to use some more modern terminals, such as: Tabby, WindTerm, and Warp, etc.\nI randomly chose Tabby, which makes remote connections more convenient and has built-in SFTP for easy file transfer. If you have the energy to customize the terminal interface, Tabby can be very aesthetically pleasing😄\nOf course, more traditional solutions include using FileZilla; and if you don\u0026rsquo;t mind the hassle, you can directly use terminal commands: use rsync or scp commands.\nIf you are using a Tencent Cloud server and are curious about what the lighthouse folder is🤨: this folder is the account for one-click password-free login. Install Docker # Execute the following commands one by one, each command has an explanation:\nsudo apt-get update # Upgrade sudo apt install -y apt-transport-https ca-certificates curl gnupg lsb-release # Install dependency tools, mainly https transmission and verification-related toolkits # Install Docker\u0026#39;s GPG key to ensure the Docker image you download has not been tampered with sudo curl -fsSL https://mirrors.cloud.tencent.com/docker-ce/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc # Add Docker\u0026#39;s official software repository to the system\u0026#39;s APT source list # I don\u0026#39;t think you want to know what this bunch means٩(•̤̀ᵕ•̤́๑) sudo install -m 0755 -d /etc/apt/keyrings sudo chmod a+r /etc/apt/keyrings/docker.asc echo \u0026#34;deb [arch= \\\\((dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://mirrors.cloud.tencent.com/docker-ce/linux/ubuntu/ \\\\\\\\)(. /etc/os-release \u0026amp;\u0026amp; echo \u0026#34;$VERSION_CODENAME\u0026#34;) stable\u0026#34; | sudo tee /etc/apt/sources.list.d/docker.list \u0026gt; /dev/null # Update to let apt recognize the newly added Docker software source sudo apt-get update # Install Docker engine sudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin # Start Docker sudo docker info # Verify installation information sudo systemctl start docker # Start service sudo systemctl enable docker # Enable auto-start on boot sudo systemctl status docker # Verify service status After running the last command, you will enter a paged log view mode, similar to a Vim editor. Enter :q to return to the regular command line. Use Docker # sudo systemctl start docker # Start service # Configure Tencent Cloud\u0026#39;s Docker source sudo vim /etc/docker/daemon.json # Create configuration file # Press the I key to switch to I mode and enter { \u0026#34;registry-mirrors\u0026#34;: [ \u0026#34;https://mirror.ccs.tencentyun.com\u0026#34; ] } # Press esc, then enter \u0026#39;:wq\u0026#39;, this is a Vim operation, meaning save and exit sudo systemctl restart docker # Restart Docker service sudo systemctl stop docker # Stop Docker service # Verify source information, you should see the Registry Mirrors value is the URL you set sudo docker info # Pull image sudo docker pull nginx # If you want to use docker-compose # On Ubuntu, use: docker compose sudo docker compose up --build # Start service without rebuilding sudo docker compose up -d # Stop all services sudo docker compose down Before using the docker compose command, please create docker-compose.yml and Dockerfile according to the official tutorial. Configure pip/poetry # Strangely, Tencent Cloud installs Python by default at the system level but does not install pip🤔 So you need to manually install it. Before installation, please confirm whether Python is installed.\nIf you are not using pip at the system level but pip inside a Docker container, then changing the pip source at the system level will not have any effect. Change pip source at the system level:\n# Check Python version python3 -V # Check the path of python3 which python3 # Confirm Python is installed and pip is not installed sudo apt install python3-pip # If using Tencent\u0026#39;s server, you can use the intranet domain for faster speed pip config set global.index-url https://mirrors.tencentyun.com/pypi/simple # If not, use the external domain pip config set global.index-url https://mirrors.cloud.tencent.com/pypi/simple Change pip source inside Docker container: Directly write the following command in your Dockerfile\n# Configure pip source # Use Tencent Cloud mirror source, note this is the intranet domain RUN pip config set global.index-url https://mirrors.tencentyun.com/pypi/simple \\ \u0026amp;\u0026amp; pip config set global.trusted-host mirrors.tencentyun.com If you use poetry at the system level:\n# Install poetry using online script curl -sSL https://raw.githubusercontent.com/python-poetry/poetry/master/get-poetry.py | python # Install poetry using apt sudo apt install python3-poetry # Use Tencent Cloud mirror source, note this is the intranet domain sudo poetry config repositories.tencentyun https://mirrors.tencentyun.com/pypi/simple If your system does not have pip installed by default, it is not recommended to install pip first and then install poetry. SSL/TLS # SSL/TLS is a set of encrypted transmission protocols, carefully designed by cryptography experts. The widely used Hypertext Transfer Protocol Secure (HTTPS) is obtained by encrypting the plaintext transmission protocol HTTP using SSL/TLS.\nA core part of HTTPS is the handshake before data transmission, during which the password for data encryption is determined. During the handshake, the website sends an SSL certificate to the browser. The SSL certificate is similar to our daily ID card, serving as an identity proof for HTTPS websites. The SSL certificate contains the website\u0026rsquo;s domain name, certificate validity period, certificate issuing authority, and the public key used for encrypting the transmission password. Before generating the password, the browser needs to verify whether the currently accessed domain name matches the domain name bound to the certificate, and also verify the Certificate Authority (CA). If the verification fails, the browser will give a certificate error prompt.\nGenerally, SSL certificates need to be purchased from CA institutions, but some CA institutions provide free SSL certificate services, such as: Let’s Encrypt\nAfter long-term development, the certificates issued by Let’s Encrypt can now be automatically processed through Certbot, and the certificates can be automatically re-applied when they expire. Of course, such convenience comes at a cost😢: You need to configure certbot, which is quite troublesome.\nHere, I chose to directly deploy a certbot inside a Docker container to apply for SSL certificates, meaning you can directly modify the content in docker-compose.yml to pull the official certbot image.\n# First, stop all Docker services sudo docker compose down # Start all services sudo docker compose up -d # Start part of the services sudo docker compose up nginx -d # View logs of a specific image sudo docker logs server-app-1 # View logs of a specific service sudo docker compose logs certbot-init # Use recursive deletion to clear cache rm -rf ./certbot/conf/* # Enter a container\u0026#39;s shell sudo docker exec -it server-app-1 /bin/sh # Remove a Docker image forcefully sudo docker rmi -f server-frontend # Restart a specific service in Docker Compose sudo docker compose restart app # Clear npm cache forcefully npm cache clean --force # Activate a Python virtual environment source your_venv_name/bin/activate Filing # Filing using a corporate entity is not difficult, so I won\u0026rsquo;t elaborate too much here. This section mainly discusses individual entity filing. Main reference: Tencent Cloud Server Filing Full Process 40 Days of Filing Blood and Tears - Blog Garden\ngraph LR; p1(Server Platform Real-name Authentication)--\u003ep2(Real-name Authentication Purchase Domain)--\u003ep3(Apply for Filing)--\u003ep4(Platform Review)--\u003ep5(Regulatory Authority Review)--\u003ep6(Public Security Filing) If you are a Tencent Cloud user, you can use the Tencent Cloud Website Filing Mini Program; after submitting the application, the platform will first review it, usually taking about 8 hours; then the platform will submit it to the regulatory authority for review, usually taking about 7 working days; after the regulatory authority review is passed, you need to complete the public security filing within 30 working days.\nMiscellaneous # This section mainly documents issues that may arise during the entire operation process.\nCertbot Access Denied? # First, of course, check the network reasons. Check whether the security group of the server has opened ports 443 and 80, and check whether the firewall of the server operating system has blocked port access.\n# Ubuntu installs ufw firewall by default # Check ufw status, inactive status means not started, which is the ideal state sudo ufw status If you\u0026rsquo;re based in China, here\u0026rsquo;s another issue: Certbot can\u0026rsquo;t reach your server if you don\u0026rsquo;t have ICP备案 (ICP license). 😢\nReferences # Using Third-party SSH Terminal to Log in to Linux Instance - Operation Guide - Tencent Cloud Using Local Built-in SSH Terminal to Log in to Linux Instance - Operation Guide - Tencent Cloud Installing Docker in Linux Environment Ubuntu 22.04 Install Docker What is Docker\u0026rsquo;s Official GPG Key For? Cloud Server Setup Docker - Practice Tutorial - Tencent Cloud Installing Docker and Configuring Mirror Acceleration Source - Practice Tutorial - Tencent Cloud Debian 12 / Ubuntu 24.04 Install Docker and Docker Compose Tutorial - Shaobing Blog Step-by-Step Guide to Python pip Source Change Operation in Linux System - Tencent Cloud pip Source Configuration - Tencent Cloud Developer Community - Tencent Cloud How to Change Domestic Source for Poetry - Data Science SourceResearch Linux Stop Docker Container: Single, Multiple, or All Where to View Docker Logs? How to View Logs in Linux Server - CSDN Blog Troubleshooting and Solving Linux Firewall Closed but Still Unable to Access Web About Ubuntu Console Opened All Ports, but External Hosts Still Cannot Access Corresponding Port Services - CSDN Blog Three Minutes to Explain SSL Authentication and Encryption Technology - CSDN Blog One Article to Thoroughly Understand SSL/TLS Protocol - Zhihu HTTPS and SSL Certificate Overview | Rookie Tutorial Four Common Free Certificate Application Methods - CSDN Blog Using Docker to Deploy Nginx and Configure HTTPS - TandK - Blog Garden Using Docker + Nginx + Certbot to Automate SSL Certificate Management - CSDN Blog The Certificate Authority Failed to Download the Temporary Challenge Files Created by Certbot \u0026ndash; Connection Refused - Help - Let\u0026rsquo;s Encrypt Community Support ICP Filing First Filing - Tencent Cloud ICP Filing Requirements by Province - Tencent Cloud ","date":"6 March 2025","externalUrl":null,"permalink":"/en/blog/cloud-server-build/","section":"Blogs","summary":"\u003cdiv class=\"lead text-neutral-500 dark:text-neutral-400 !mb-9 text-xl\"\u003e\n  Detailed documentation of the process of deploying an LLM application from scratch, focusing mainly on the initial setup of cloud servers, continuously updated\u0026hellip;\n\u003c/div\u003e\n\n\n\n\u003ch2 class=\"relative group\"\u003ePreface \n    \u003cdiv id=\"preface\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#preface\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h2\u003e\n\u003cp\u003eBefore deploying your LLM application, you should have already completed:\u003c/p\u003e","title":"Cloud Service Deployment","type":"blog"},{"content":" A detailed record of the core features of the Qdrant vector database, helping developers design better databases in practical applications 😋 Preface # Vector database-based RAG is the most fundamental form of RAG and is widely used in production. As the name suggests, vector databases primarily store vectors. Through vector operations, we can achieve more precise relevance searches, which serve as the cornerstone for many application scenarios.\nThere are many vector database providers: Weaviate, Qdrant, Milvus, Chroma, and more. However, they are generally similar. For a detailed comparative analysis of mainstream vector databases, see: Vector Database Comparison Report: A Detailed Comparison of 12 Mainstream Vector Databases.\nThis article focuses solely on Qdrant. All content is based on the Qdrant Official Documentation (as of March 2025).\nThis article focuses less on specific syntax and more on features and principles, helping developers quickly design product workflows without being constrained by cumbersome syntax, thereby improving imagination and efficiency.\nBasic Data Types # Qdrant defines some abstract data types to better handle vector data. Understanding these types is fundamental to flexible usage.\nPoint # Points are the core data type in a vector database. All operations revolve around points.\nA very basic point contains only its vector, but typically, points are tagged with additional metadata to provide more information beyond the vector data. These tags are called Payload 👇.\nThus: Point = Vector + Payload Tags\n// A simple point { \u0026#34;id\u0026#34;: 129, \u0026#34;vector\u0026#34;: [0.1, 0.2, 0.3, 0.4], \u0026#34;payload\u0026#34;: {\u0026#34;color\u0026#34;: \u0026#34;red\u0026#34;}, } Qdrant points can be configured with various types of vectors: Dense Vectors, Sparse Vectors, Multi-Vectors, and Named Vectors.\nVector Type Description Dense Vectors Standard vectors; most embedding models generate this type. Sparse Vectors Variable-length vectors with few non-zero elements; typically used for exact token matching and collaborative filtering. Multi-Vectors Matrices composed of multiple dense vectors; dimensions must match across points, but the number of vectors can vary. Used to store different vector descriptions of the same target. Named Vectors A hybrid of the above types, allowing different vector types to coexist in a single point. These vectors are abstracted as named vectors. Basic CRUD operations are straightforward and won\u0026rsquo;t be elaborated here.\nPayload # Additional metadata attached to vectors, described and stored using JSON syntax. Here\u0026rsquo;s an example:\n{ \u0026#34;name\u0026#34;: \u0026#34;jacket\u0026#34;, \u0026#34;colors\u0026#34;: [\u0026#34;red\u0026#34;, \u0026#34;blue\u0026#34;], \u0026#34;count\u0026#34;: 10, \u0026#34;price\u0026#34;: 11.99, \u0026#34;locations\u0026#34;: [ { \u0026#34;lon\u0026#34;: 52.5200, \u0026#34;lat\u0026#34;: 13.4050 } ], \u0026#34;reviews\u0026#34;: [ { \u0026#34;user\u0026#34;: \u0026#34;alice\u0026#34;, \u0026#34;score\u0026#34;: 4 }, { \u0026#34;user\u0026#34;: \u0026#34;bob\u0026#34;, \u0026#34;score\u0026#34;: 5 } ] } Since this data is stored in the database, it serves a purpose: vector databases primarily rely on semantic similarity matching, and these tags allow you to add additional logical filtering conditions on top of that.\nFor more on filtering, see: Filtering\nCollection # A collection is simply a group of points. At this level, you can define:\nSimilarity algorithms between points Vector dimensions Optimizer configurations HNSW algorithm configurations: The key parameter to adjust is ef, which determines the number of neighboring nodes the algorithm visits. Higher values yield more accurate but slower queries. WAL configurations Quantization configurations Common Operations # These are the most fundamental and important operations when using a vector database.\nSearch # In the context of vector databases, \u0026ldquo;search\u0026rdquo; primarily refers to similarity search. The theoretical premise is that objects with higher similarity in the real world are closer in vector space.\nThe term \u0026ldquo;closer\u0026rdquo; implies a similarity metric. Qdrant supports several popular similarity algorithms:\nDot Product Similarity Cosine Similarity Euclidean Distance Manhattan Distance To improve performance, all vectors are normalized before storage. This means dot product similarity and cosine similarity are equivalent in Qdrant. For a better user experience, Qdrant provides a complete API for easy invocation: Search API - Qdrant.\nIn summary, the functionalities you can invoke include:\nBasic Operations: Input a vector to perform similarity matching in the database. If your points store \u0026ldquo;named vectors,\u0026rdquo; you need to specify which vector to use for matching. Search Algorithm Control: You can enable exact search, which performs similarity matching on every point, taking longer (more parameters are adjustable but rarely used). Result Filtering: Filter results before the actual search using payload tags to narrow the scope, or after the search using similarity score thresholds. Result Count: The limit parameter controls the number of results returned (the top limit most similar results). Batch Search: Input multiple vectors for search in one go. Search Grouping: Group search results by certain tags. The group_size parameter sets the number of results per group. Search Planning: Based on optional indexes, filter complexity, and the total number of points, a heuristic method selects an appropriate search approach (improving performance 🤔). If both group_size and limit are set, limit represents the number of groups. Additionally, sparse and dense vector searches in Qdrant have key differences:\nComparison Sparse Vectors Dense Vectors Similarity Algorithm Defaults to dot product; no need to specify You can specify supported algorithms Search Method Only exact search Can use HNSW for approximate search Search Results Returns only vectors with shared non-zero elements Returns the top limit vectors Explore # Explore operations are more flexible searches: they can query based on similarity as well as dissimilarity.\nRecommendation # \u0026ldquo;Recommendation\u0026rdquo; allows you to provide both positive and negative vectors for search. Here\u0026rsquo;s an official example:\nimport { QdrantClient } from \u0026#34;@qdrant/js-client-rest\u0026#34;; const client = new QdrantClient({ host: \u0026#34;localhost\u0026#34;, port: 6333 }); client.query(\u0026#34;{collection_name}\u0026#34;, { query: { recommend: { positive: [100, 231], negative: [718, [0.2, 0.3, 0.4, 0.5]], strategy: \u0026#34;average_vector\u0026#34; } }, filter: { must: [ { key: \u0026#34;city\u0026#34;, match: { value: \u0026#34;London\u0026#34;, }, }, ], }, limit: 3 }); In the official example, 100 and 231 are vector IDs, each corresponding to a 4-dimensional vector. The strategy parameter controls the search algorithm. Here are the details:\nAverage Algorithm: Averages positive and negative examples separately, then combines them into a final search vector. Best Score Algorithm: Each candidate point is matched against all positive and negative examples to compute scores. The highest scores are selected, and the final score is calculated as: let score = if best_positive_score \u0026gt; best_negative_score { best_positive_score } else { -(best_negative_score * best_negative_score) }; Negative-Only Algorithm: Uses the best score algorithm 👆 without positive examples, yielding a reverse scoring algorithm to find the least relevant points. Multi-vectors and other special vectors can also be processed with similar logic, though the syntax differs. Discovery # \u0026ldquo;Discovery\u0026rdquo; operations partition the sample space. You provide pairs of positive and negative vectors, each dividing the space into positive and negative regions. The search returns points that lie more in positive regions or less in negative regions.\nSimilar to Recommendation, but here you pair positive and negative vectors together as input.\nDue to hard partitioning, consider increasing the HNSW ef parameter to compensate for precision loss. Discovery operations enable Qdrant to handle two new search requirements:\nDiscovery Search: Given a target vector and a set of positive-negative vector pairs as contextual constraints. Region Partition Search: A special case of discovery search 👆 where no target vector is provided. The database partitions the space directly and returns points lying most in positive regions. In discovery search, contextual constraints are enforced with higher priority. In other words, discovery search first performs region partitioning, then standard similarity search. Distance Matrix # This operation resembles batch search. Batch search inputs multiple vectors and searches for similar vectors for each. Distance matrix randomly selects a subset of vectors, then searches for similar vectors within this subset for each vector.\nFor example, if sample=100 vectors are selected to form a subset, and limit=10 similar vectors are searched for each, the returned distance matrix will be a \\(100 \\times 10\\) matrix, where each row represents the top 10 similar vectors for one point.\nThis operation is typically used for data visualization or dimensionality reduction.\nFiltering # Official English guide: A Complete Guide to Filtering in Vector Search. The guide explains how Qdrant internally performs filtering, which helps design efficient systems.\nThe guide briefly lists some functionalities. For complete details, see: Filtering.\nFilter Conditions # These refer to individual filter conditions, the basic units of filtering. Here are the types:\nType Function Match The condition is a specific value; the attribute must exactly match it. Match Any The condition is a set of options; the attribute must match any of them. Match Except The condition is a set of options; the attribute must not match any of them. Range The condition is a range; the attribute must lie within it. Values Count The attribute is an array; filtering is based on the number of elements. Is Empty Filters based on whether the attribute exists. These are the basic filter types. Syntax varies for different payload types:\nJSON Payload: A point\u0026rsquo;s payload can be a JSON object, and any field can participate in filtering. See Nested Key. Date Range: Similar to numeric ranges, date ranges can also filter. Geofiltering: Geographic locations can be filtered. Named Vectors: Named vectors contain vectors of different dimensions. Filtering can check for the presence of specific vectors (e.g., filtering points with image embeddings). These basic conditions can be nested using logical keywords 👇 to form complex filters.\nLogical Keywords # Similar to SQL\u0026rsquo;s AND, OR, NOT, Qdrant uses must, should, must_not to express similar logic. These keywords build complex filters.\nmust: Returns true only if all listed conditions are met. should: Returns true if any listed condition is met. must_not: Returns true if none of the listed conditions are met. Advanced Operations # Hybrid Queries # Optimization # Storage # Indexing # Snapshots # References # Vector Database Comparison Report: A Detailed Comparison of 12 Mainstream Vector Databases | SynDataWorks Qdrant Official Documentation ","date":"5 March 2025","externalUrl":null,"permalink":"/en/blog/qdrant-feature-guide/","section":"Blogs","summary":"\u003cdiv class=\"lead text-neutral-500 dark:text-neutral-400 !mb-9 text-xl\"\u003e\n  A detailed record of the core features of the Qdrant vector database, helping developers design better databases in practical applications 😋\n\u003c/div\u003e\n\n\n\n\u003ch2 class=\"relative group\"\u003ePreface \n    \u003cdiv id=\"preface\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#preface\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h2\u003e\n\u003cp\u003eVector database-based RAG is the most fundamental form of RAG and is widely used in production. As the name suggests, vector databases primarily store vectors. Through vector operations, we can achieve more precise \u003cstrong\u003erelevance searches\u003c/strong\u003e, which serve as the \u003cstrong\u003ecornerstone\u003c/strong\u003e for many application scenarios.\u003c/p\u003e","title":"Qdrant Feature Guide","type":"blog"},{"content":" This is an independent full-stack development journey, driven by the goal of building an LLM application. It covers various issues encountered during development.\nIt\u0026rsquo;s also the introductory article—article zero—of the \u0026ldquo;AI Engineering\u0026rdquo; series, serving both as an index to the series and a summary of personal insights.\nIntroduction # Nowadays, we\u0026rsquo;ve become accustomed to effortlessly interacting with AI chat assistants like Doubao, Kimi, and DeepSeek. It seems like creating something similar ourselves shouldn\u0026rsquo;t be too difficult, right?\nThat innocent thought was exactly where my dream began 😢. Sounds silly, I know, but that\u0026rsquo;s genuinely how I felt. And from there began a long, challenging development journey.\nIf you\u0026rsquo;ve ever had a similar idea, the following content might help you 🤗.\nAfter countless late nights, I have to admit: building an LLM application is a massive endeavor, especially for an independent developer. Choosing independent development or assembling a small team means stepping onto a path filled with challenges. There are no \u0026ldquo;senior mentors\u0026rdquo; guiding your technical choices; you have to explore everything on your own.\nBut, that\u0026rsquo;s exactly the joy of independent development—you have complete control over your product and get to witness every incremental improvement.\nYou might wonder: \u0026ldquo;What if my product doesn’t perform well?\u0026rdquo; \u0026ldquo;What if I hit a development roadblock?\u0026rdquo; \u0026ldquo;If nobody uses it, wouldn’t my effort go to waste?\u0026rdquo; 🤔\nAdmittedly, independent development demands significant investment, and these worries are inevitable, even universal, among independent developers.\nHowever, if you\u0026rsquo;ve genuinely committed yourself to independent development, regardless of the outcome, you\u0026rsquo;ll certainly gain something valuable. If you succeed, congratulations 🥳—your hard work has paid off. If you fail or give up, I understand your frustration, disappointment, even anger.\nBut no one finds happiness or success by completely dismissing their past experiences. Momentarily set aside the pain, and move forward to your next adventure.\nIn life, there are no wasted steps; every step counts 🫡.\nIdentifying Needs # Draft draft draft\u0026hellip;\nExploring Implementation Options # Draft draft draft\u0026hellip;\nProject Construction # Draft draft draft\u0026hellip;\nDeploying the Service # Cloud Service Deployment ","date":"4 March 2025","externalUrl":null,"permalink":"/en/blog/llm-app-driven-fullstack-dev/","section":"Blogs","summary":"\u003cdiv class=\"lead text-neutral-500 dark:text-neutral-400 !mb-9 text-xl\"\u003e\n  \u003cp\u003eThis is an independent full-stack development journey, driven by the goal of building an LLM application. It covers various issues encountered during development.\u003c/p\u003e\n\u003cp\u003eIt\u0026rsquo;s also the introductory article—article zero—of the \u0026ldquo;AI Engineering\u0026rdquo; series, serving both as an index to the series and a summary of personal insights.\u003c/p\u003e","title":"A Guide to Independent Full-Stack Development Driven by LLM Applications","type":"blog"},{"content":" The basic info and operations of Neo4j Graph Database. Introduction # Neo4j is a high-performance graph database that stores data and the relationships between data in the form of graphs.\nThe specific form of the graph is a labeled property graph, and the query language used is Cypher.\nLabeled Property Graph # A labeled property graph is a specific type of graph:\nA node has one or more labels to define its type. Relationships and nodes are treated equally, holding the same level of importance. Each node and relationship possesses accessible properties. Cypher # Cypher is a declarative query language that allows you to identify patterns in your data using an ASCII-art style syntax consisting of brackets, dashes and arrows.\nPatterns # A pattern in a graph database is a specific combination of nodes and relationships. For example, a person (Person) acting in a movie (Movie) is a pattern, represented in code as:\n(p:Person)-[r:ACT_IN]-\u0026gt;(m:Movie) Here, the parts enclosed in parentheses represent nodes, while the parts enclosed in square brackets represent relationships. In the node section, p and m are variables referring to the respective nodes, with Person and Movie being the labels of the nodes, connected by a colon. In the relationship section, r is the variable referring to the relationship, and ACT_IN is the specific type of relationship. Translated into natural language, this means: use p to refer to a node labeled Person, use m to refer to a node labeled Movie, and represent the relationship between them with r, which is of type ACT_IN.\nData Reading # Data reading operations rely on pattern matching, using the keyword MATCH. This is equivalent to sending an instruction to the database to filter out only the node-relationship pairs that match the specified pattern, i.e., a collection of triples (p, r, m).\nMATCH (p:Person)-[r:ACT_IN]-\u0026gt;(m:Movie) Similar to SQL, you can further filter using the WHERE keyword:\nMATCH (p:Person)-[r:ACT_IN]-\u0026gt;(m:Movie) WHERE p.name = \u0026#39;Tom Hanks\u0026#39; RETURN p, r, m Here, name is a property of the node p, accessed using the . operator. The RETURN keyword marks the values to be returned.\nBelow is a more complex pattern matching command, where the AS keyword is used to set aliases:\nMATCH (p:Person)-[:ACTED_IN]-\u0026gt;(m:Movie)\u0026lt;-[r:ACTED_IN]-(p2:Person) WHERE p.name = \u0026#39;Tom Hanks\u0026#39; RETURN p2.name AS actor, m.title AS movie, r.role AS role This command filters out all actors who have acted in the same movie as Tom Hanks, returning their names, the movies they acted in, and the roles they played.\nSorting by a specific property and pagination are also supported:\nMATCH (m:Movie) WHERE m.released IS NOT NULL RETURN m.title AS title, m.url AS url, m.released AS released ORDER BY released DESC LIMIT 5 This will filter out the 5 most recent movies.\nCypher keywords are case-insensitive; property names, variable names, and other identifiers are case-sensitive. Data Writing # Data writing uses the MERGE keyword, which means merging a node or relationship into the data graph.\nMerging nodes: MERGE (m:Movie {title: \u0026#34;Arthur the King\u0026#34;}) SET m.year = 2024 RETURN m This command creates a new Movie node with the title property set to \u0026quot;Arthur the King\u0026quot; and the year property set to 2024.\nYou might wonder why not write it like this:\nMERGE (m:Movie) SET m.year = 2024, m.title = \u0026#34;Arthur the King\u0026#34; RETURN m The reason is that the content within the curly braces is used to check whether the pattern you want to create already exists, avoiding duplicate patterns.\nMerging relationships: MERGE (m:Movie {title: \u0026#34;Arthur the King\u0026#34;}) MERGE (u:User {name: \u0026#34;Adam\u0026#34;}) MERGE (u)-[r:RATED {rating: 5}]-\u0026gt;(m) RETURN u, r, m Installation # As of February 15, 2025, after extensive testing by the developer community, it has been found that Neo4j Desktop is blocked for users in mainland China, causing the software interface to fail to display properly. While bypassing the block through methods such as disabling the network is possible, this removes the key advantage of the desktop version—easy deployment.\nThus, I recommend using the Docker deployment method.\nDocker Desktop # Simply install Docker Desktop and run it in the background.\nPulling the Image # Normally, you can simply pull the latest image with the command: docker pull neo4j.\nHowever, if your project requires the APOC plugin, you should consider the version of APOC, as the image may be ahead of the APOC version. The community version of APOC is released on Releases · neo4j/apoc.\nFor APOC compatibility, use the following command: docker pull neo4j:5.26.2.\nBuilding the Container # docker run -d -p 7474:7474 -p 7687:7687 -v E:/neo4j/data:/data -v E:/neo4j/logs:/logs -v E:/neo4j/conf:/var/lib/neo4j/conf -v E:/neo4j/import:/var/lib/neo4j/import -v E:/neo4j/plugins:/var/lib/neo4j/plugins -e NEO4J_dbms_security_procedures_unrestricted=\u0026#34;apoc.*\u0026#34; -e NEO4J_dbms_security_procedures_allowlist=\u0026#34;apoc.*\u0026#34; -e NEO4JLABS_PLUGINS=\u0026#39;[\u0026#34;apoc\u0026#34;]\u0026#39; -e NEO4J_AUTH=neo4j/mo123456789 --name neo4j neo4j:5.26.2 Explanation of the parameters:\nThe -p option exposes ports; here, we open two ports. The -v option mounts directories from the host machine (this means specifying where the Docker application will store its data). The -e option configures environment variables. The apoc-related configurations are for the APOC plugin; NEO4J_AUTH sets the username to neo4j and the password to mo123456789. If you use Neo4j for language model-enhanced generation (RAG), be sure to include the APOC-related configurations. Otherwise, you can omit these settings. After running the command above, check the host machine\u0026rsquo;s mounted directory to confirm if the APOC plugin is installed:\nManual Installation of APOC # Visit Releases · neo4j/apoc, download the latest apoc-5.26.2-core from the \u0026ldquo;Assets\u0026rdquo; section, and paste it into the host machine\u0026rsquo;s specified directory. Then restart the Docker container.\nBrowser UI # By default, the 7474 port is used for the browser UI, and the 7687 port is for other backend applications.\nWhen the container is running in the background, access the UI at: http://localhost:7474/browser/preview.\nConnect to the database using: neo4j://localhost:7687.\nYou should now be able to access the browser UI successfully. 😄\nReferences # Docker: Docker Deployment of Neo4j Graph Database - Angry Radish - Blog ","date":"5 February 2025","externalUrl":null,"permalink":"/en/blog/neo4j-basics/","section":"Blogs","summary":"\u003cdiv class=\"lead text-neutral-500 dark:text-neutral-400 !mb-9 text-xl\"\u003e\n  The basic info and operations of Neo4j Graph Database.\n\u003c/div\u003e\n\n\n\n\u003ch2 class=\"relative group\"\u003eIntroduction \n    \u003cdiv id=\"introduction\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#introduction\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/neo4j/neo4j\" target=\"_blank\"\u003eNeo4j\u003c/a\u003e is a high-performance graph database that stores data and the relationships between data in the form of graphs.\u003c/p\u003e","title":"Neo4j Basics","type":"blog"},{"content":"TODO_UPDATE Analysis Report on the Current State of Schedule Management Software Functionality Preface # The term \u0026ldquo;schedule management\u0026rdquo; frequently appears in our daily lives, yet it remains elusive: What exactly is schedule management? Why do we need it? What purpose does it serve? Who engages in schedule management, and how effective is it?\nWhat is Schedule Management? # On the surface, \u0026ldquo;schedule management\u0026rdquo; seems like a common concept, but in reality, it encompasses complex and multifaceted content.\nIn summary, schedule management is a method of clearly recording, planning, organizing, and optimizing personal or team time and tasks to achieve set goals.\nDepending on different needs, it can be categorized into the following types:\nPersonal Growth and Self-Improvement:\nCharacteristics: Entirely dependent on personal will, often with a broad goal but no specific events or timelines, possibly with a quantitative metric. Primary Purpose: Facilitate long-term personal growth, improve quality of life, and enhance satisfaction. Professional and Academic Management:\nCharacteristics: Highly specific events and timelines, not driven by personal preference, often requiring collaboration with others, usually with one or more quantitative metrics. Primary Purpose: Achieve concrete career goals, enhance professional skills, and improve efficiency. Social and Relationship Management:\nCharacteristics: Involves complex interpersonal dynamics, lacks quantitative metrics, is unpredictable, and has low controllability. Primary Purpose: Maintain, expand, and optimize social networks, increase social capital. Financial and Daily Life Management:\nCharacteristics: Tasks are trivial, frequently repetitive, flexible yet follow periodic patterns, and generally short in duration. Primary Purpose: Ensure a sense of order and stability in life. In reality, schedules are a mix of the above aspects. Thus, when we casually refer to \u0026ldquo;schedule management,\u0026rdquo; we vaguely mean \u0026ldquo;managing all aspects of life,\u0026rdquo; leading to a very ambiguous understanding of the concept.\nWhat Constitutes \u0026ldquo;Good\u0026rdquo; Schedule Management? # Generally, \u0026ldquo;good schedule management\u0026rdquo; is a clear, realistic, flexible system that effectively balances long-term goals with short-term actions, incorporates regular feedback and continuous adjustment mechanisms, significantly enhances personal control, reduces stress, and is easy to record and execute. Specifically, it should meet the following criteria:\nGoal Clarity Realistic and Executable Flexibility \u0026amp; Resilience Balance between Long-term and Short-term Feedback and Adjustment Mechanisms Reduce Stress and Enhance Control Ease of Use \u0026amp; Clear Recording From a user experience perspective, \u0026ldquo;good schedule management\u0026rdquo; simply needs to answer the following questions:\nHow to collect necessary information: User schedules, preferences, weather data, online information, etc. How to process the collected information: Different workflows for different types of schedules. How to enhance user experience: Streamlined and warm interactions, intuitive and concise information presentation. Industry Status # Below is a list of existing schedule management solutions. However, to be honest, there are few domestic applications in this niche, while international offerings, though numerous, suffer from severe homogenization.\nTickTick # TickTick offers comprehensive functionality with a clean design, representing a classic static schedule management app. Its logic for schedule management is roughly as follows:\ngraph LR; id1(\u0026#34;Schedule Collection\u0026#34;)--\u0026gt;id2(\u0026#34;Manual Schedule Arrangement\u0026#34;)--\u0026gt;id3(\u0026#34;Execute Schedule on Time\u0026#34;); id2(\u0026#34;Manual Schedule Arrangement\u0026#34;)--\u0026gt;id4(\u0026#34;Schedule Not Executed on Time\u0026#34;)--\u0026gt;id5(\u0026#34;Manual Adjustment\u0026#34;); Dola # Dola is a futuristic AI-powered schedule management app: it has no interface, and all interactions with the AI occur via messaging platforms like WhatsApp or Apple Messages. However, it does not support mainstream Chinese apps like QQ or WeChat.\nBased on official descriptions, here’s an approximate user flow:\ngraph LR; id1(\u0026#34;Send Message via Platform\u0026#34;)--\u0026gt;id2(\u0026#34;AI Analyzes and Generates Schedule\u0026#34;)--\u0026gt;id3(\u0026#34;Store Schedule Internally\u0026#34;); id1(\u0026#34;Send Message via Platform\u0026#34;)--\u0026gt;id4(\u0026#34;AI Analyzes and Modifies Schedule\u0026#34;); id1(\u0026#34;Send Message via Platform\u0026#34;)--\u0026gt;id5(\u0026#34;AI Analyzes and Returns Query Results\u0026#34;) Motion # Motion is an AI-driven work management app primarily targeting entrepreneurial teams, with many features designed for collaboration. During testing, we focused on its schedule arrangement capabilities but found the results underwhelming.\nThe core logic is: divide time into work and personal blocks, then let the AI insert tasks into available slots. The outcome essentially devolves into simple task insertion: if there’s free time, a task is added—even if the deadline is a month away or the day is already packed.\nSummary # Static Schedule Management # Manual adjustments are unavoidable in static schedule management apps and represent the most labor-intensive part of the process. While these apps can create time blocks for scheduling, they lack dynamic adjustment capabilities. This isn’t an issue for fixed, external schedules with clear start and end times.\nHowever, for internal, flexible, or trivial tasks—like memorizing 15 words, doing laundry, or reading a magazine—these tasks often lack specific start times and only need completion by the end of the day. They also lack clear end times; for example, memorizing 15 words might take 30 minutes, but on a good day, it could take only 20.\nThese trivial tasks pose a significant challenge to static scheduling. While a single task’s delay or early completion may not disrupt the schedule, their cumulative effect can be highly disruptive.\nAdditionally, if these scattered tasks aren’t aligned, they can create time vacuums, leaving users unsure of what to do next. For instance, if a user finishes memorizing words early and has 10 minutes to spare, how should they use it? Without clear guidance, they might default to scrolling through short videos, leading to further delays and more time vacuums.\nTo break this vicious cycle, three strategies exist:\nManual Adjustment by the User: Highly cumbersome, wasting precious time on trivial arrangements, and potentially causing further delays. Ignore Adjustments and Time Control: This renders schedule management meaningless, reducing it to a simple to-do list without time control—defeating its purpose. Strict Adherence to the Schedule: Theoretically perfect, but practically ineffective. Life is unpredictable, and delays or early completions disrupt subsequent tasks, leading to rushed work or wasted time. Time vacuums and their cascading effects are the Achilles\u0026rsquo; heel of static schedule management apps, severely limiting their adoption. Consequently, schedule management is often seen as a niche activity for highly disciplined individuals, and the software is perceived as catering only to this small group.\nI’ve used several static schedule management apps but struggled to stick with them: either the learning curve was too steep, or rigid schedules were impractical to follow. Thus, a truly meaningful schedule management tool shouldn’t demand a life free of surprises or strict adherence to plans. Instead, it should provide solutions for when things go off track.\nDynamic Schedule Management # Existing dynamic schedule management apps partially address the inflexibility of static tools but remain constrained by traditional paradigms, requiring extensive manual input. This creates a paradox: if users are inputting so much manually, why use AI at all? Why not rely on faster, more precise algorithms?\nBolder innovations are needed to leverage the full potential of large language models for generalized processing.\nUnderstanding the Current Landscape # Understanding public perceptions and needs regarding schedule management is crucial for defining the app’s form.\nWe conducted a small-scale survey, revealing:\nMost respondents only occasionally plan their schedules. Dedicated schedule management apps are less popular than built-in phone notes. Over half reported that most planned schedules weren’t completed as intended. Among desired features, \u0026ldquo;ease of use\u0026rdquo; ranked highest, followed by \u0026ldquo;personalization.\u0026rdquo; Though the sample size was small, we can infer:\nPublic awareness of schedule management is low. Simplicity and lowering barriers to use should be foundational. Personalization is another key area for optimization. Technical Proposal # Based on the above, we propose the following goals:\nBreak the illusion that mental scheduling is efficient, reduce reliance on deadlines, address the high cost and inflexibility of traditional tools, and solve AI’s inability to capture implicit preferences or balance long- and short-term goals. Make schedule management a practical part of daily life, genuinely improving efficiency and quality of life—truly enabling: Easy Schedule, Simple Life.\nFor feasibility, we exclude: social/relationship management, financial tracking, and team collaboration, focusing solely on individual users’ needs for personal growth and daily task management.\nInterface Design # To keep the interface intuitive, we draw inspiration from life: a boss communicates simply with a secretary to arrange schedules.\nThus, the interface adopts the simplest form: a chat dialog, akin to messaging a personal assistant on QQ or WeChat.\nEssential Data Collection # Since user interaction occurs via chat, all user-related data must be extracted from messages. Key data includes:\nSchedule Data: Tasks the user needs to complete, e.g., \u0026ldquo;Submit academic English homework by next Monday.\u0026rdquo; User Preferences: Unconscious habits, e.g., procrastinating on English homework. Execution Data: Whether scheduled tasks were completed. External Data: Weather, news, etc., fetched from the web. Schedule data comes from user messages, preferences are inferred from adjustments to \u0026ldquo;unreasonable\u0026rdquo; schedules, and external data is retrieved as needed.\nProcessed data is stored long-term in a \u0026ldquo;memory system,\u0026rdquo; technically implemented as RAG: vector databases for schedules/preferences, relational databases for arranged tasks, with AI granted direct access for \u0026ldquo;memory updates.\u0026rdquo;\nData Processing # The \u0026ldquo;memory system\u0026rdquo; uses semantic and temporal searches to populate prompts, guiding the AI’s scheduling decisions.\nA pre-built knowledge base provides scientific principles for scheduling, ensuring plans are both personalized and scientifically sound.\nData Presentation # To maintain simplicity, only two elements are displayed:\nChat Dialog: For input and scheduling. TODO List: For tracking task completion. System Implementation # The system architecture is shown above, with the following workflow:\nUser input is saved to an internal message history database for global access. Parameter Extraction: The LLM extracts parameters from input and handles unsupported messages. Memory Query: Relevant data is fetched from schedules and vector databases, formatted into XML for the LLM to decide the next action and extract parameters. Action Execution: Database APIs update schedules and vector stores based on the LLM’s decision. Response Generation: Update logs are formatted into XML prompts, and the LLM generates a user-friendly response. Schedule Storage Implementation # Vector Schedule Database # Schedule Table # Parameter Extraction Implementation # Memory Query Implementation # Action Decision Implementation # Database Interface Implementation # Response Generation Implementation # Testing Analysis # ","date":"27 January 2025","externalUrl":null,"permalink":"/en/blog/schedule-management-report/","section":"Blogs","summary":"\u003cp\u003eTODO_UPDATE\n\u003cdiv class=\"lead text-neutral-500 dark:text-neutral-400 !mb-9 text-xl\"\u003e\n  Analysis Report on the Current State of Schedule Management Software Functionality\n\u003c/div\u003e\n\u003c/p\u003e\n\n\n\u003ch2 class=\"relative group\"\u003ePreface \n    \u003cdiv id=\"preface\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#preface\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h2\u003e\n\u003cp\u003eThe term \u0026ldquo;schedule management\u0026rdquo; frequently appears in our daily lives, yet it remains elusive: What exactly is schedule management? Why do we need it? What purpose does it serve? Who engages in schedule management, and how effective is it?\u003c/p\u003e","title":"Schedule Management Report","type":"blog"},{"content":" Summary of CUMCM 2024 competition experience, focusing on the A problem and the collaborative approach with MATLAB code improvement Process Overview # graph LR; id1(\"Task Allocation\")--\u003eid2(\"VS Code Collaboration\")--\u003eid3(\"MATLAB Code Execution\"); Task Allocation # There are two key points:\nTasks are divided into two parts: a main part and a secondary part. Tasks should be independent of each other, meaning no dependencies between them. Code Collaboration # Software Tools # VS Code Editor, along with the Live Server plugin, MATLAB plugin MATLAB. Naming Conventions # Main function should be uniformly named main: The main function is the one that directly computes the final result, while others should be called auxiliary functions. Data processing code: This refers to code that does not return any value but generates data tables. It should start with data, e.g., converting solar altitude angle \\(\\phi\\) to cosine value, dataCosPhi. Internal data conversion code: This should start with to, e.g., converting coordinates from a natural coordinate system to a Cartesian coordinate system StoXY. Plotting code: Code related to plotting graphs should start with fig. Testing code: Should start with test. For other special types of functions, they should be named based on their functionality. Functions that return boolean values should start with is, such as a collision detection function isCollided; functions that return other data should start with get. File Documentation Comments # Except for the common types of files mentioned in the naming conventions (data, test, fig, main), all other files should include documentation comments.\nThe documentation should be concise yet clear. It generally includes a brief description of the function, the input parameters, and the output parameters.\nIf you\u0026rsquo;re unsure how to write documentation, you can use AI assistance.\nInternal Variable Naming # Fixed Parameters # All fixed parameters should be placed in a config.m file located in the root directory for centralized management. Each parameter should be followed by a comment explaining its purpose, for example:\nlearningRate = 0.001; % Learning rate batchSize = 32; % Batch size numEpochs = 100; % Number of epochs To use this configuration file in other code files, simply include the following line:\nrun(\u0026#39;config.m\u0026#39;); Even the parameters are transformed into another file, the VS Code can also recognizes it and provide complement suggestion. Function-Internal Parameters # All variables used within a function should be explicitly defined at the beginning of the function and should include brief Chinese comments.\nIf similar parameters are used across multiple files, make sure to use consistent naming, especially in AI-generated code. Use F2 in VS Code to rename them. Code Formatting # Code formatting is mainly handled using the official MATLAB plugin. After you activate the plugin, press Shift+Alt+F to format your code.\nGit Version Control # A GitHub repository needs to be created to store the entire project files. Although the Live Server plugin enables faster real-time code collaboration, it is still necessary to commit and save at key development milestones to maintain the ability to roll back code.\nAll Git version control operations are performed on the lead developer\u0026rsquo;s computer, while other supporting developers use the Live Server plugin for more immediate code collaboration.\nLocal commit to save small improvements push operations are done in major increments Code Execution # Thanks to the latest function of the plugin MATLAB, we can now directly run and debug in the VS Code IDE. Though there are some limitations, it deserves a standing ovation.\nOther # AI Instruction Set # Since generative AI\u0026rsquo;s code style might differ from the project\u0026rsquo;s standards, if you need to generate an entire file, please include the following code standard instructions at the beginning of your request:\nYou are a mature and standardized MATLAB programmer. While correctly implementing the user\u0026#39;s goal, you should follow these code standards: 1. Clear and concise file documentation comments: The documentation should include a brief description of the function, input parameters, and output parameters. 2. Function-internal variable names should be concise and easy to read. 3. All variables used within a function should be explicitly defined at the beginning of the function, with brief Chinese comments added next to them. User\u0026#39;s instruction: Example Project # In order to develop a project as fast and well-defined as possible, I have created an Example Project which contains all the regulations mentioned before. You can directly change the files in the example project without mnemonic cost. 😄\nCode Tips # Parallel Execution\nMATLAB supports multi-threaded computing. You can convert a typical for loop to a parfor loop for parallel execution. The execution of parfor functions is subject to strict requirements. Please refer to the official documentation for more details. Huge Table Process # The comment output of the Mathematical Model is a Huge table in .mat file, which is usually hard to abstract the target data.\nHere I directly write a simply Python program, Data extractor, to automatically operate the huge table data. With tiny modification, you can get any data you want from the .mat file.\nUtilizing the online LaTeX table editor, we can get the capacity to quickly insert the table data into your thesis.\n","date":"16 January 2025","externalUrl":null,"permalink":"/en/blog/code-collaboration-scheme/","section":"Blogs","summary":"\u003cdiv class=\"lead text-neutral-500 dark:text-neutral-400 !mb-9 text-xl\"\u003e\n  Summary of CUMCM 2024 competition experience, focusing on the A problem and the collaborative approach with MATLAB code improvement\n\u003c/div\u003e\n\n\n\n\u003ch2 class=\"relative group\"\u003eProcess Overview \n    \u003cdiv id=\"process-overview\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#process-overview\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h2\u003e\n\u003cdiv class=\"mermaid\" align=\"center\"\u003e\n  \ngraph LR;\nid1(\"Task Allocation\")--\u003eid2(\"VS Code Collaboration\")--\u003eid3(\"MATLAB Code Execution\");\n\n\u003c/div\u003e\n\n\n\n\u003ch2 class=\"relative group\"\u003eTask Allocation \n    \u003cdiv id=\"task-allocation\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#task-allocation\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h2\u003e\n\u003cp\u003eThere are two key points:\u003c/p\u003e","title":"Code Collaboration Scheme","type":"blog"},{"content":" MySQL Installation and Deployment Process + Quick Syntax CookBook + Study Notes Information Source # SQL Tutorial - Liao Xuefeng\u0026rsquo;s Official Website\nThis is an extremely user-friendly MySQL tutorial website with an embedded web-based database, making it easy for beginners to get a hands-on understanding of MySQL operations. It also provides concise yet essential explanations of the background of SQL. MySQL Installation # The simplest way to install MySQL is through Docker Desktop. It takes only two steps to complete:\nRun the following command in your terminal:\ndocker pull mysql Run MySQL With Shell # Then initialize and run the SQL container:\ndocker run -d --name mysql -p 3306:3306 -e MYSQL_ROOT_PASSWORD=password -v /Users/liaoxuefeng/mysql-data:/var/lib/mysql mysql Parameter Explanation:\nParameter Description -d Run the container in the background --name Assign a name to the container (if not specified, Docker will choose one automatically) -p 3306:3306 Map the container\u0026rsquo;s port 3306 to the local machine, allowing you to connect to MySQL through port 3306 -e MYSQL_ROOT_PASSWORD=password Set the root password for MySQL (in this case, the password is password). If not set, Docker will generate a password, which you’ll need to check the logs to retrieve. It’s recommended to set a password. -v /path:/var/lib/mysql Mount a local directory to /var/lib/mysql in the container, which will store MySQL data. Replace /path with the actual directory path on your machine mysql Specifies the name of the Docker image you want to run When using Docker to run MySQL, you can always delete the MySQL container and rerun it. If you delete the locally mounted directory, rerunning the container is equivalent to starting with a fresh MySQL instance. Run MySQL With docker-compose # Create docker-compose.yml：\nservices: mysql: image: mysql ports: - \u0026#34;3306:3306\u0026#34; environment: MYSQL_ROOT_PASSWORD: password volumes: - ../data:/var/lib/mysql restart: unless-stopped Run docker-compose build up -d to start service.\nMySQL Basic Syntax # Querying # Comment Syntax:\n-- This is a comment Basic Query Syntax:\nSELECT * FROM \u0026lt;table_name\u0026gt;; Conditional Query:\nSELECT * FROM \u0026lt;table_name\u0026gt; WHERE \u0026lt;condition\u0026gt;; In the condition expression, you can use various logical operators such as: AND, NOT, OR.\nProjection Query:\nSELECT \u0026lt;column1\u0026gt;, \u0026lt;column2\u0026gt;, \u0026lt;column3\u0026gt; FROM \u0026lt;table_name\u0026gt;; SELECT \u0026lt;column1\u0026gt; alias1, \u0026lt;column2\u0026gt; alias2, \u0026lt;column3\u0026gt; alias3 FROM \u0026lt;table_name\u0026gt;; Sorting:\n-- Sort by score in ascending order: SELECT id, name, gender, score FROM students ORDER BY score; -- Sort by score in descending order: SELECT id, name, gender, score FROM students ORDER BY score DESC; -- Sort by score, gender: SELECT id, name, gender, score FROM students ORDER BY score DESC, gender; -- Sorting with a WHERE condition: SELECT id, name, gender, score FROM students WHERE class_id = 1 ORDER BY score DESC; Pagination Query:\n-- Query the first page: SELECT id, name, gender, score FROM students ORDER BY score DESC LIMIT 3 OFFSET 0; -- Start from the 0th record, fetch up to 3 records, which may be less than 3. Aggregation Query, using MySQL’s aggregation functions:\n-- Count the total number of records: SELECT COUNT(*) FROM students; -- Set the alias for the count result: SELECT COUNT(*) num FROM students; -- Conditional aggregation: SELECT COUNT(*) boys FROM students WHERE gender = \u0026#39;M\u0026#39;; Even though COUNT(*) returns a scalar value, the result is still a two-dimensional table, but with only one row and one column. Other commonly used aggregation functions: MAX(), MIN(), AVG(), SUM(), etc., similar to COUNT().\nGrouped Aggregation Query:\n-- Group by class_id and count records, similar to a for loop: SELECT COUNT(*) num FROM students GROUP BY class_id; -- Include class_id in the result: SELECT class_id, COUNT(*) num FROM students GROUP BY class_id; -- Group by multiple fields, e.g., class_id and gender: SELECT class_id, gender, COUNT(*) num FROM students GROUP BY class_id, gender; Multi-table Query (Cartesian Product):\n-- Querying from students and classes: SELECT * FROM students, classes; -- Set aliases and distinguish columns with table names: SELECT students.id sid, students.name, students.gender, students.score, classes.id cid, classes.name cname FROM students, classes; -- Using aliases for both tables to make it cleaner: SELECT s.id sid, s.name, s.gender, s.score, c.id cid, c.name cname FROM students s, classes c; The result of a multi-table query is still a two-dimensional table, but this table is organized using a Cartesian product, which is why it’s also known as a Cartesian Query.\nJoin Queries, unlike multi-table queries where the tables are combined using Cartesian products, join queries select one table as the main table and combine it with an auxiliary table based on a relationship:\nInner Join (using INNER): -- Select all students and their corresponding class names: SELECT s.id, s.name, s.class_id, c.name class_name, s.gender, s.score FROM students s INNER JOIN classes c ON s.class_id = c.id; Outer Join, with three variations: RIGHT, LEFT, FULL: -- Using RIGHT OUTER JOIN: SELECT s.id, s.name, s.class_id, c.name class_name, s.gender, s.score FROM students s RIGHT OUTER JOIN classes c ON s.class_id = c.id; Understanding method: You can think of it as sets, where tableA is the main table (also known as the left table) and tableB is the related table (the right table).\n-- Here tableA is the main table, also known as the left table, and tableB is the related table. SELECT ... FROM tableA ??? JOIN tableB ON tableA.column1 = tableB.column2; Modifying Data # Inserting Data: -- Insert a new record: INSERT INTO students (class_id, name, gender, score) VALUES (2, \u0026#39;Daniu\u0026#39;, \u0026#39;M\u0026#39;, 80); -- Insert multiple records at once: INSERT INTO students (class_id, name, gender, score) VALUES (1, \u0026#39;Dabao\u0026#39;, \u0026#39;M\u0026#39;, 87), (2, \u0026#39;Erbao\u0026#39;, \u0026#39;M\u0026#39;, 81), (3, \u0026#39;Sanbao\u0026#39;, \u0026#39;M\u0026#39;, 83); Updating Data: -- Update the record with id=1: UPDATE students SET name=\u0026#39;Daniu\u0026#39;, score=66 WHERE id=1; -- Update records with score \u0026lt; 80: UPDATE students SET score=score+10 WHERE score\u0026lt;80; -- Update record with id=999, but no matching records will be found: UPDATE students SET score=100 WHERE id=999; -- Without a WHERE clause, the update will affect all records in the table: UPDATE students SET score=60; Deleting Data: -- Delete the record with id=1: DELETE FROM students WHERE id=1; -- Deleting without a WHERE clause will remove all records from the table: DELETE FROM students; Here\u0026rsquo;s a fluent and natural English translation:\nCreating Database and Tables # Creating the Database: CREATE DATABASE your_db_name -- name of the database CHARACTER SET utf8mb4 -- character set of the database COLLATE utf8mb4_unicode_ci; -- collation rule Creating Tables: CREATE TABLE table_name ( column1 datatype constraints, column2 datatype constraints, column3 datatype constraints, ... PRIMARY KEY (primary_key_column) ); -- Example of a user table CREATE TABLE users ( id INT AUTO_INCREMENT, username VARCHAR(50) NOT NULL UNIQUE, email VARCHAR(100) NOT NULL UNIQUE, password VARCHAR(255) NOT NULL, created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP, PRIMARY KEY (id) ); Creating Triggers： CREATE TRIGGER trigger_name BEFORE INSERT ON orders FOR EACH ROW BEGIN SET NEW.order_time = NOW(); END; ","date":"15 January 2025","externalUrl":null,"permalink":"/en/blog/mysql-basics/","section":"Blogs","summary":"\u003cdiv class=\"lead text-neutral-500 dark:text-neutral-400 !mb-9 text-xl\"\u003e\n  MySQL Installation and Deployment Process + Quick Syntax CookBook + Study Notes\n\u003c/div\u003e\n\n\n\n\u003ch3 class=\"relative group\"\u003eInformation Source \n    \u003cdiv id=\"information-source\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#information-source\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://liaoxuefeng.com/books/sql/introduction/index.html\" target=\"_blank\"\u003eSQL Tutorial - Liao Xuefeng\u0026rsquo;s Official Website\u003c/a\u003e\u003cbr\u003e\nThis is an extremely user-friendly MySQL tutorial website with an embedded web-based database, making it easy for beginners to get a hands-on understanding of MySQL operations. It also provides concise yet essential explanations of the background of SQL.\u003c/li\u003e\n\u003c/ul\u003e\n\n\n\u003ch3 class=\"relative group\"\u003eMySQL Installation \n    \u003cdiv id=\"mysql-installation\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#mysql-installation\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h3\u003e\n\u003cp\u003eThe simplest way to install MySQL is through Docker Desktop. It takes only two steps to complete:\u003c/p\u003e","title":"MySQL Basics","type":"blog"},{"content":" Familiarizing oneself with the project analysis process, this is a practical analysis report. After I had completed my small plugin, I realized that there doesn\u0026rsquo;t seem to be a one-click solution in the note-to-blog field. So, I wrote this article to analyze whether it would make sense to create a new project to fill this gap.\nThere is no such thing as a \u0026ldquo;better\u0026rdquo; or \u0026ldquo;worse\u0026rdquo; project; there is only whether it is suitable or not. The evaluation criteria in this article are based on whether they meet the requirements of a blog website. Therefore, some projects may not be suitable for creating a blog site, but this does not diminish their value.\nField Definition # Before we begin the analysis, it\u0026rsquo;s important to have a clear understanding of the note-to-blog field.\nNotes: In this context, \u0026ldquo;notes\u0026rdquo; specifically refers to the notes in Obsidian, which use Obsidian Flavored Markdown. This adds certain complexities to the process of converting notes to a webpage.\nBlog: A blog, by definition, is a means to gain traffic. Creators spend time writing notes, and then using conversion software, generate beautifully formatted, feature-rich blog websites.\nEvaluation criteria are as follows:\nPrivacy:\nDoes it run locally? Is it open-source? Usability:\nHow well does it adapt to Obsidian\u0026rsquo;s syntax? How complex is the service deployment? How detailed is the documentation? How easy is it to customize settings? Web Functionality:\nDoes the default web template include all essential functions (search, day/night mode, etc.)? How visually appealing is the default webpage? Does it support SEO? How smoothly does it convert Obsidian\u0026rsquo;s native syntax (for example, are there untranslatable code blocks in internal links, or does it discard some Obsidian syntax features)? Project Overview # In my vision, this project would be an Obsidian plugin that seamlessly exports Obsidian notes into Hugo blog webpages, supporting all of Obsidian\u0026rsquo;s basic core features.\nResult: It greatly reduces the cost of creating a blog webpage, allowing anyone who can use Obsidian to have their own blog.\nMarket and User Feasibility Analysis # Market Demand Analysis # Overview # Basic Needs: The demand to build a personal website and continuously produce content, including for self-improvement, self-expression, and creating a unique and comprehensive personal skill showcase (for corporate recruitment), etc.\nTarget User Group: Heavy Obsidian users who want to share notes; knowledge creators who want to build a personal blog but have abandoned the idea due to technical difficulty.\nRelevant Data # Flowershow: As of October 2024, the plugin had been downloaded 3,355 times; by January 2025, this number had risen to 4,594, while the most downloaded plugin had reached 3,211,992 downloads. Quartz: As of January 2025, Quartz had accumulated 7.7k stars on GitHub. Existing Solutions # Quartz (Hugo) # Recommendation: ❤️‍🔥❤️‍🔥❤️‍🔥❤️‍🔥❤️‍🔥\nIntroduction # Quartz is a toolset that converts Obsidian notes into web pages. The latest version, v4, has undergone a complete rewrite compared to v3, removing its dependency on Hugo and optimizing the user customization experience. The v4 version is now primarily built with TypeScript, and the original Hugo templates have been replaced with JSX.\nAs a result, Quartz in its current form is almost entirely disconnected from Hugo. However, much of the information available in Internet still advertises Quartz as being built on top of Hugo.\nOfficial example website: Welcome to Quartz 4\nReview # Pros:\nExtremely complete functionality The only toolset that successfully handles display wiki links Detailed documentation Cons:\nVirtually no drawbacks, but one notable limitation is the lack of Chinese documentation. Summary: An excellent project, where the styles displayed in Obsidian are the same as those displayed on the webpage. It has garnered the most stars on GitHub among all available solutions.\nFlowershow # Recommendation: ❤️‍🔥❤️‍🔥❤️‍🔥❤️‍🔥🩶\nIntroduction # Flowershow is an overall publishing service based on Obsidian, which can convert your Obsidian notes into an online digital garden website with directory structure. Vercel is a cloud service for front-end deployment, enabling serverless front-end deployment via GitHub. Each content submission triggers automatic deployment. For domestic users, Netlify is an alternative.\nSubjectively, the development team behind Flowershow is very passionate and mission-driven. Their core philosophies are detailed in their About page.\nObjectively, Flowershow\u0026rsquo;s positioning as a blog webpage generation platform based on Obsidian is spot-on, and the final result is very good both from a front-end and back-end perspective.\nReview # Pros:\nClear positioning and a straightforward workflow Comprehensive feature support Professional team behind maintenance and operations Highly customizable, suitable for creators who enjoy personalizing their setup Cons (as of January 2025):\nSome Obsidian features are not handled, such as display wiki links. At least this section is omitted in the documentation. A reverse link feature is mentioned on the homepage, but it’s unclear in the site’s details. Summary: Overall, the project is well done, but some details still need improvement. This solution is suitable for creators who don\u0026rsquo;t require high support for Obsidian syntax.\nOfficial Publish # Recommendation: ❤️‍🔥❤️‍🔥❤️‍🔥🩶🩶\nIntroduction # Examples of websites using Obsidian\u0026rsquo;s official publishing service:\nObsidian Chinese Tutorial - A Chinese tutorial website, using the official Obsidian publishing service. Digital 3D Garden - Deep front-end customizations. Mister Chad - A simple, neat site with rich content. Discrete Structures for Computer Science - Official simple style. Review # Pros:\nThe official publish service offers top-notch support for Obsidian’s internal representations, ensuring all Obsidian features are correctly displayed on the webpage. Continuous maintenance ensures quick adaptation to updates from Obsidian. Highly customizable settings for users with coding experience, and a wide range of themes from other developers. Privacy settings, password protection, and access control for internal document management. SEO support and mobile platform adaptation for greater potential traffic. Cons:\nCosts $8 per month. Since personal websites typically have very little traffic initially, this can become a significant expense over time. This is the major drawback of the official service. If you stop paying, the website becomes inaccessible. Limited support in certain regions, with traffic constraints in China. Summary: The official service is suitable for users with sufficient funds and moderate customization needs.\nDigital Garden # Recommendation: ❤️‍🔥❤️‍🔥❤️‍🔥🩶🩶\nIntroduction # Digital Garden is an Obsidian plugin that exports notes as webpages and hosts them on GitHub, with deployment via Vercel or Netlify.\nExample sites:\nDigital Garden - Official example Aaron Youn - Created by an individual user John\u0026rsquo;s Digital Galaxy - Rich content showcasing Digital Garden’s features, including display links. Review # Pros:\nComprehensive feature support Supports Obsidian theme migration Cons:\nNot friendly with Chinese paths Web interface customization requires direct handling of source code (HTML, JavaScript, CSS), and the default interface is not very visually appealing. Summary: The workflow is simple, and the feature support is extensive. However, the interface requires effort to improve, and creators who care less about aesthetics can jump straight into using it.\nPerlite # Recommendation: ❤️‍🔥❤️‍🔥🩶🩶🩶\nIntroduction # Perlite is an open-source alternative to Obsidian\u0026rsquo;s official publishing service, providing a browser\n-based file reader with an interface nearly identical to Obsidian’s.\nReview # Pros:\nSupports almost all Obsidian features. Classic native interface, offering a familiar experience for users. Cons:\nIt is not a blog page but a \u0026ldquo;file reader\u0026rdquo; instead. Requires Docker, which can result in slower startup times compared to the simplicity of a plugin experience. Summary: Perlite is best suited for those who need a browser-based Obsidian experience, rather than as a public-facing blog.\nJekyll + Netlify + GitHub Pages # Recommendation: ❤️‍🔥❤️‍🔥🩶🩶🩶\nIntroduction # This method is derived from obsidian\u0026rsquo;s most perfect free publishing solution.\nExample website by the author: oldwinter’s Digital Garden\nReview # Pros:\nSimple configuration Highly customizable Cons:\nDoes not support certain Obsidian features like callout syntax No dark mode support No search functionality Summary: A good solution for converting Obsidian to a blog, but missing some core features, making it unsuitable for creators seeking a complete web experience.\nTiddlyWiki # Recommendation: ❤️‍🔥🩶🩶🩶🩶\nIntroduction # TiddlyWiki is an older note-taking framework that remains very active today, with developers continuously enhancing it.\nReview # Pros:\nExtremely simple and lightweight Widely used with a strong user base Domestic services available with no need for VPNs Cons:\nThe simplicity might result in a somewhat primitive interface. Not a full-fledged personal blog; lacks SEO and is difficult to access via search engines, limiting traffic potential. Summary: TiddlyWiki is ideal for personal note storage but not for creators seeking a blog that attracts traffic.\nConclusion # Before conducting a thorough analysis, I was unaware of the actual landscape in the note-to-blog field, which led me to consider creating a simplified plugin. 💡\nHowever, after systematic research, I must admit that Quartz stands out as the best project in this space. Whether it\u0026rsquo;s the adaptation to Obsidian\u0026rsquo;s syntax, ease of configuration, front-end aesthetics, customization options, or backend blog creation, there is very little room for improvement.\nThus, there is no need for me to initiate a project to duplicate what’s already been done. I salute all the teams involved in the note-to-blog field, whether mentioned in this article or not. 🫡\nThere are no \u0026ldquo;better\u0026rdquo; or \u0026ldquo;worse\u0026rdquo; projects, only those that are suitable or not. The evaluation criteria in this article focus on whether the solution meets the requirements for a blog webpage, and thus, some projects may not be ideal for blogging but still offer great value.\nSaluting open-source pioneers! 🫡🫡🫡\n","date":"10 January 2025","externalUrl":null,"permalink":"/en/blog/note-to-blog-report/","section":"Blogs","summary":"\u003cdiv class=\"lead text-neutral-500 dark:text-neutral-400 !mb-9 text-xl\"\u003e\n  Familiarizing oneself with the project analysis process, this is a practical analysis report.\n\u003c/div\u003e\n\n\u003cp\u003eAfter I had completed my \u003ca href=\"https://morethan987.github.io/en/blog/plugin-writing-experience/\"\u003esmall plugin\u003c/a\u003e, I realized that there doesn\u0026rsquo;t seem to be a one-click solution in the \u003cstrong\u003enote-to-blog\u003c/strong\u003e field. So, I wrote this article to analyze whether it would make sense to create a new project to fill this gap.\u003c/p\u003e","title":"Analysis Report on the Note-to-Blog Project","type":"blog"},{"content":" A blog website journey, from hand-coding to Hugo, a story of twists and turns. Why Hugo? # It all started with hearing that Hugo could generate webpages and that it was incredibly efficient at compiling static pages. I decided to dive into the research—Hugo is said to be the fastest static site generator in the world, as claimed on its official website.\nOf course, words are just words, so here’s the output I got when I compiled and ran Hugo locally without the public folder at the beginning:\nZH-CN EN Pages 53 51 Paginator pages 0 0 Non-page files 13 13 Static files 7 7 Processed images 3 0 Aliases 18 17 Cleaned 0 0 Built in 872 ms\nIn total, compiling 104 pages (both Chinese and English) took just 0.872 seconds, including the time to build the local server. That speed is hard to criticize. And the local server can listen for changes to the source code in real time and do incremental refactoring, depending on the size of the change, usually around 0.03 seconds.\nI haven\u0026rsquo;t used other page generators for setting up blogs, so I can\u0026rsquo;t compare Hugo\u0026rsquo;s speed with others. References # Here are all the resources I used during the blog setup process:\n莱特雷-letere This is a blogger’s site also built with Hugo. It contains a lot of tutorials on other web tools as well. The series is also available on Bilibili video tutorial. Blowfish This is the Hugo theme I used, and the documentation is excellent. I’ve never seen such a patient author. Official Hugo Website Hugo Themes Full Deployment Process # Setting Up Hugo # This part is covered in detail in the webpage and video tutorial by the blogger. If you don’t like reading text, you can follow the video tutorial. 😝\nHonestly, setting up Hugo is one of the easiest setups I\u0026rsquo;ve ever seen, no exaggeration. You simply download Hugo from the official website, place it in a folder, and unzip it. You’ll find just one file, hugo.exe—it’s that simple.\nHugo is really convenient. I tried Hexo before, but the Node.js setup turned me away. Even now, I have no idea why it failed to compile. 😢 The only slight difficulty is adding the directory containing hugo.exe to your environment variables.\nCreating a Template System # Open the terminal in the folder where hugo.exe is located and run the command hugo new site your-site-name. You’ll see a new folder appear in the current directory.\nThe template system sounds advanced, but it’s just a special folder structure created in the same directory as hugo.exe. You can’t arbitrarily modify its contents because each folder has a specific purpose.\nName Purpose asset Stores images, icons, and other assets used by the website config Website configuration folder (may not exist initially; some themes require it) hugo.toml One of the website configuration files content All content goes here public The folder containing the fully compiled website (empty initially) themes Stores your website’s themes Basic Theme Configuration # Hugo has a lot of themes, and you can browse them on Hugo Themes. You can download the theme you like and place it in the themes folder. The process might sound abstract, but you can check out the video tutorial for more guidance.\nHere’s one important tip: most themes come with a sample site located in the exampleSite folder. If you don’t want to configure everything from scratch, you can just use the sample configuration.\nAfter configuring the theme, you’ll need to customize it. I highly recommend the Blowfish theme, which is fantastic, and I truly respect the author 🫡.\nBlowfish Theme # The Blowfish official documentation is incredibly detailed, so I won’t repeat it here. Any additional words would be disrespectful to such a comprehensive guide 🫡.\nHowever, there are some issues you might encounter, and I’ll briefly mention them below. You should carefully read the official documentation to fully understand these points 🤔.\nWhy does the \u0026ldquo;Recent Articles\u0026rdquo; section still show up even if params.homepage.showRecent = false is set? If you face this issue, it’s likely because, like me, you lazily used the exampleSite configuration. This is because the homepage layout is controlled by more than one interface, and another one is located in the layouts\\partials\\home\\custom.html file.\nIf you don’t mind, just ignore it. But if you care (like I did 🤪), you can comment out the following code in the file:\n\u0026lt;section\u0026gt; {{ partial \u0026#34;recent-articles-demo.html\u0026#34; . }} \u0026lt;/section\u0026gt; Why doesn’t the logo change between day and night modes when I use an svg logo? This is a bug I discovered, and I’ve already submitted an improvement to the theme author. See code improvement or SVG support\nWhy does the small icon in the browser window still show the blowfish logo even after I change the site’s logo? This is mentioned in the official documentation, but it’s buried deep. You can find it under Partials Documentation for Blowfish.\nTo be honest, the official documentation is excellent. 👍 After going through the entire process, I only encountered a few minor issues that were not easy to understand 😋.\nPlugins I Use # I prefer using Obsidian for writing articles. However, the format used by Obsidian and the one used by Blowfish is quite different, so converting between the two can be a hassle 🤔.\nAfter searching around, I found that there weren’t any suitable plugins! So, I developed my own plugin: Hugo-Blowfish-Exporter.\nWhile the plugin is simple, it covers almost all of my needs, including:\n- Callouts (supports all official callout names, with additional icons) - Inline math formulas (Blowfish supports block-level formulas)\n- Mermaid (supports Mermaid diagrams)\n- Image embedding (automatically exports images)\n- Wiki-style links (only support the none-displayed link 😢)\nThe none-displayed link is simply exported as normal hyperlink in the HTML file;\nThe displayed link is more complex: change(override) the source code of Blowfish to support the file injection through the shortcode, mdimporter; every Obsidian file should includes a meta data slug to tag the folder that contains the target markdown file in your website repository.\nThe overriding of the theme\u0026rsquo;s source code can be found in the mdimporter and the stripFrontMatter used to remove metadata from the injected files\u0026rsquo; headers. For overriding the directory, refer to the configuration on GitHub.\nI put a lot of effort into this plugin, even though it only took a few days 🤔. But those few days were quite exhausting 😵‍💫.\nIf this plugin helps you, feel free to share it! If you’re not happy with the functionality, you can submit an issue on GitHub 🫡. If you\u0026rsquo;re familiar with the code, you can modify it directly; the code is well-commented and quite standard 🤗.\nAnd if you modify and upgrade the code, I’d be very grateful if you share your changes with me (via a pull request on GitHub)! ☺️\nFinal Thoughts # Setting up a blog site is just the first step in a long journey; the real challenge is filling it with content.\nAs I mentioned in An experience of writing plugins, many personal blogs fade into obscurity in as little as a year, from the initial burst of excitement to the eventual silence.\nIn this fast-paced world, most meaningless and inefficient things are eventually replaced by efficiency, and the original enthusiasm and dreams often compromise with reality. I too no longer have the passion I once had, and my actions have become more like those of a real adult.\nBut there\u0026rsquo;s still a bit of unwillingness in me. This website is a form of resistance, and I’ll do my best to maintain it. That’s also why I developed the plugin—to make updating the blog easier.\nI hope this tutorial helps anyone planning to set up their own blog. Let’s keep moving forward, together 🫡.\n","date":"7 January 2025","externalUrl":null,"permalink":"/en/blog/hugo-blog/","section":"Blogs","summary":"\u003cdiv class=\"lead text-neutral-500 dark:text-neutral-400 !mb-9 text-xl\"\u003e\n  A blog website journey, from hand-coding to Hugo, a story of twists and turns.\n\u003c/div\u003e\n\n\n\n\u003ch2 class=\"relative group\"\u003eWhy Hugo? \n    \u003cdiv id=\"why-hugo\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#why-hugo\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h2\u003e\n\u003cp\u003eIt all started with hearing that Hugo could generate webpages and that it was incredibly efficient at compiling static pages. I decided to dive into the research—Hugo is said to be the \u003cstrong\u003efastest static site generator\u003c/strong\u003e in the world, as claimed on its official website.\u003c/p\u003e","title":"Hugo Blog Setup","type":"blog"},{"content":" A Reflection on Writing a Plugin and What I Learned from the Experience. The Beginning # It all started with my blog website. I stumbled upon an article on WeChat about building a blog with Hugo, and since I wanted to revamp my old, simple site, I decided to give it a try. My original site was extremely rudimentary, and the whole writing process involved jumping between HTML, JS, and CSS in a rather awkward manner. On top of that, I had always admired the blog of a great tech guru, Lilian Weng, which was also built with Hugo. This further strengthened my resolve to change my site\u0026rsquo;s underlying platform.\nSo, I quickly started diving into Hugo.\nTo my surprise, the results were extraordinary! My old webpage took me nearly a month to build, but with Hugo, I was able to finish everything in less than half a day. What shocked me even more was that Hugo, a program written in Go, didn’t require users to set up a Go environment! 😮\nAt the same time, I discovered an incredibly well-documented Hugo theme—Blowfish. This was by far the most detailed documentation I had ever seen for any project, bar none (๑•̀ㅂ•́)و✧.\nWith Hugo and Blowfish working in tandem, my small site quickly took shape. Of course, I’m not great at designing, so I just used the default layout from Blowfish, as I felt any changes would ruin the beauty of the page.\nTo be honest, after all this work, I didn’t have any strong emotional reactions, except for deep respect for the coding skills of the authors of Hugo and Blowfish.\nThat was until I wanted to upload the massive amount of notes I had in Obsidian to my new blog.\nThe Bitter Taste of Originality # I soon realized that there wasn’t a plugin available to directly convert the format of my Obsidian notes to fit the Blowfish theme. Fueled by the earlier \u0026ldquo;pleasant experience,\u0026rdquo; I decided to write a plugin myself! (😄 Although, I would soon stop laughing 😢)\nThe rest of the experience wasn’t anything particularly exciting—just endless switching between webpages, searching through API documentation, and never-ending conversations with AI bots. After countless revisions, I finally ended up with something exceedingly simple: a plugin that identifies specific patterns in documents and performs content replacement.\nIt was quite laughable. Compared to the few hours it took to set up the website, the nearly forty hours I spent writing that plugin felt almost negligible. At one point, I seriously considered just deleting my few hundred lines of code.\nYes, such a simple plugin drained me mentally and physically. I truly tasted the bitterness of originality.\nNow, looking back at Hugo and Blowfish, I feel deeply shocked by their complexity and the effort required to implement all of those features. If they were getting paid for this work, I could at least understand the level of effort involved. But they were both open-source, relying entirely on user goodwill and appreciation.\nI saw the last update of the Blowfish author’s blog, which was in March 2024, and I fell into deep thought.\nSentiments and Idealism # I imagine that the author of Blowfish must have paused the development of the theme for some reason—perhaps due to life circumstances. After all, this project didn’t bring in much real income.\nSuddenly, I remembered the changes I had noticed before—those GitHub profiles, once full of green squares, gradually becoming sparse, and eventually disappearing. Beneath this peaceful change, there might be a shift in someone\u0026rsquo;s life. Whether it\u0026rsquo;s because of busy work or the gradual fading of motivation, the original passionate drive eventually drowns in silence. I can\u0026rsquo;t stop this from happening, but I understand the reasons behind it.\nOpen-source is driven by passion, but passion doesn’t pay the bills. People need to live in the present.\nI recalled a tech YouTuber, Ma Nong Gao Tian, a core Python developer who humorously complained about the harsh realities of open-source life. His prematurely graying hair made me feel a pang of empathy—he had spent most of his life writing code and yet found himself out of work, surviving on a few extra bucks from his videos.\nIn Conclusion # Life is rarely as we wish. Once again, I looked at my forty-plus hours of work and couldn’t help but laugh and shake my head.\nAfter writing this, I’m off to bed. It’s now 1:48 AM on January 6, 2025, and I still haven’t reviewed for my English final exam tomorrow.\nLooking at this blog again, I laughed and shook my head.\nSuch is life.\n","date":"6 January 2025","externalUrl":null,"permalink":"/en/blog/plugin-writing-experience/","section":"Blogs","summary":"\u003cdiv class=\"lead text-neutral-500 dark:text-neutral-400 !mb-9 text-xl\"\u003e\n  A Reflection on Writing a Plugin and What I Learned from the Experience.\n\u003c/div\u003e\n\n\n\n\u003ch3 class=\"relative group\"\u003eThe Beginning \n    \u003cdiv id=\"the-beginning\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#the-beginning\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h3\u003e\n\u003cp\u003eIt all started with my blog website. I stumbled upon an article on WeChat about building a blog with \u003ccode\u003eHugo\u003c/code\u003e, and since I wanted to revamp my old, simple site, I decided to give it a try. My original site was extremely rudimentary, and the whole writing process involved jumping between \u003ccode\u003eHTML\u003c/code\u003e, \u003ccode\u003eJS\u003c/code\u003e, and \u003ccode\u003eCSS\u003c/code\u003e in a rather awkward manner. On top of that, I had always admired the \u003ca href=\"https://lilianweng.github.io/\" target=\"_blank\"\u003eblog\u003c/a\u003e of a great tech guru, \u003ccode\u003eLilian Weng\u003c/code\u003e, which was also built with \u003ccode\u003eHugo\u003c/code\u003e. This further strengthened my resolve to change my site\u0026rsquo;s underlying platform.\u003c/p\u003e","title":"An experience of writing plugin","type":"blog"},{"content":"Morethan\u0026rsquo;s dummy blog page~\n","date":"3 January 2025","externalUrl":null,"permalink":"/en/blog/moravecs-paradox/","section":"Blogs","summary":"\u003cp\u003eMorethan\u0026rsquo;s dummy blog page~\u003c/p\u003e","title":"The reflection of Moravec's paradox","type":"blog"},{"content":" Preface # This article is primarily a review and summary of the entire process of CUMCM 2024.\nOur team was formed in the winter of 2023, and CUMCM 2024 was our first participation in the \u0026ldquo;Mathematical Modeling\u0026rdquo; competition. After numerous mock contests, we finally made it to the national competition. After submitting the final paper, we won the first prize at the provincial level and were recommended for the first prize at the national level, ultimately receiving the second prize at the national level.\nThere were moments of excitement and surprise, as well as disappointment; we must have done some things right in the competition, which is why we won a national award in our first attempt; but there are definitely shortcomings, after all, there must be a reason for going from \u0026ldquo;recommended for the first prize at the national level\u0026rdquo; to \u0026ldquo;second prize at the national level\u0026rdquo;.\nIn short, this experience is truly unforgettable, and it is even more worth summarizing and learning from the experience to prepare for next year\u0026rsquo;s competition.\nCUMCM stands for Chinese Undergraduate Mathematical Contest in Modeling; it is commonly referred to as the \u0026ldquo;National Mathematical Modeling Competition\u0026rdquo;. Terminology Explanation # Term Explanation Computational System The traditional modeling process, encapsulating a large function Optimization System A system used to optimize the adjustable parameters in the computational system to generate the best parameter configuration Computational Flow The process of handling input data in the computational system Computational Flow Node A key intermediate step in the workflow Optimization Flow The main logic of the optimization system Main Body of the Paper Includes the abstract, restatement, descriptions of computational and optimization flows, results presentation and analysis, that is, all content before the conclusion of the paper Conclusion of the Paper Includes sensitivity analysis and model extension Objective Conditions # Task Division # Although there were many topics to choose from for the competition, our group chose to focus on optimization problems, which is Topic A.\nMe: Modeling + Coding + Part of Paper Writing CL: Modeling + Paper Writing + Part of Coding HWJ: Paper Beautification Workflow # The coding part of the entire Topic A can be roughly divided into two systems:\nComputational System: Function: Accept input data and parameters, return the required results Nature: Directly determined by the problem, different topics have different computational systems, which need to be constructed temporarily Optimization System: Function: Accept the computational system as the target function to be optimized, execute its own optimization logic, and finally return the computational results Nature: The method system is relatively mature and can be prepared in advance of the competition with various optimization systems The paper writing part is divided into:\nOverall Framework: Determined by the LaTeX template Main Content Filling: Clear description of the workflow and optimization flow Typesetting and Beautification: Adjust the details of each part, with illustrative images (flowcharts, schematics) Concluding Content Pre-Modeling # Objective: Under the premise of accurately understanding the problem, quickly carry out preliminary modeling, basically determine the direction of modeling and calculation methods;\nEstimated Time: 3 hours\nWork: All team members conduct a web search to see if there are any literature materials that basically hit the topic.\nHit Successful: The most ideal situation, at this time, you can directly study the papers and collect ideas; Hit Unsuccessful: Although there are no ready-made materials for reference, some ideas have been accumulated in the process of literature review. Early Modeling # Overall Objective: Construct a precise and optimization method adaptable computational system\nModeling: Clarify the operations between input data and each computational flow node Coding: Implement the computational flow with code and achieve data visualization Paper: Fill in the content of the first question and initially typeset Estimated Time: 30 hours\nWork:\nAll team members model together, first clarify the modeling ideas, and provide a complete mathematical derivation process Me and CL: Code implementation and paper content filling are carried out simultaneously HWJ: Draw more vivid schematic diagrams that cannot be generated by code Mid-Modeling # Overall Objective: Construct a suitable optimization system\nModeling: According to the particularity of the computational system, choose the most matching optimization system Coding: Make minor changes in the implementation of the optimization system to match the computational system Paper: Complete the main part of the paper and start local detail fine-tuning Estimated Time: 20 hours\nWork: Similar to the previous, but the focus of work has shifted from code writing to paper writing\nSimplify the paper, at this time, the paper is very bloated Fine-tune the logic of the paper to make the context more closely related Beautify the typesetting, reduce text, increase images Late Modeling # The basic modeling is completed, and all members check for loopholes: Conventional checks such as typos, inaccurate expressions, formula spelling errors, etc. Optimize code comments to make them more readable Focus on checking personal information Personal information must not be retained in the competition paper, including file paths in the code, such as C:\\Users\\Morethan; retaining personal information is a very serious mistake! Actual Combat Effectiveness # When we applied the above strategies to the actual combat process, that is, the formal competition of CUMCM 2024, the results were as follows:\nEffective Time: The total duration of the competition is three days, a total of 72 hours The team works from seven in the morning to eight in the evening, excluding meal times, with an effective time of 12 hours a day Time utilization rate is \\(50\\%\\) (quite low in comparison🤔) Completed Work: The main body of the paper is 28 A4 pages The code part is 35 A4 pages, excluding the reused code between each sub-question, there should be about 20 pages A total of 25 illustrations in the paper The above data is after the paper has been streamlined, with the initial draft of the paper being nearly 50 pages Uncompleted Work: The final result calculation, due to the large amount of calculation (the code efficiency is not high), the code was finished two hours in advance, but there was not enough time to calculate the results😭😭 The calculation accuracy of the model is not enough, the accuracy is 1s which does not meet the standard answer\u0026rsquo;s precision The conclusion part of the paper was not actually completed Strengths # Topic Selection # Focused on Topic A, accumulated sufficient experience in mock contests, and polished a set of efficient workflows\nThe methodology for Topic A is relatively well-constructed\nWorkflow # The workflow is relatively clear, and the efficiency is high\nGuided by the final paper, modeling, paper, and code are carried out simultaneously, ensuring sufficient content in the paper\nDivision of Labor # Adopted a blurred division of labor, each team member has a main job and a secondary job, can work independently on their main job, and can also complete some work on the secondary job, greatly improving time utilization\nThe team members are very capable, as handling two division tasks means more learning costs\nWeaknesses # Workflow # The plan is perfect, but some necessary links were not well done in practice\nEffective time ratio: finishing work at eight in the evening is too early! More time should be taken to model trial and error to ensure the correctness and accuracy of the model\nDivision of Labor # The code writing, code debugging, code visualization, result calculation, and result visualization involve too much code, which is difficult for one person to handle;\nTask overlap caused by blurred division of labor increases collaboration costs\nModeling # Topic understanding accuracy: This time, there was a significant deviation in our understanding of the topic, which led to wasting a lot of time on model correction; Code # Code efficiency: Due to no time limit before, there was insufficient preparation for \u0026ldquo;very long\u0026rdquo; code, no experience with code parallelism;\nResult precision: The initial modeling was too rough, and a bad characteristic was used: setting the time step to 1, and using it as an array index, which made it difficult to reduce the time step later, resulting in insufficient precision of the final results\nImprovement Plans # Carefully select the venue, increasing effective time✨is the most important✨ Division of Labor # Slightly change the division of labor, increase the investment of human resources in coding\nIncrease learning input in each main and secondary division to increase work efficiency\nModeling # Focus more on understanding the topic, don\u0026rsquo;t rush; correcting modeling errors is not worth the loss Code # Build a set of effective code collaboration plans to enhance code writing speed\nStart building code writing standards:\nVariable naming Documentation at the beginning of the file Code writing process standards Code parallelization: Add some parallelizable code to the code to increase running speed\nAll code improvements must be implemented in a document! Not just slogans! The final output: Code Collaboration Scheme Paper # Study excellent papers\nPay attention to its paper framework Pay attention to its language style, text readability, detail, illustration logic, and image readability Improve ourselves\nOptimize the paper\u0026rsquo;s main logic framework, refine the content of each section Improvements in language style, text readability, detail, illustration logic, and image readability, etc. The results are fixed in the form of comments in the LaTeX template! Summary # A test paper without full marks is more rewarding than one with full marks!\nAccumulating knowledge of applied mathematics, enhancing paper writing skills, and improving the ability to discover problems are more meaningful than the competition itself. 🫡\nCUMCM, every MathModeler can benefit from it. 🤗\n","date":"12 September 2024","externalUrl":null,"permalink":"/en/blog/cumcm2024/","section":"Blogs","summary":"\u003ch2 class=\"relative group\"\u003ePreface \n    \u003cdiv id=\"preface\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#preface\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h2\u003e\n\u003cp\u003eThis article is primarily a review and summary of the entire process of CUMCM 2024.\u003c/p\u003e","title":"CUMCM 2024 Summary","type":"blog"},{"content":" Virtual Env # Plain Python Method # Creat # Some tipical code 👇\n# creat a virtual env named \u0026#34;your_env_name\u0026#34; python -m venv your_env_name # assign the version of python, make sure your python in default direction python -m venv your_env_name --python=python3.11 # simply list the absolute direction of python, simple and efficient D:/PythonPython311/python.exe -m venv your_env_name More parameters you may need for a customized virtual env. 🤔\nParams Meaning --system-site-packages Give the virtual environment access to the system site-packages dir. --clear Delete the contents of the environment directory if it already exists, before environment creation. --version print the python version of the env All the detailed expaination of the parameters can be got by the code python -m venv -h. No need to search everywhere~😆 Activate # The virtual env is defaulted not active. In the direction your_env_name/Scripts/ will be a file named activate. Run it with your cmd.\n# activate virtual env your_env_name/Scripts/activate Poetry # Poetry is a Python package management tool that started development in February 2018, according to its changelog. It\u0026rsquo;s not exactly a new tool, but it has become quite popular in recent years 💫.\nThe key highlights promoted by its official documentation include:\nMore detailed and comprehensive analysis of third-party dependencies Automated virtual environment creation More intuitive and user-friendly commands 🤔 Installation # It\u0026rsquo;s straightforward: pip install poetry for global installation.\nGlobal Configuration # Poetry has some unique mechanisms that differ from tools like pip. You might need to set up some configurations, which can be done as follows:\n# List all configurations poetry config --list # By default, Poetry creates virtual environments in a dedicated folder, not within the project directory # Use the following command to create the virtual environment in the project directory poetry config virtualenvs.in-project true # Poetry will automatically create a virtual environment when one doesn\u0026#39;t exist in the project # To be sure, you can run this command poetry config virtualenvs.create true # Other settings can usually be left as default Initialize a Project # If you need to create a new project from scratch, you\u0026rsquo;ll want to create a project folder, navigate to it in your terminal, and then run:\npoetry init An interactive CLI will guide you through creating a new project, which will generate two key files for Poetry: pyproject.toml for managing dependencies, and poetry.lock to lock package versions, similar to pip\u0026rsquo;s requirements.txt. Those familiar with Node.js should find this concept quite familiar 😝.\nIf you\u0026rsquo;re working with someone else\u0026rsquo;s project, it will usually include those two files.\nIf you want a simple directory structure, you can use:\npoetry new my-package This will create a basic project structure like this:\nmy-package ├── pyproject.toml ├── README.md ├── my_package │ └── __init__.py └── tests └── __init__.py For more complex needs, refer to the new Command documentation.\nCreate a Virtual Environment # There are a few situations to consider here:\nIf you\u0026rsquo;re building your project from scratch: poetry env use python # Default Python version you installed Poetry with poetry env use python3.7 # Use the Python 3.7 version on your system poetry env use 3.7 # A shortcut for the above poetry env use /full/path/to/python # Use this if the above shortcuts don\u0026#39;t work If you\u0026rsquo;re working with someone else\u0026rsquo;s project: There are two very similar commands. As shown in the comments, the official recommendation is to use sync, as it avoids installing any packages that aren\u0026rsquo;t tracked by poetry.lock, which may include unintended packages that the original developer added.\nHowever, I personally recommend to use install. The sync command may result into some strange error: it may remove itself and causes the command line break out.🤔\n# Automatically creates a virtual environment and installs all dependencies from pyproject.toml poetry install # Make sure virtualenvs.create is set to true # Automatically creates a virtual environment, installs dependencies from poetry.lock and remove redundant package in pyproject.toml poetry sync To activate the virtual environment, use:\npoetry shell Add Packages # Poetry lets you differentiate between packages needed in the production environment and those only required for development:\n# Add a package to the production environment poetry add your-package # Add a package to the development environment poetry add your-package -D Update Packages # poetry update # Automatically analyzes dependencies and updates all packages if necessary poetry update your-package1 your-package2 # Only updates the specific packages you list Remove Packages # This is one of Poetry\u0026rsquo;s standout features: While pip installs the specified package and its third-party dependencies, it can’t remove these dependencies when you uninstall a package. Poetry, on the other hand, safely and completely removes the dependencies tied to a package without affecting others.\n# Remove a package from the production environment poetry remove your-package # Remove a package from the development environment poetry remove your-package -D Show Dependencies # poetry show --tree # Show the dependency tree of the packages in pyproject.toml poetry show your-package --tree # Show the dependency tree for a specific package Program Packaging # We often need to share our Python programs. However, sharing the source code alone can be frustrating for users who aren\u0026rsquo;t familiar with coding, as it requires setting up a local environment to run the code. Therefore, program packaging comes in handy.\nPyInstaller # Installation is very simple, just like any other Python package: pip install pyinstaller. To use it, open the terminal in the target directory and run the command.\nFeatures: Fast packaging speed; relatively large packaged files\nCommon command codes:\n# Package the main.py file as a standalone executable; the command window is disabled when the executable runs pyinstaller -F -w main.py # Package the main.py file into a project folder; the command window is enabled when the executable runs pyinstaller -D main.py Parameter Description -h, --help Show help information and exit -v, --version Show program version information and exit -F, --onefile Package everything into a single standalone executable -D, --onedir Package everything into a directory (default option) -w, --windowed, --noconsole Disable the command window (only works on Windows) -c, --console, --nowindowed Use the command window to run the program (default option, only on Windows) -a, --ascii Exclude Unicode character set support (included by default) -d, --debug Generate a debug version of the executable -n NAME, --name=NAME Specify the name of the generated executable or directory (default is script name) -o DIR, --out=DIR Specify the directory where the spec file will be generated (default is the current directory) -p DIR, --path=DIR Set the Python module import path (similar to setting PYTHONPATH) -i \u0026lt;FILE\u0026gt;, --icon \u0026lt;FILE\u0026gt; Set the executable file icon (supports .ico or .icns formats) --distpath DIR Specify the output directory for the executable file (default is ./dist) --workpath WORKPATH Specify the temporary working file directory (default is ./build) --add-data \u0026lt;SRC;DEST or SRC:DEST\u0026gt; Add additional data files or directories to the executable (use semicolon for Windows, colon for Linux/OSX to separate source and destination paths) --add-binary \u0026lt;SRC;DEST or SRC:DEST\u0026gt; Add additional binary files to the executable --hidden-import MODULENAME Add modules not automatically detected --exclude-module EXCLUDES Exclude specified modules --clean Clean PyInstaller cache and temporary files --log-level LEVEL Set the verbosity of console messages during the build process (possible values: TRACE, DEBUG, INFO, WARN, ERROR, FATAL) Nuitka # Nuitka packages Python code into an executable (.exe). The underlying process first converts Python code to C code, then compiles the C code.\nFeatures: Slow packaging speed; requires an additional C compiler (though automatic setup is possible, it can be strict on memory management, so may not be suitable for users with limited space); smaller packaged files (tested to be about one-tenth the size of PyInstaller\u0026rsquo;s output)\nInstallation command:\npip install -U nuitka Common usage command:\n# Package the main.py file into an exe file with link-time optimization, and remove temporary files after packaging python -m nuitka --lto=yes --remove-output --onefile main.py Parameter Description --standalone Create a standalone executable folder that includes all dependencies. --onefile Package everything into a single .exe file. --optimize=N Set optimization level (0, 1, or 2), the higher the number, the more optimized the result. --lto Enable Link-Time Optimization (possible values: no, yes, thin). --enable-plugin=\u0026lt;plugin_name\u0026gt; Enable a specific plugin, such as tk-inter, numpy, anti-bloat, etc. --output-dir=\u0026lt;dir\u0026gt; Specify the output directory for the compiled files. --remove-output Delete intermediate .c files and other temporary files after compilation. --nofollow-imports Do not recursively process any imported modules. --include-package=\u0026lt;package_name\u0026gt; Explicitly include an entire package and its submodules. --include-module=\u0026lt;module_name\u0026gt; Explicitly include a specific module. --follow-import-to=\u0026lt;module/package\u0026gt; Specify which modules or packages to recursively process. --nofollow-import-to=\u0026lt;module/package\u0026gt; Specify which modules or packages not to recursively process. --include-data-files=\u0026lt;source\u0026gt;=\u0026lt;dest\u0026gt; Include specified data files. --include-data-dir=\u0026lt;directory\u0026gt; Include all data files in the specified directory. --noinclude-data-files=\u0026lt;pattern\u0026gt; Exclude data files matching a specified pattern. --windows-icon-from-ico=\u0026lt;path\u0026gt; Set the Windows executable icon. --company-name, --product-name, --file-version, --product-version, --file-description Set Windows executable properties. Reference # poetry related: poetry 入门完全指南_poetry使用-CSDN博客, a detailed and first-hand blog. Poetry, the official documents. ","date":"10 August 2024","externalUrl":null,"permalink":"/en/blog/pytips/","section":"Blogs","summary":"\u003ch2 class=\"relative group\"\u003eVirtual Env \n    \u003cdiv id=\"virtual-env\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#virtual-env\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h2\u003e\n\n\n\u003ch3 class=\"relative group\"\u003ePlain Python Method \n    \u003cdiv id=\"plain-python-method\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#plain-python-method\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h3\u003e\n\n\n\u003ch4 class=\"relative group\"\u003eCreat \n    \u003cdiv id=\"creat\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#creat\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h4\u003e\n\u003cp\u003eSome tipical code 👇\u003c/p\u003e","title":"Python Tpis","type":"blog"},{"content":" Reference # Honestly, I\u0026rsquo;m not familiar with BayesianOPT, the opinions mentioned stem from the below. 👇\n【机器学习】一文看懂贝叶斯优化/Bayesian Optimization\n一文详解贝叶斯优化（Bayesian Optimization）原理\n贝叶斯优化(BayesianOptimization)\n超参数优\u0026mdash;贝叶斯优化及其改进（PBT优化）\n贝叶斯优化 (Bayesian Optimization)\nMATLAB Offical Document\nAdvantages \u0026amp; Algorithm Principle # Here we are going to talk about the advantages \u0026amp; algorithm principle of BayesianOPT. If you only want to konw how to use it, you can read the #Advantage section, then go to the MATLAB Practice\nAdvantages # Algorithm Principle # MATLAB Practice # Well, we can put Bayesian Optimization into practice even though we don\u0026rsquo;t understand it, using the predefined function of MATLAB, the bayesopt. Here is the official guidance of the function: bayesopt\nFinal code display # % define the obj function function y = objectiveFcn(x) y = (1 - x.x1)^2 + 100 * (x.x2 - x.x1^2)^2; end % define the variables vars = [optimizableVariable(\u0026#39;x1\u0026#39;, [-2, 2]) optimizableVariable(\u0026#39;x2\u0026#39;, [-2, 2])]; % conduce the optimizer results = bayesopt(@objectiveFcn, vars, ... \u0026#39;AcquisitionFunctionName\u0026#39;, \u0026#39;expected-improvement-plus\u0026#39;, ... \u0026#39;MaxObjectiveEvaluations\u0026#39;, 30, ... \u0026#39;IsObjectiveDeterministic\u0026#39;, true, ... \u0026#39;Verbose\u0026#39;, 1); % get result bestPoint = results.XAtMinObjective; bestObjective = results.MinObjective; % result output fprintf(\u0026#39;最优解 x1: %.4f, x2: %.4f\\n\u0026#39;, bestPoint.x1, bestPoint.x2); fprintf(\u0026#39;最优目标值: %.4f\\n\u0026#39;, bestObjective); I\u0026rsquo;d commit that the code is generated by AI. 🥲 AI is a better coder, at least when comparing with me. 🫠 Parameters Explaination # Params Meaning AcquisitionFunctionName select a Acquisition Function, which determines the method how bayesopt choose the next acquisition point MaxObjectiveEvaluations the maximize evalu turns IsObjectiveDeterministic If the obj function contains noise, set to true ; Otherwise, set to false Verbose Determine the detailing extend of console output, the complete output includes many figures. Want more detailed information? Refer to the Offical document: bayesopt. It\u0026rsquo;s more completed and with amount of examples.\nIt\u0026rsquo;s basic for every MathModeler to read the offical document. 😝 ","date":"5 August 2024","externalUrl":null,"permalink":"/en/blog/bayesianopt/","section":"Blogs","summary":"\u003ch2 class=\"relative group\"\u003eReference \n    \u003cdiv id=\"reference\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#reference\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h2\u003e\n\u003cp\u003eHonestly, I\u0026rsquo;m not familiar with BayesianOPT, the opinions mentioned stem from the below. 👇\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca href=\"https://blog.csdn.net/qq_27590277/article/details/115451660\" target=\"_blank\"\u003e【机器学习】一文看懂贝叶斯优化/Bayesian Optimization\u003c/a\u003e\u003c/p\u003e","title":"Bayesian Optimization","type":"blog"},{"content":" Background Overview # You should know how to interact with the computer via the command line, including but not limited to: how to open the command line/terminal in Windows, when a running command ends, etc.\nA little knowledge of bypassing internet censorship is helpful. OverLeaf is foreign software, and its related LaTeX projects are also hosted abroad. Therefore, directly accessing foreign traffic when downloading dependencies can save a lot of trouble. If you don’t have a VPN, you will need to specify a domestic source for each package manager, though sometimes the updates from domestic sources are not timely.\nBasic familiarity with Vim operations is useful, such as: how to enter insert mode, how to save and exit, how to exit without saving, etc.\nFull Deployment Process # Installing Linux # Search for a Linux distribution in the Windows App Store and download it. The author chose Kali. After installation, you can open it directly from the Start menu. Upon opening, a command-line window will pop up, and you will need to register with a username and password.\nAt this point, your command line should display a warning. This is because you haven’t installed WSL (Windows Subsystem for Linux). Also, when entering the password, your input will not be displayed in the command line, but it has been recorded. Why do you need a Linux system? Because OverLeaf\u0026rsquo;s ShareLaTeX model requires a Linux environment. It is said that OverLeaf runs more smoothly on Linux systems.\nInstalling WSL # To install WSL2, run the following in the Windows command line:\nwsl --install After installation, you can open it directly. Another warning will appear. At this point, you need to create a text file in the C:\\Users\\ASUS directory and rename it to .wslconfig.\nEnter the following content:\n[experimental] autoMemoryReclaim=gradual # gradual | dropcache | disabled networkingMode=mirrored dnsTunneling=true firewall=true autoProxy=true Installing Docker # Go to the Docker website to download Docker, which will be the container for the ShareLaTeX model. Docker is an open-source application container engine that includes images, containers, and repositories. Its purpose is to manage the lifecycle of application components, such as encapsulation, distribution, deployment, and operation, allowing users to \u0026ldquo;package once, run anywhere,\u0026rdquo; much like a container, developed and encapsulated by programmers, which users can directly move around.\nOnce Docker is installed, you can double-click to start it in the background. We will interact with Docker later via the command line.\nPulling the Image # Open Kali, and run the following command:\ngit clone https://github.com/overleaf/toolkit.git ./overleaf-toolkit Then run:\ncd ./overleaf-toolkit bin/init vim ./config/variables.env At this point, you should be in the document interface of the Vim text editor. Vim has many shortcuts, and pressing the \u0026quot;I\u0026quot; key will enter insert mode for text editing. Press \u0026quot;esc\u0026quot; to return to normal mode. In insert mode, type: OVERLEAF_SITE_LANGUAGE=zh-CN.\nAfter typing, press \u0026quot;esc\u0026quot; to return to normal mode, then type :wq to \u0026ldquo;save and quit.\u0026rdquo; If you make a mistake, type :e! to discard all changes and start over. This step will set your OverLeaf interface to Chinese.\nAfter successfully saving and quitting, return to the familiar Kali command-line interface and run bin/up. This will pull the ShareLaTeX image and related network tools. There will be a large amount of data transfer, so ensure that your network is stable (your VPN should be reliable!).\nConfiguring the User # Once the previous command finishes, run bin/start. At this point, open Docker and enter the ShareLaTeX container. You should see code \u0026ldquo;flashing.\u0026rdquo; If there are no red messages, everything is running normally.\nNow open a browser and visit http://localhost/launchpad.\nAfter registering an Administrator Account, you will be redirected to http://localhost/project. The basic OverLeaf webpage should now be displayed.\nIf you compile now, it will most likely report an error ᕕ( ᐛ )ᕗ. This is because ShareLaTeX is missing many required packages🙃 Installing Extension Packages # Open Kali, navigate to the appropriate directory, and run bin/shell. Then execute the following one by one:\ncd /usr/local/texlive # Download and run the upgrade script wget http://mirror.ctan.org/systems/texlive/tlnet/update-tlmgr-latest.sh sh update-tlmgr-latest.sh -- --upgrade # Change the TeX Live download source tlmgr option repository https://mirrors.sustech.edu.cn/CTAN/systems/texlive/tlnet/ # Upgrade tlmgr tlmgr update --self --all # Install the full TeX Live package (this will take time, so don’t let the shell disconnect) tlmgr install scheme-full # Exit the ShareLaTeX command-line interface exit # Restart the ShareLaTeX container docker restart sharelatex After restarting, enter the shell again and run:\napt update # Install fonts apt install --no-install-recommends ttf-mscorefonts-installer fonts-noto texlive-fonts-recommended tex-gyre fonts-wqy-microhei fonts-wqy-zenhei fonts-noto-cjk fonts-noto-cjk-extra fonts-noto-color-emoji fonts-noto-extra fonts-noto-ui-core fonts-noto-ui-extra fonts-noto-unhinted fonts-texgyre # Install pygments apt install python3-pygments # Install Beamer and others apt install texlive-latex-recommended apt install texlive-latex-extra # Install English fonts echo \u0026#34;yes\u0026#34; | apt install -y --reinstall ttf-mscorefonts-installer # Install Chinese fonts apt install -y latex-cjk-all texlive-lang-chinese texlive-lang-english cp fonts/* /usr/share/fonts/zh-cn/ cd /usr/share/fonts fc-cache -fv # Update font cache fc-list :lang=zh-cn fc-match Arial Finally, in the shell directory, run:\nvim /usr/local/texlive/2023/texmf.cnf Open the configuration file and add shell_escape = t at the bottom.\nI’m not sure what this does, but it was passed down by the predecessors 🤔 Note, if the TeX Live version (the official name for extension packages) differs, the directory path may also change. You will need to adjust the path based on the actual version, for example, change 2023 to 2024.\nYou can use ls -l in the Linux command line to view all files in the current directory. Successful Deployment # Now you can happily use your local OverLeaf version without worrying about compilation timeouts~\nIf you\u0026rsquo;re lucky and happen to be a CQUer, here’s a graduation thesis template from Chongqing University, super user-friendly: CQUThesis\n","date":"12 July 2024","externalUrl":null,"permalink":"/en/blog/localoverleaf/","section":"Blogs","summary":"\u003ch2 class=\"relative group\"\u003eBackground Overview \n    \u003cdiv id=\"background-overview\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#background-overview\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eYou should know how to interact with the computer via the command line, including but not limited to: how to open the command line/terminal in Windows, when a running command ends, etc.\u003c/p\u003e","title":"Local OverLeaf Deployment","type":"blog"},{"content":"","date":"2 May 2025","externalUrl":null,"permalink":"/en/blog/","section":"Blogs","summary":"","title":"Blogs","type":"blog"},{"content":"Here are notes and micro-thesises to reinforce knowledge through writing.\nHope my casual words can provide you something useful.🤗\n","date":"2 May 2025","externalUrl":null,"permalink":"/en/","section":"Welcome to Morethan's website","summary":"\u003cp\u003eHere are notes and micro-thesises to reinforce knowledge through writing.\u003c/p\u003e\n\u003cp\u003eHope my casual words can provide you something useful.🤗\u003c/p\u003e","title":"Welcome to Morethan's website","type":"page"},{"content":"","date":"20 March 2025","externalUrl":null,"permalink":"/en/authors/","section":"Authors List","summary":"","title":"Authors List","type":"authors"},{"content":" Hi, welcome to my info page. 👋 # Basic Info # My casual English name is Morethan because it resembles my Chinese name. 🙃\nNow I\u0026rsquo;m a university student in China. ᕕ( ᐛ )ᕗ Nothing more to say. 🫠\nBlog Focus # Personal Knowledge Base: to store frequently-used operations and valuable experience.\nMicro Paper Stack: to store inspirations for my Graduation Thesis, usually serious and logical, attempt to follow the standard thesis working stream.\nKnowledge Outlet: to put what I leant into practice.\nFinal # If you find the content is useful, click a like please at the beginning of that page. 🤗\nIf you want to share the content, cite this website please. 🫡\nIf you find some bug, push an issue on the GitHub please. 🥰\n","date":"20 March 2025","externalUrl":null,"permalink":"/en/authors/morethan/","section":"Authors List","summary":"\u003ch1 class=\"relative group\"\u003eHi, welcome to my info page. 👋 \n    \u003cdiv id=\"hi-welcome-to-my-info-page-\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#hi-welcome-to-my-info-page-\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h1\u003e\n\n\n\u003ch2 class=\"relative group\"\u003eBasic Info \n    \u003cdiv id=\"basic-info\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#basic-info\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h2\u003e\n\u003cp\u003eMy casual English name is Morethan because it resembles my Chinese name. 🙃\u003c/p\u003e","title":"Morethan","type":"authors"},{"content":"","date":"20 March 2025","externalUrl":null,"permalink":"/en/tags/notes/","section":"Tags","summary":"","title":"Notes","type":"tags"},{"content":"","date":"20 March 2025","externalUrl":null,"permalink":"/en/tags/paper/","section":"Tags","summary":"","title":"Paper","type":"tags"},{"content":"","date":"20 March 2025","externalUrl":null,"permalink":"/en/series/paper-notes/","section":"Seires","summary":"","title":"Paper Notes","type":"series"},{"content":"","date":"20 March 2025","externalUrl":null,"permalink":"/en/tags/rstar/","section":"Tags","summary":"","title":"RStar","type":"tags"},{"content":" ","date":"20 March 2025","externalUrl":null,"permalink":"/en/series/","section":"Seires","summary":"\u003chr\u003e","title":"Seires","type":"series"},{"content":" ","date":"20 March 2025","externalUrl":null,"permalink":"/en/tags/","section":"Tags","summary":"\u003chr\u003e","title":"Tags","type":"tags"},{"content":"","date":"2025-03-20","externalUrl":null,"permalink":"/tags/%E7%AC%94%E8%AE%B0/","section":"标签","summary":"","title":"笔记","type":"tags"},{"content":"","date":"2025-03-20","externalUrl":null,"permalink":"/tags/%E8%AE%BA%E6%96%87/","section":"标签","summary":"","title":"论文","type":"tags"},{"content":"","date":"2025-03-20","externalUrl":null,"permalink":"/series/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/","section":"系列","summary":"","title":"论文笔记","type":"series"},{"content":"","date":"6 March 2025","externalUrl":null,"permalink":"/en/series/ai-project/","section":"Seires","summary":"","title":"AI-Project","type":"series"},{"content":"","date":"6 March 2025","externalUrl":null,"permalink":"/en/tags/cloud-service/","section":"Tags","summary":"","title":"Cloud-Service","type":"tags"},{"content":"","date":"2025-03-06","externalUrl":null,"permalink":"/tags/%E4%BA%91%E6%9C%8D%E5%8A%A1/","section":"标签","summary":"","title":"云服务","type":"tags"},{"content":"","date":"5 March 2025","externalUrl":null,"permalink":"/en/tags/qdrant/","section":"Tags","summary":"","title":"Qdrant","type":"tags"},{"content":"","date":"5 March 2025","externalUrl":null,"permalink":"/en/series/technical-miscellany/","section":"Seires","summary":"","title":"Technical Miscellany","type":"series"},{"content":"","date":"2025-03-05","externalUrl":null,"permalink":"/series/%E6%8A%80%E6%9C%AF%E6%9D%82%E9%A1%B9/","section":"系列","summary":"","title":"技术杂项","type":"series"},{"content":"","date":"4 March 2025","externalUrl":null,"permalink":"/en/tags/full-stack/","section":"Tags","summary":"","title":"Full-Stack","type":"tags"},{"content":"","date":"4 March 2025","externalUrl":null,"permalink":"/en/tags/llm/","section":"Tags","summary":"","title":"LLM","type":"tags"},{"content":"","date":"2025-03-04","externalUrl":null,"permalink":"/tags/%E5%85%A8%E6%A0%88%E5%BC%80%E5%8F%91/","section":"标签","summary":"","title":"全栈开发","type":"tags"},{"content":"","date":"5 February 2025","externalUrl":null,"permalink":"/en/tags/neo4j/","section":"Tags","summary":"","title":"Neo4j","type":"tags"},{"content":"","date":"27 January 2025","externalUrl":null,"permalink":"/en/tags/management/","section":"Tags","summary":"","title":"Management","type":"tags"},{"content":"","date":"27 January 2025","externalUrl":null,"permalink":"/en/series/project-reports/","section":"Seires","summary":"","title":"Project Reports","type":"series"},{"content":"","date":"27 January 2025","externalUrl":null,"permalink":"/en/tags/report/","section":"Tags","summary":"","title":"Report","type":"tags"},{"content":"","date":"27 January 2025","externalUrl":null,"permalink":"/en/tags/schedule/","section":"Tags","summary":"","title":"Schedule","type":"tags"},{"content":"","date":"2025-01-27","externalUrl":null,"permalink":"/tags/%E6%8A%A5%E5%91%8A/","section":"标签","summary":"","title":"报告","type":"tags"},{"content":"","date":"2025-01-27","externalUrl":null,"permalink":"/tags/%E6%97%A5%E7%A8%8B%E7%AE%A1%E7%90%86/","section":"标签","summary":"","title":"日程管理","type":"tags"},{"content":"","date":"2025-01-27","externalUrl":null,"permalink":"/series/%E9%A1%B9%E7%9B%AE%E6%8A%A5%E5%91%8A/","section":"系列","summary":"","title":"项目报告","type":"series"},{"content":"The CUMCM (Chinese Undergraduate Mathematical Contest in Modeling) is a competition where participants do not directly enter the national finals.\nInstead, they must go through a selection process involving the University-level competition (校赛), Provincial-level competition (省赛), and only after that will their papers be submitted for evaluation by national experts. Thus, the competition is informally divided into three stages: University-level competition, Provincial-level competition, and National-level competition.\n","date":"16 January 2025","externalUrl":null,"permalink":"/en/tags/cumcm/","section":"Tags","summary":"\u003cp\u003eThe \u003cstrong\u003eCUMCM\u003c/strong\u003e (Chinese Undergraduate Mathematical Contest in Modeling) is a competition where participants do not directly enter the national finals.\u003c/p\u003e\n\u003cp\u003eInstead, they must go through a selection process involving the \u003cstrong\u003eUniversity-level competition\u003c/strong\u003e (校赛), \u003cstrong\u003eProvincial-level competition\u003c/strong\u003e (省赛), and only after that will their papers be submitted for evaluation by national experts. Thus, the competition is informally divided into three stages: \u003cstrong\u003eUniversity-level competition\u003c/strong\u003e, \u003cstrong\u003eProvincial-level competition\u003c/strong\u003e, and \u003cstrong\u003eNational-level competition\u003c/strong\u003e.\u003c/p\u003e","title":"CUMCM","type":"tags"},{"content":"","date":"16 January 2025","externalUrl":null,"permalink":"/en/series/mathmodel/","section":"Seires","summary":"","title":"MathModel","type":"series"},{"content":"","date":"16 January 2025","externalUrl":null,"permalink":"/en/tags/matlab/","section":"Tags","summary":"","title":"MATLAB","type":"tags"},{"content":"","date":"15 January 2025","externalUrl":null,"permalink":"/en/tags/mysql/","section":"Tags","summary":"","title":"MySQL","type":"tags"},{"content":"","date":"10 January 2025","externalUrl":null,"permalink":"/en/tags/blog/","section":"Tags","summary":"","title":"Blog","type":"tags"},{"content":"","date":"10 January 2025","externalUrl":null,"permalink":"/en/tags/hugo/","section":"Tags","summary":"","title":"Hugo","type":"tags"},{"content":"","date":"10 January 2025","externalUrl":null,"permalink":"/en/series/project-report/","section":"Seires","summary":"","title":"Project Report","type":"series"},{"content":"","date":"2025-01-10","externalUrl":null,"permalink":"/tags/%E5%8D%9A%E5%AE%A2/","section":"标签","summary":"","title":"博客","type":"tags"},{"content":"","date":"6 January 2025","externalUrl":null,"permalink":"/en/series/casual-essay/","section":"Seires","summary":"","title":"Casual Essay","type":"series"},{"content":"","date":"6 January 2025","externalUrl":null,"permalink":"/en/tags/experience/","section":"Tags","summary":"","title":"Experience","type":"tags"},{"content":"","date":"2025-01-06","externalUrl":null,"permalink":"/tags/%E7%BB%8F%E5%8E%86/","section":"标签","summary":"","title":"经历","type":"tags"},{"content":"","date":"2025-01-06","externalUrl":null,"permalink":"/series/%E9%9A%8F%E7%AC%94/","section":"系列","summary":"","title":"随笔","type":"series"},{"content":"","date":"3 January 2025","externalUrl":null,"permalink":"/en/tags/ai/","section":"Tags","summary":"","title":"AI","type":"tags"},{"content":"","date":"2025-01-03","externalUrl":null,"permalink":"/series/ai%E9%81%90%E6%83%B3/","section":"系列","summary":"","title":"AI遐想","type":"series"},{"content":"","date":"3 January 2025","externalUrl":null,"permalink":"/en/tags/imagination/","section":"Tags","summary":"","title":"Imagination","type":"tags"},{"content":"","date":"3 January 2025","externalUrl":null,"permalink":"/en/series/wild-imagination-of-ai/","section":"Seires","summary":"","title":"Wild Imagination of AI","type":"series"},{"content":"","date":"2025-01-03","externalUrl":null,"permalink":"/tags/%E9%81%90%E6%83%B3/","section":"标签","summary":"","title":"遐想","type":"tags"},{"content":"","date":"12 September 2024","externalUrl":null,"permalink":"/en/tags/math/","section":"Tags","summary":"","title":"Math","type":"tags"},{"content":"","date":"10 August 2024","externalUrl":null,"permalink":"/en/tags/python/","section":"Tags","summary":"","title":"Python","type":"tags"},{"content":"","date":"12 July 2024","externalUrl":null,"permalink":"/en/tags/latex/","section":"Tags","summary":"","title":"LaTeX","type":"tags"},{"content":"","date":"12 July 2024","externalUrl":null,"permalink":"/en/tags/overleaf/","section":"Tags","summary":"","title":"Overleaf","type":"tags"},{"content":"","externalUrl":null,"permalink":"/en/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"Mathematical Modeling is the use of mathematical models to precisely and systematically describe objects in life, and it is an important combination of mathematics and practice.\n","externalUrl":null,"permalink":"/en/series/ai%E5%B7%A5%E7%A8%8B/","section":"Seires","summary":"\u003cp\u003e\u003ccode\u003eMathematical Modeling\u003c/code\u003e is the use of mathematical models to \u003cstrong\u003eprecisely and systematically\u003c/strong\u003e describe objects in life, and it is an important combination of mathematics and practice.\u003c/p\u003e\n\u003chr\u003e","title":"Mathematical Modeling","type":"series"},{"content":"Mathematical Modeling is the use of mathematical models to precisely and systematically describe objects in life, and it is an important combination of mathematics and practice.\n","externalUrl":null,"permalink":"/en/series/%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1/","section":"Seires","summary":"\u003cp\u003e\u003ccode\u003eMathematical Modeling\u003c/code\u003e is the use of mathematical models to \u003cstrong\u003eprecisely and systematically\u003c/strong\u003e describe objects in life, and it is an important combination of mathematics and practice.\u003c/p\u003e\n\u003chr\u003e","title":"Mathematical Modeling","type":"series"}]