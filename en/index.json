


[{"content":" ä¸€ä¸ªæ–°å‹çš„äººè„‘å¯å‘å¼æ¶æ„ï¼Œåœ¨ç°æœ‰æ—¶åºæ— å…³çš„ç¥ç»å…ƒæ¨¡å‹çš„åŸºç¡€ä¸Šå¼•å…¥äº†\u0026quot;æ—¶åº\u0026quot;æ¦‚å¿µï¼Œæ¶Œç°å‡ºä¸€ç³»åˆ—ç±»ä¼¼äººè„‘çš„ç°è±¡ å‰è¨€ # å…³æ³¨åˆ°è¿™ç¯‡è®ºæ–‡å…¶å®å°±æ˜¯å› ä¸ºå…¶å®£ç§°çš„é‚£ç§\u0026quot;å¥‡å¦™\u0026quot;çš„æ¶Œç°ç°è±¡ï¼Œå°†æ—¶åºä¿¡æ¯å¼•å…¥ç¥ç»å…ƒæ¨¡å‹ä¹‹åäº§ç”Ÿäº†ç±»ä¼¼äººç±»æ€ç»´æ¨¡å¼çš„æ³¨æ„åŠ›ç°è±¡ã€‚è¿™äº›ç°è±¡å¹¶éäººä¸ºè®¾è®¡çš„ï¼Œè€Œæ˜¯åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è‡ªç„¶äº§ç”Ÿçš„ï¼Œè¿™æ˜¯æœ€ä»¤äººæŒ¯å¥‹çš„ã€‚\næœ€ç›´è§‚çš„ä¾‹å­å°±æ˜¯å®˜æ–¹ç»™å‡ºçš„\u0026quot;è¿·å®«æ¨¡å‹\u0026quot;ï¼Œä»å¯è§†åŒ–çš„ç»“æœå¯ä»¥ç›´è§‚åœ°çœ‹åˆ°ï¼Œæ¨¡å‹çš„æ³¨æ„åŠ›ç„¦ç‚¹åœ¨è¿·å®«ä¸­æ¸¸èµ°ï¼Œå°±åƒäººç±»æ±‚è§£è¿·å®«é—®é¢˜ä¸€æ ·ã€‚å¦å¤–å®˜æ–¹è¿˜ç»™å‡ºäº†ä¸€ä¸ªå¯äº¤äº’çš„å±•ç¤ºç½‘é¡µï¼šDemoï¼Œåœ¨è¿™ä¸ªç½‘é¡µä¸­ä½ å¯ä»¥ç›´æ¥æŒ‡æŒ¥æ¨¡å‹æ±‚è§£è¿·å®«ï¼Œå¹¶å®æ—¶æŸ¥çœ‹æ¨¡å‹çš„æ³¨æ„åŠ›ç„¦ç‚¹ã€‚\nå¦å¤–ï¼Œå®˜æ–¹çš„ä»£ç ä»“åº“ä¸­é…ç½®äº†è¯¦å°½çš„è¯´æ˜æ–‡æ¡£å’Œä»£ç æ³¨é‡Šï¼Œä»£ç æ–‡ä»¶çš„åˆ‡åˆ†ä¹Ÿéå¸¸ç®€æ´ç›´è§‚ï¼Œæœ‰ä¸€ç§ç¨‹åºå‘˜æ‰æ‡‚çš„\u0026quot;è‰ºæœ¯æ„Ÿ\u0026quot;ã€‚\nåœ¨é˜…è¯»äº†å¤§é‡çš„å±å±±ä»£ç ä¹‹åï¼Œå¯èƒ½æ‰ä¼šå¯¹ä»£ç çš„\u0026quot;ä¼˜é›…\u0026quot;ç¨‹åº¦æœ‰æ‰€ä½“ä¼šğŸ˜‡å¹¶æ·±åˆ»ç†è§£è¿™ä»½\u0026quot;ä¼˜é›…\u0026quot;èƒŒåçš„å·¥ä½œé‡ å¼•å…¥ # ç¥ç»ç½‘ç»œæœ€åˆå—ç”Ÿç‰©å¤§è„‘å¯å‘ï¼Œå´ä¸ç”Ÿç‰©å¤§è„‘å·®å¼‚å·¨å¤§ã€‚ç”Ÿç‰©å¤§è„‘å±•ç°å‡ºéšæ—¶é—´æ¼”å˜çš„å¤æ‚ç¥ç»åŠ¨åŠ›å­¦è¿‡ç¨‹ï¼Œè€Œç°ä»£ç¥ç»ç½‘ç»œä¸ºäº†ä¾¿äºå¤§è§„æ¨¡æ·±åº¦å­¦ä¹ ï¼Œå¯ä»¥æ‘’å¼ƒäº†\u0026quot;æ—¶åº\u0026quot;ç‰¹å¾ã€‚\nå…³äºä¸ºä»€ä¹ˆè¦å¼€å±•è¿™é¡¹ç ”ç©¶ï¼Œå®˜æ–¹è®ºæ–‡ä¸­å·²ç»æœ‰äº†éå¸¸æ˜ç¡®çš„è¡¨è¿°ï¼Œæ— éœ€å¤šè¨€ï¼š\n\u0026ldquo;ä¸ºä½•å¼€å±•æ­¤é¡¹ç ”ç©¶ï¼Ÿè¯šç„¶ï¼Œç°ä»£äººå·¥æ™ºèƒ½åœ¨è¯¸å¤šå®è·µé¢†åŸŸå±•ç°å‡ºçš„å“è¶Šæ€§èƒ½ä¼¼ä¹è¡¨æ˜ï¼Œå¯¹ç¥ç»åŠ¨åŠ›å­¦çš„æ¨¡æ‹Ÿå®æ— å¿…è¦ï¼ŒæŠ‘æˆ–æ˜¾å¼è€ƒé‡æ™ºèƒ½çš„æ—¶é—´ç»´åº¦å®å±åå®ç”¨ä¹‹ä¸¾ã€‚ç„¶è€Œï¼Œäººç±»æ™ºèƒ½å…·æœ‰é«˜åº¦çµæ´»æ€§ã€æ•°æ®é«˜æ•ˆæ€§ä»¥åŠä¼˜å¼‚çš„æœªè§æƒ…å¢ƒå¤–æ¨èƒ½åŠ›ï¼Œä¸”å­˜åœ¨äºå­¦ä¹ ä¸é€‚åº”çš†ä¸æ—¶é—´ä¹‹ç®­ç´§å¯†å…³è”çš„å¼€æ”¾ä¸–ç•Œä¸­ã€‚å› æ­¤ï¼Œäººç±»æ™ºèƒ½å¤©ç„¶å…·å¤‡å¸¸è¯†è¿ç”¨ã€æœ¬ä½“è®ºæ¨ç†èƒ½åŠ›ã€é€æ˜æ€§/å¯è§£é‡Šæ€§ä»¥åŠå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›â€”â€”è¿™äº›ç‰¹è´¨åœ¨ç°æœ‰äººå·¥æ™ºèƒ½ä¸­å°šæœªå¾—åˆ°ä»¤äººä¿¡æœçš„ä½“ç°ã€‚\u0026rdquo;\nè¿™ç¯‡æ–‡ç« çš„æ ¸å¿ƒæŠ€æœ¯è´¡çŒ®å¦‚ä¸‹æ‰€ç¤ºï¼š\nè§£è€¦çš„å†…éƒ¨æ—¶é—´ç»´åº¦ ç¥ç»å…ƒå±‚é¢çš„æ¨¡å‹(NLMs) ç¥ç»æ´»åŠ¨åŒæ­¥ å½“ç„¶ï¼Œåœ¨æ²¡æœ‰è¯¦ç»†çœ‹ä»£ç å®ç°çš„æƒ…å†µä¸‹ï¼Œå…‰çœ‹è¿™äº›åè¯æ˜¯æ²¡æœ‰æ„ä¹‰çš„ã€‚ä½†æ˜¯ä½œè€…çš„æ€è·¯å’Œè§‚ç‚¹è¿˜æ˜¯å¯ä»¥äº†è§£ï¼š\næ¨ç†æ¨¡å‹å’Œå¾ªç¯ï¼šä½œè€…é”è¯„äº†å½“å‰æ¨ç†æ¨¡å‹ï¼ŒæŒ‡å‡º\u0026quot;ç»§ç»­æ‰©å±•å½“å‰æ¨¡å‹æ¶æ„\u0026quot;è¿™æ¡æŠ€æœ¯è·¯çº¿å·²ç»è¢«å¾ˆå¤šç ”ç©¶è´¨ç–‘ã€‚è€Œä½¿ç”¨å¾ªç¯æŠ€æœ¯ä¹Ÿçš„ç¡®èƒ½å¤Ÿè®©ç°æœ‰æ¨¡å‹æ¶æ„äº§ç”Ÿæ›´åŠ è‰¯å¥½çš„è¡¨ç°ï¼Œä½†æ˜¯ä½œè€…è®¤ä¸ºï¼Œå¾ªç¯æœºåˆ¶å¾ˆé‡è¦ï¼Œä½†æ˜¯è¢«å¾ªç¯æœºåˆ¶è§£é”çš„ç¥ç»å…ƒæ´»åŠ¨ä¹‹é—´çš„ç²¾ç¡®æ—¶åºä¸äº¤äº’åŒæ ·é‡è¦ æœ‰è¶£çš„å‰¯ä½œç”¨ï¼šCTM å†…éƒ¨çš„å¾ªç¯ç±»ä¼¼äºäººç±»çš„æ€è€ƒï¼Œåœ¨æ²¡æœ‰ä»»ä½•æ˜¾å¼çš„ç›‘ç£å‡½æ•°å¼•å¯¼çš„æƒ…å†µä¸‹å°±èƒ½å¤Ÿä¸ºä¸åŒéš¾åº¦çš„ä»»åŠ¡è‡ªåŠ¨åˆ†é…åˆé€‚çš„è®¡ç®—èµ„æºï¼Œç®€å•çš„ä»»åŠ¡å°†ä¼šæå‰ç»ˆæ­¢è®¡ç®—ï¼Œè€Œå¤æ‚çš„ä»»åŠ¡å°†ä¼šè¿›è¡Œæ›´åŠ æ·±å…¥çš„è®¡ç®— ä¿¡æ¯å¯ä»¥è¢«ç¼–ç åœ¨æ—¶åºåŠ¨æ€ä¸­ï¼Œè¿™å°†èµ‹äºˆç½‘ç»œæ›´åŠ å¼ºå¤§çš„ä¿¡æ¯å‹ç¼©èƒ½åŠ› è¯´æ˜ä¸€ä¸‹æˆ‘å¯¹ç¬¬ä¸‰ç‚¹çš„æ€è€ƒï¼ŒæŒ‰ç…§\u0026quot;å‹ç¼©å³æ™ºèƒ½\u0026quot;çš„è§‚ç‚¹ï¼Œå°†å°†ä¿¡æ¯ç¼–ç åˆ°æ—¶åºåŠ¨æ€ä¸­å°†æå¤§åœ°æé«˜ç½‘ç»œçš„ä¿¡æ¯\u0026quot;å‹ç¼©ç‡\u0026quot;ï¼Œä»æŸç§ç¨‹åº¦ä¸Šæ¥è¯´ä¹Ÿå°±æå‡äº†\u0026quot;æ™ºèƒ½\u0026quot; æœ€åä½œè€…ä¹Ÿæ˜ç¡®äº†è‡ªå·±è¿™ä¸ªç ”ç©¶çš„ç›®çš„æ‰€åœ¨ï¼š\n\u0026ldquo;é€šè¿‡CTMæ˜¾å¼å»ºæ¨¡ç¥ç»æ—¶åºæœºåˆ¶ï¼Œæˆ‘ä»¬æ—¨åœ¨ä¸ºå¼€å‘æ›´å…·ç”Ÿç‰©åˆç†æ€§ä¸”é«˜æ€§èƒ½çš„äººå·¥æ™ºèƒ½ç³»ç»Ÿå¼€è¾Ÿæ–°è·¯å¾„ã€‚\u0026rdquo;\næ–¹æ³• # CTMæ¶æ„æ¦‚è¿°ï¼šâ‘ çªè§¦æ¨¡å‹ï¼ˆæƒé‡ä»¥è“çº¿è¡¨ç¤ºï¼‰é€šè¿‡æ¨¡æ‹Ÿç¥ç»å…ƒé—´ç›¸äº’ä½œç”¨ç”Ÿæˆé¢„æ¿€æ´»å€¼ã€‚æ¯ä¸ªç¥ç»å…ƒä¼šä¿ç•™ â‘¡é¢„æ¿€æ´»å†å²è®°å½•ï¼Œå…¶ä¸­æœ€æ–°æ•°æ®è¢« â‘¢ç¥ç»å…ƒçº§æ¨¡å‹ï¼ˆæƒé‡ä»¥çº¢çº¿è¡¨ç¤ºï¼‰ç”¨äºäº§ç”Ÿ â‘£åæ¿€æ´»å€¼ã€‚ç³»ç»ŸåŒæ—¶ç»´æŠ¤ â‘¤åæ¿€æ´»å†å²è®°å½•ï¼Œå¹¶æ®æ­¤è®¡ç®—â‘¥åŒæ­¥çŸ©é˜µã€‚åŸºäºè¯¥çŸ©é˜µ â‘¦ç­›é€‰ç¥ç»å…ƒå¯¹ï¼Œç”±æ­¤äº§ç”Ÿçš„ â‘§æ½œè¡¨å¾è¢«CTMç”¨äº â‘¨ç”Ÿæˆè¾“å‡ºå¹¶é€šè¿‡äº¤å‰æ³¨æ„åŠ›æœºåˆ¶è°ƒèŠ‚æ•°æ®ã€‚ç»è°ƒèŠ‚çš„æ•°æ®ï¼ˆå¦‚æ³¨æ„åŠ›è¾“å‡ºï¼‰ä¼š â‘©ä¸åæ¿€æ´»å€¼æ‹¼æ¥ï¼Œä½œä¸ºä¸‹ä¸€å†…éƒ¨æ—¶é’Ÿæ­¥çš„è¾“å…¥\nå¦‚æœåªæ˜¯å•çº¯çš„å¼•å…¥æ—¶é—´ç»´åº¦çš„è¯ï¼Œä¼ ç»Ÿçš„ RNN æ¶æ„ä¹Ÿèƒ½å¤Ÿå®ç°ï¼Œä½† CTM çš„åˆ›æ–°ä¹‹å¤„åœ¨äºï¼š\nä½¿ç”¨ç¥ç»å…ƒçº§æ¨¡å‹å–ä»£äº†ä¼ ç»Ÿçš„æ¿€æ´»å‡½æ•° ä½¿ç”¨ç¥ç»åŒæ­¥ä½œä¸ºæ½œåœ¨è¡¨å¾æ¥è°ƒåˆ¶æ•°æ®å¹¶ç”Ÿæˆè¾“å‡º è¿ç»­æ€è€ƒï¼šå†…éƒ¨åºåˆ—ç»´åº¦ # è®ºæ–‡å®šä¹‰äº†ä¸€ä¸ªå†…éƒ¨çš„æ—¶é—´ç»´åº¦ï¼š $$ t\\in \\{1,\\dots,T\\} $$\nå›¾ä¸­å‚æ•°å³ä¸Šè§’çš„è§’æ ‡å°±æŒ‡ä»£çš„æ˜¯æŸä¸€ä¸ªç‰¹å®šçš„æ—¶é—´æ­¥ï¼Œè€Œæ¯ä¸€ä¸ªæ—¶é—´æ­¥ä¸­éƒ½ä¼šè¿›è¡Œä¸€æ¬¡å›¾ä¸­å®Œæ•´çš„è®¡ç®—æµç¨‹(ä»â‘ åˆ°â‘©)\nè¿™ç§å†…éƒ¨ç»´åº¦å¹¶éå…¨æ–°çš„æ¦‚å¿µï¼ŒRNN å’Œ Transformer ä¸­éƒ½æœ‰ä½¿ç”¨ã€‚ä½†æ˜¯ä¼ ç»Ÿçš„æ¶æ„æŒ‰ç…§æ•°æ®è¾“å…¥çš„é¡ºåºæ¥é€æ­¥å¤„ç†ï¼Œè¿™éšå«åœ°å°†å†…éƒ¨çš„æ—¶é—´ç»´åº¦ä¸æ•°æ®çš„è¾“å…¥é¡ºåºæŒ‚é’©äº†ã€‚\nä¸¾ä¸ªä¾‹å­ï¼ŒRNN ä¸­ä¹Ÿæœ‰å†…éƒ¨æ—¶é—´ç»´åº¦çš„æ¦‚å¿µï¼Œåªæ˜¯æ¯ä¸€ä¸ªæ—¶é—´æ­¥æ¨¡å‹éƒ½ä¼šæ¥å—ä¸€ä¸ªæ–°çš„ token è¾“å…¥ï¼Œç„¶åæ ¹æ®å†…éƒ¨éšçŠ¶æ€å»äº§ç”Ÿä¸‹ä¸€ä¸ª token å¹¶æ›´æ–°å†…éƒ¨éšçŠ¶æ€ã€‚å¦‚æœåªçœ‹ token çš„è¾“å…¥å°±ä¼šå‘ç°æˆ‘ä»¬å…¶å®æ˜¯é»˜è®¤å°†æ•°æ®å†…åœ¨çš„é¡ºåºä½œä¸ºæ¨¡å‹çš„å†…éƒ¨æ—¶é—´ç»´åº¦\nTransformer ä¸­å…¶å®å¹¶æ²¡æœ‰éå¸¸æ˜æ˜¾çš„é¡ºåºå¤„ç†ï¼šæ¯ä¸€ä¸ªæ—¶é—´æ­¥æ³¨æ„åŠ›æœºåˆ¶éƒ½ä¼šå¹¶è¡Œå¤„ç†åŸæ¥æ‰€æœ‰çš„ tokenğŸ¤”æˆ–è®¸è¿™ä¹Ÿæ˜¯å…¶å¼ºå¤§æ‰€åœ¨ï¼Ÿ è€Œ CTM åˆ™å®Œå…¨è§£è€¦äº†è¿™ç§å…³è”ï¼Œè®©å†…éƒ¨çš„å¤„ç†ä¸è¾“å…¥æ•°æ®æ— å…³ã€‚ä¸åªæ˜¯é¡ºåºæ— å…³ï¼Œè¿˜ä¸è¾“å…¥åºåˆ—çš„é•¿åº¦æ— å…³ã€‚è¿™ç§\u0026quot;è§£è€¦\u0026quot;ä½¿å¾—æ¨¡å‹å†…éƒ¨çš„\u0026quot;æ€è€ƒ\u0026quot;å¯ä»¥æœ‰ä»»æ„é•¿åº¦ï¼Œå¯ä»¥è¿­ä»£åœ°æ„å»ºå’Œå®Œå–„å†…éƒ¨è¡¨å¾ï¼Œå¹¶ä¸”å¯ä»¥æ‰©å±•åˆ°\u0026quot;éåºåˆ—åŒ–\u0026quot;çš„ä»»åŠ¡ä¸Šï¼Œä¾‹å¦‚æ±‚è§£è¿·å®«é—®é¢˜ã€‚\nå¾ªç¯æƒé‡ï¼šçªè§¦æ¨¡å‹ # è¿™ä¸€éƒ¨åˆ†ä¸»è¦è®²è§£çªè§¦æ¨¡å‹ï¼Œå¯¹åº”å›¾ä¸­çš„ç•ªå·â‘ éƒ¨åˆ†ã€‚å½¢å¼åŒ–æ¥è®²çªè§¦æ¨¡å‹è¿›è¡Œçš„å·¥ä½œå¦‚ä¸‹ï¼š $$ a^t=f_{\\theta_{syn}}(concat(z^t,o^t))\\in R^D $$ å…¶ä¸­ \\(z^t\\) è¡¨ç¤º \\(t\\) æ—¶é—´æ­¥çš„è¾“å…¥åºåˆ—ï¼Œ\\(o^t\\) æ˜¯ä¸Šä¸€ä¸ªæ—¶é—´æ­¥è®¡ç®—å¾—å‡ºçš„è°ƒåˆ¶æ•°æ®ï¼Œä¸åŸæ¥çš„ \\(z^t\\) è¿›è¡Œæ‹¼æ¥åæ•´ä½“é€å…¥çªè§¦æ¨¡å‹è¿›è¡Œè®¡ç®—ï¼Œç„¶åå°±å¾—å‡º \\(a^t\\) ä¹Ÿå°±æ˜¯é¢„æ¿€æ´»å€¼\nçªè§¦æ¨¡å‹æœ¬è´¨ä¸Šå°±æ˜¯ä¸€ä¸ªå‡½æ•° \\(f_{\\theta_{syn}}\\) å¯ä»¥ç”¨å¤šç§æ–¹å¼è¡¨è¾¾ï¼Œç»è¿‡å®éªŒï¼ŒåŸè®ºæ–‡ä¸­ä½¿ç”¨äº† MLP æ¥è¡¨ç¤ºè¿™ä¸ªå‡½æ•°ï¼Œæ›´å…·ä½“çš„å°±æ˜¯ U-NET-esque MLP\nç„¶åå°†æœ€è¿‘çš„ \\(M\\) ä¸ªé¢„æ¿€æ´»å€¼ç”¨ä¸€ä¸ªçŸ©é˜µ \\(A^t\\) å­˜å‚¨èµ·æ¥ï¼š\n$$ A^t=[a^{t-M+1}\\quad a^{t-M+1}\\dots a^{t}]\\in R^{D\\times M} $$ å†å²åºåˆ—ä¸­çš„å‰ \\(M\\) ä¸ªå…ƒç´ åŠåˆå§‹æ—¶åˆ» \\(t=1\\) çš„ \\(z\\) å€¼éœ€è¦è¿›è¡Œåˆå§‹åŒ–ï¼Œå®éªŒè¡¨æ˜å°†å…¶è®¾ç½®ä¸ºå¯å­¦ä¹ å‚æ•°èƒ½è·å¾—æœ€ä½³æ•ˆæœã€‚\nå‚æ•°ç§æœ‰åŒ–çš„ç¥ç»å…ƒçº§æ¨¡å‹ # ","date":"2025-05-14","externalUrl":null,"permalink":"/blog/ctm-note/","section":"Blogs","summary":"\u003cp\u003e\n\n\u003cdiv class=\"lead text-neutral-500 dark:text-neutral-400 !mb-9 text-xl\"\u003e\n  ä¸€ä¸ªæ–°å‹çš„äººè„‘å¯å‘å¼æ¶æ„ï¼Œåœ¨ç°æœ‰æ—¶åºæ— å…³çš„ç¥ç»å…ƒæ¨¡å‹çš„åŸºç¡€ä¸Šå¼•å…¥äº†\u0026quot;æ—¶åº\u0026quot;æ¦‚å¿µï¼Œæ¶Œç°å‡ºä¸€ç³»åˆ—ç±»ä¼¼äººè„‘çš„ç°è±¡\n\u003c/div\u003e\n\u003c/p\u003e","title":"CTMè¯»æ–‡ç¬”è®°","type":"blog"},{"content":" A summary of thoughts on the memory of large language models Introduction # This reflection originates from a common issue encountered in AI-assisted programming: the need to repeatedly familiarize itself with the code repository from scratch.\nCurrent AI-assisted programming tools provide a wealth of foundational information to help AI quickly understand a code project. They use various functions to read project code, directory structures, and edit file contents. During each interaction, the AI model goes through a \u0026ldquo;from-scratch\u0026rdquo; familiarization process with the project: when you instruct it to solve a specific problem, the AI explores your code repository step by step, understands the relevant details, and makes the corresponding changes. This seems reasonable enough.\nHowever, when you start a new round of conversation, the AI completely forgets what changes it made previously and also forgets the details of your code repository that are worth \u0026ldquo;remembering.\u0026rdquo; It then begins a new round of exploration and familiarization with the code repository.\nThe obvious issues are as follows:\nWaste of computational resources: Repeatedly exploring the repository without utilizing the useful information obtained from the historical exploration process. Difficulty in handling complex projects: The need to start from scratch each time leads to an incomplete understanding of the repository, resulting in incoherent code. But this issue is not unique to AI-assisted programming. The \u0026ldquo;seven-second memory\u0026rdquo; characteristic of large language models (LLMs) limits their application in multiple commercial fields:\nCode development: As mentioned, it cannot form a coherent understanding of a project. Intelligent education: It struggles to provide coherent guidance to students. Emotional companionship: It fails to form a coherent user profile. In summary, the problem boils down to one thing: Current large language models cannot achieve a coherent, accurate, and dynamically adjustable understanding of complex matters.\nIn contrast, human performance is quite different: A company employee may not initially understand the core logic and important details of a project, but over time, they become more and more familiar with the project code and can even apply their unique understanding to code development; a human teacher can gradually explore a student\u0026rsquo;s characteristics based on their own experience and tailor their teaching accordingly; your good friend will not forget the travel experiences you shared yesterday, nor will they forget your personality traits.\nThis is also an important reason why current large language model technologies are difficult to implement: Although large language models have consumed all the text information on the internet and are even expanding into multimodal information such as images and videos, there is no effective \u0026ldquo;experience accumulation\u0026rdquo; pathway for specific engineering projects that cannot be fully conveyed through text (such as project concepts and technology stack selection). No matter how powerful the large language model is, it cannot effectively handle these tasks.\nCurrent Status Analysis # First, we provide a general analysis of the current focus of large language model evaluations, highlighting that current evaluations emphasize isolated tasks while neglecting coherent tasks. We then analyze the typical interaction patterns, demonstrating that current interaction modes lack effective and coherent experience accumulation.\nLarge Language Model Evaluation # There are a multitude of benchmark tests for large language models. Since Qwen 3 recently released its benchmark scores, we will use the benchmark scores from Qwen 3 as an example here.\nArenaHard # Official GitHub link: ArenaHard; Online demo: Arena-Hard-Auto Benchmark Viewer\nArenaHard is a benchmark test based on model versus model scoring. The test set is derived from 200,000 real user queries collected from the Chatbot Arena, from which 500 high-quality prompts are selected as the test set.\nEach test in the test set is sent to the model under test to obtain the corresponding response. The responses of the baseline model (GPT-4-0314) have been pre-generated by the official team. Another large model is then used as a referee to evaluate the responses of these two models. Finally, the model\u0026rsquo;s win-loss record is collected and further processed to calculate a score out of 100. This score represents \u0026ldquo;the probability that the referee model expects you to defeat GPT-4-0314 on 500 high-difficulty instructions.\u0026rdquo;\nThe benchmark test has released version 2.0, which is significantly more challenging than before. As of May 3, 2025, the latest benchmark results for the high-difficulty prompts and style control tests are as follows:\nModel Scores (%) CI (%)\r0 o3-2025-04-16 85.9 (-0.8 / +0.9)\r1 o4-mini-2025-04-16-high 79.1 (-1.4 / +1.2)\r2 gemini-2.5 79.0 (-2.1 / +1.8)\r3 o4-mini-2025-04-16 74.6 (-1.8 / +1.6)\r4 gemini-2.5-flash 68.6 (-1.6 / +1.6)\r5 o3-mini-2025-01-31-high 66.1 (-1.5 / +2.1)\r6 o1-2024-12-17-high 61.0 (-2.0 / +2.1)\r7 claude-3-7-sonnet-20250219-thinking-16k 59.8 (-2.0 / +1.8)\r8 Qwen3-235B-A22B 58.4 (-1.9 / +2.1)\r9 deepseek-r1 58.0 (-2.2 / +2.0)\r10 o1-2024-12-17 55.9 (-2.2 / +1.8)\r11 gpt-4.5-preview 50.0 (-1.9 / +2.0)\r12 o3-mini-2025-01-31 50.0 (-0.0 / +0.0)\r13 gpt-4.1 50.0 (-1.9 / +1.7)\r14 gpt-4.1-mini 46.9 (-2.4 / +2.1)\r15 Qwen3-32B 44.5 (-2.2 / +2.1)\r16 QwQ-32B 43.5 (-2.5 / +2.1)\r17 Qwen3-30B-A3B 33.9 (-1.6 / +1.5)\r18 claude-3-5-sonnet-20241022 33.0 (-2.3 / +1.8)\r19 s1.1-32B 22.3 (-1.7 / +1.5)\r20 llama4-maverick-instruct-basic 17.2 (-1.5 / +1.2)\r21 Athene-V2-Chat 16.4 (-1.4 / +1.4)\r22 gemma-3-27b-it 15.0 (-1.4 / +1.0)\r23 Qwen3-4B 15.0 (-1.1 / +1.5)\r24 gpt-4.1-nano 13.7 (-1.1 / +1.0)\r25 Llama-3.1-Nemotron-70B-Instruct-HF 10.3 (-0.8 / +1.0)\r26 Qwen2.5-72B-Instruct 10.1 (-0.9 / +1.3)\r27 OpenThinker2-32B 3.2 (-0.3 / +0.3) From the above introduction, it is clear that ArenaHard only tests the model\u0026rsquo;s ability to solve isolated problems.\nAIME # AIME, the American Invitational Mathematics Examination, is not strictly a benchmark test for large language models, but it does reflect a model\u0026rsquo;s mathematical reasoning ability.\nThe competition consists of 15 questions, all of which have numerical answers, with no points for steps. The final score is simply the accuracy rate, which may be averaged or take the highest value over multiple tests, but there is no unified standard.\nClearly, AIME also only tests the large language model\u0026rsquo;s ability to solve isolated problems in one go.\nTypical Interaction Patterns # Do large language models have memory? Yes, but only short-term memory. The current interaction between humans and large language models can be summarized as follows:\nHumans provide background information for the task. The large language model attempts to complete the task based on the background information. The model continuously interacts with humans during task execution (short-term memory). The model completes the task. The so-called \u0026ldquo;short-term\u0026rdquo; memory is reflected in its \u0026ldquo;short\u0026rdquo; duration: no longer than the length of the large language model\u0026rsquo;s context window. The specific implementation method is to simply put all the interaction history between the user and the model into the window.\nAlthough the context window of existing large language models is getting longer and longer, and can even reach 2 million tokens (equivalent to 1.5 copies of Dream of the Red Chamber), this does not mean that we can put all interaction information into the window and expect the model to generate content at a low cost and with high accuracy.ğŸ˜¢ Summary # The current focus of evaluation and interaction patterns are very suitable for isolated tasks. However, in reality, the tasks we need to complete are all \u0026ldquo;subtasks\u0026rdquo; under a larger goal framework, and these \u0026ldquo;subtasks\u0026rdquo; are interrelated and mutually restrictive.\nAn interesting phenomenon is that after we try to solve one of the subtasks, solving the other tasks becomes easier. The reason is that some of the exploration results obtained in the process of solving other subtasks are beneficial to solving other subtasks because these subtasks all belong to the same goal.\nTherefore, the direction for improvement is now clear: introduce an online learning long-term memory module for large language models, enabling them to accumulate experience from historical exploration.\nRelated Work # Titans # The Titans architecture is Google\u0026rsquo;s latest improvement on the Transformer architecture, proposing the concept of \u0026ldquo;Test-time Memory.\u0026rdquo; It aims to expand the model\u0026rsquo;s context length while providing a more accurate attention mechanism. It introduces a parameter network that can be dynamically updated at test time to store long-term memory information and enhance the performance of the attention mechanism.\nKBLaM # KBLaM is an official implementation of \u0026ldquo;KBLaM: Knowledge Base augmented Language Model.\u0026rdquo;\nTTRL # The TTRL algorithm can update the model\u0026rsquo;s weight parameters using reinforcement learning at test time, achieving real-time dynamic updates of model parameters.\nrStar # rStar uses Monte Carlo Tree Search (MCTS) to explore high-quality reasoning paths. Unlike model distillation methods that require a strong model as guidance, this framework borrows the idea of peer-to-peer verification learning, using a voting mechanism to obtain a simulated \u0026ldquo;correct answer\u0026rdquo; to guide the generation of high-quality paths.\nTokenformer # The Tokenformer framework embraces the philosophy of \u0026ldquo;everything can be tokenized.\u0026rdquo;\nSD-LoRA # SD-LoRA\nDifferential Transformer # The Differential Transformer improves the attention mechanism in traditional Transformer architectures by introducing the concept of \u0026ldquo;differential attention.\u0026rdquo; Unlike conventional softmax attention, the Diff Transformer divides the query and key vectors into two separate groups, computes two independent softmax attention maps, and then eliminates noise through a subtraction operation. This mechanism is analogous to the differential amplifier in electronic engineering, which cancels out common-mode noise by taking the difference between two signals.\nIdeas and Thoughts # Memory in Large Language Models # When we run a large language model offline and ask it a question: \u0026ldquo;Who proposed the theory of relativity?\u0026rdquo; It can tell us: \u0026ldquo;The theory of relativity was proposed by Einstein.\u0026rdquo; This actually shows that the model\u0026rsquo;s parameters already store relevant factual information, which is the so-called \u0026ldquo;memory.\u0026rdquo;\nThis section mainly answers: Where is the memory of Transformer stored?\nDeconstructing Transformer # Research on the interpretability of Transformer has been ongoing. This article interprets the structure of Transformer based on DeepMind\u0026rsquo;s related research and Anthropic\u0026rsquo;s related research.\nIn summary: The residual connections throughout the process are regarded as a \u0026ldquo;data bus,\u0026rdquo; running through the entire model\u0026rsquo;s computation process; the Attention Layer and MLP Layer are seen as the read/write heads for information, reading and writing data on the data bus.\nEmbedding # The embedding process is the first step in the Transformer processing pipeline:\nTokenization: Tokenize the text. Embedding: Convert tokens into vectors and add positional encoding. Intuitively, this step turns a sentence into a cluster of vectors. The embedding matrix is used to convert tokens into reasonable numerical vectors. This means that the embedding matrix stores some factual information, including relationships between different entities and differences between tokens. However, this information is based on the average of a large amount of text and cannot represent dynamic semantics.\nFor example, the word \u0026ldquo;bank\u0026rdquo; can mean both \u0026ldquo;riverbank\u0026rdquo; and \u0026ldquo;financial institution,\u0026rdquo; but the embedding matrix describes a kind of \u0026ldquo;average\u0026rdquo; of these two meanings.\nMulti-head Attention # As mentioned above, the embedding matrix alone cannot describe dynamic semantics based on context. Therefore, the multi-head attention mechanism was introduced to construct more precise word vectors.\nA vivid description is: After embedding, a cluster of vectors is generated, and these vectors influence each other. The parameters in the multi-head attention layer model this complex mutual influence. According to the circuit interpretation framework, the Attention Layer calculates the \u0026ldquo;increment of mutual influence\u0026rdquo; between this cluster of vectors and writes this increment back into the \u0026ldquo;data bus.\u0026rdquo;\nMLP # Research by DeepMind has shown that factual information is mostly stored in the MLP layer. Here, the operations in the MLP layer are broken down into two steps:\nFact matching Calculating the facts to be injected The vectors processed by the multi-head attention have been shifted to the correct positions in the current context, and these vectors already contain a large amount of complex information superimposed together (a large number of explicit semantics are additively fused into a single vector). The role of the embedding matrix here is similar to a checklist: each row is a check item (the specific meaning of these items is also highly superimposed), and the check items and word vectors are matched through dot products to obtain \u0026ldquo;matching scores,\u0026rdquo; which are then filtered out by the nonlinear truncation function (ReLU) to remove unrelated matches. This step is the \u0026ldquo;fact matching\u0026rdquo; process.\nFor example, when using an extended matrix to detect the token \u0026ldquo;relativity,\u0026rdquo; a certain row might simultaneously check: \u0026ldquo;whether it is an academic concept + whether it is a ball sport + whether it is the name of a galaxy.\u0026rdquo; The detection results could be relevant or irrelevant, but they will all be linearly combined in the end.\nWhy can they be combined? Because of the distributive property of vector dot products: \u0026ldquo;combine first then match\u0026rdquo; is equivalent to \u0026ldquo;match first then combine.\u0026rdquo; The projection matrix then acts as the \u0026ldquo;calculation of fact increment\u0026rdquo;: based on the matching scores, it calculates which factual information needs to be added to the original word vectors, adds all the necessary factual information together to form a fact increment vector.\nThe MLP layer then writes the \u0026ldquo;fact increment vector\u0026rdquo; back into the residual flow.\nStacking # Transformer stacks the above structure repeatedly, and the information extracted by deeper layers becomes more complex and harder to understand.\nIt should be noted that the injection of factual information does not occur in a single isolated MLP layer, nor does it happen solely within one Transformer Block. Instead, it iterates continuously throughout the entire network, gradually producing a clearer and more pronounced impact.\nThis complex mechanism of factual information injection poses significant challenges to interpretability efforts and makes it difficult to precisely control the flow of information within large models. However, this process precisely highlights the advantage of storing factual information in neural networks: for a given piece of complex information, it initially associates with a broad and ambiguous set of related information. Then, through computations in each layer of the network, it progressively refines and eventually converges to the most highly relevant information.\nThis is also one of the reasons why Attention Layers and MLP Layers alternate: after factual information is injected in the MLP Layer, the Attention Layer is immediately used to trim itâ€”weakening weaker associations and amplifying stronger onesâ€”preventing a flood of redundant information from propagating to the next MLP Layer, which could trigger even more chaotic associations. Time Encoding # The Transformer inherently lacks awareness of the order between tokensâ€”the actual representation of positional sequence relies on \u0026ldquo;positional embeddings.\u0026rdquo; To more efficiently embed positional information into tokens, various methods have emerged:\nSinusoidal Embedding: Introduced in the original Transformer paper, this is an absolute positional encoding that directly adds positional encoding (PE) information to the original token embeddings. Rotary Position Embedding (RoPE): This method modifies the Q and K matrices by splitting each dimension into pairs of even dimensions and applying a rotation in the complex plane. Learnable Positional Embedding: Similar to sinusoidal embedding, this is also an absolute positional encoding, but it is trained directly via backpropagation. Similarly, to adapt to streaming data input, we should also encode the time at which information is acquired: data from the internet a decade ago should differ from current information. In this process, large models can implicitly learn the \u0026ldquo;sequence of information\u0026rdquo; just as they learn \u0026ldquo;token order,\u0026rdquo; allowing them to grasp the concept of \u0026ldquo;time\u0026rdquo; in a latent manner.\nAn effective time encoding should have the following characteristics:\nThe greater the time span, the larger the difference should be. It should preserve the periodicity of time. Fact Injection # The above text has described how factual information participates in token generation as model parameters. But how is this information stored in the parameters? Simply put, it is by using backpropagation to adjust the parameters. However, if we continue to ask:\nWhat kind of training samples are needed to store a particular factual piece of information? How many training samples are required? How does the stored information \u0026ldquo;accommodate\u0026rdquo; new information? Is the process of storing factual information inseparable from the entire network training process? If we knew the answers to the above questions, we could build a framework that continuously absorbs factual information: simply keep inputting training samples in a specified format and execute backpropagation in real time.\nExisting Approaches # This section mainly answers: What are the current methods to achieve \u0026ldquo;long-term memory\u0026rdquo;? What are their respective advantages and disadvantages?\nRAG # RAG was originally designed to alleviate the hallucination problem in large language models, but its essence is to use an external information processing module (vector database) to directly incorporate useful natural language information into the prompt.\nFollowing this line of thinking, not only can it alleviate the hallucination problem in large language models, but it can also enable the model to perform specific tasks directly (prompt engineering). Many current AI applications are implemented in this way:\nCursor: Automatically adds background information about the code repository and a variety of tools, then relies on the model\u0026rsquo;s own capabilities to complete complex code writing tasks. Manus: Provides more complex information processing tools, using the large language model\u0026rsquo;s ultra-long context to record all collected information to complete tasks. MCP: Although it is just a tool calling protocol, its underlying layer is still prompt engineering. There have been many promising results in using RAG to achieve long-term memory:\nmem0: A large language model memory system based on a vector database, allowing the model to remember user preferences from conversations. graphiti: A memory system based on a vectorized graph database, building a real-time knowledge graph for large language models, capable of storing a large amount of past factual information and accurately extracting relevant memories. memobase: A memory system based on file storage combined with a vector database, writing user preferences and other information into user profile files and continuously updating them. The advantages of this approach are very clear:\nSimple and convenient: Humans are very familiar with natural language, so using natural language to drive program execution is relatively easy to understand. Controllable information: By explicitly programming to integrate information into a long but logically coherent prompt, it is easy to control complex information. Application of large language model technology: The large language model itself is a top-tier natural language processing technology, capable of participating in various aspects of background information organization and enhancing the ability to handle complex information. The disadvantages of using RAG are also present:\nContext window limitation: The attention layer of the large language model can only support a limited context, and too long a context will be automatically truncated. Attention dispersion: Filling too much information into the context window will degrade the performance of the model\u0026rsquo;s attention mechanism, making it difficult to capture truly important information. Limited expressive ability: Complex information is difficult to express in natural language, and even if it is forcibly expressed, key information will be lost, leading to understanding åå·® by the large language model. For example, the development philosophy of a complex software project, if not experienced in practice, will become empty words when described in natural language. Difficult memory maintenance: Even with the support of large language model technology, updating, maintaining, and extracting memories remains a tricky problem, and it is difficult to balance response speed, accuracy, and cost. Fine-Tuning # I need to look into papers on fine-tuning, especially those on \u0026ldquo;efficient fine-tuning\u0026rdquo;, to see if they can enhance online learning capabilities. ğŸ¤” If possible, I should also check out research on incremental learning.\nBut for now, I\u0026rsquo;ll have to put this on hold\u0026hellip; ğŸ˜¢ Gotta focus on reviewing for finals.\nNetwork Parameters # There is currently very little work in this area. Although OpenAI has recently launched a memory function, it has not revealed the specific implementation method, and there is not much discussion online. Here, we mainly summarize the implementation method of the Titans architecture.\nEarly Ideas # This section contains some very early ideas.\nConsider the question: Where does the capability of large language models come from? It comes from the vast amount of internet text.\nLarge language models are trained on a vast amount of text, using backpropagation to compress the mutual influence of text and factual information into the parameters of the neural network. While modeling language, they also complete the modeling of the abstract concepts represented by language. Of course, this process is very slow and requires a lot of computational power.\nAfter completing such a large amount of knowledge compression, how to handle new information? This is a problem that still needs to be solved, but it is not an unsolvable problem: Human knowledge is accumulated from little to much, and the brain must process and store continuously input new information and integrate new information with existing information.\nReferences # Arena-Hard: An Open-Source High-Quality Large Language Model Evaluation Benchmark Titans KBLaM TTRL rStar Tokenformer SD-LoRA Differential Transformer A Mathematical Framework for Transformer Circuits An Intuitive Explanation of the Attention Mechanism, the Core of Transformer Fact Finding: Attempting to Reverse-Engineer Factual Recall on the Neuron Level (Post 1) Fact Finding: Simplifying the Circuit (Post 2) An Intuitive Explanation of How Large Language Models Store Facts Diff Transformerï¼šè®©æ³¨æ„åŠ›æœºåˆ¶æ›´æ™ºèƒ½ï¼Œæ¶ˆé™¤å™ªéŸ³ï¼Œæå‡å¤§è¯­è¨€æ¨¡å‹æ€§èƒ½-çŸ¥ä¹ ","date":"4 May 2025","externalUrl":null,"permalink":"/en/blog/llm-memory/","section":"Blogs","summary":"\u003cdiv class=\"lead text-neutral-500 dark:text-neutral-400 !mb-9 text-xl\"\u003e\n  A summary of thoughts on the memory of large language models\n\u003c/div\u003e\n\n\n\n\u003ch2 class=\"relative group\"\u003eIntroduction \n    \u003cdiv id=\"introduction\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#introduction\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h2\u003e\n\u003cp\u003eThis reflection originates from a common issue encountered in AI-assisted programming: the need to repeatedly familiarize itself with the code repository from scratch.\u003c/p\u003e","title":"On the Memory of LLM","type":"blog"},{"content":" Summarizing and documenting the process of tinkering with Ubuntu for future reference. Introduction # Ubuntu, as a popular Linux distribution, offers better ecosystem support compared to other Linux distros. The most notable advantage is that when you encounter issues, you\u0026rsquo;re more likely to find tutorials and solutions for Ubuntu.\nThis article primarily focuses on the GUI-based personal edition of Ubuntu. For server-specific Ubuntu systems, operations depend on your actual business needs. The article Cloud Server Setup can serve as a reference.\nUbuntu Installation # The installation was done a long time ago, so no detailed records exist. If needed, please search for keywords like \u0026ldquo;installing Ubuntu on a portable hard drive.\u0026rdquo; Hereâ€™s a relatively recent guide: Installing Ubuntu on a Portable Hard Drive.\nThe installation process is no longer traceable. The current setup involves installing Ubuntu on a portable hard drive, allowing it to be used on-the-go.\nTo use it, simply plug in the hard drive before powering on the computer. Quickly press a specific key to enter the BIOS boot menu, set the priority to the highest, save, and exit to boot into Ubuntu.\nTo switch back to Windows, just unplug the hard drive and power on normallyâ€”no additional steps are required.\nInterface Customization # I personally prefer an Apple-style interface, so I specifically chose an Apple-inspired theme: WhiteSur.\nThe installation process is straightforward:\ngit clone https://github.com/vinceliuice/WhiteSur-gtk-theme.git --depth=1 cd WhiteSur-gtk-theme ./install.sh # Run the installation script For detailed configuration, refer to the instructions on the GitHub page.\nAs for updates, the official guide doesnâ€™t specify, presumably assuming users already know:\ngit pull # Fetch the latest code ./install.sh # Re-run the installation script Fcitx 5 # Main reference: Install and Configure Fcitx 5 Chinese Input Method on Ubuntu. I initially considered using Sogou Input Method, but the official installation process seemed overly complicated, and it also required installing Fcitx 5. So, I figured I might as well just use Fcitx 5 directly.\nTo be honest, I was reluctant to use Fcitx at first ğŸ¥² because its interface is so \u0026ldquo;plain\u0026rdquo; that itâ€™s hard to accept. I believe the author of the blog post above must have felt the same way ğŸ‘†.\nUsing Windows Fonts # Approach: Copy font files from Windows to Ubuntuâ€™s dedicated font directory, assign appropriate permissions, refresh Ubuntuâ€™s font cache, and load the new fonts.\n# Windows font directory: C:/Windows/Fonts sudo cp /mnt/C/Windows/Fonts/LXGWWenKai-Regular.ttf /usr/share/fonts/custom/LXGWWenKai-Regular.ttf # Grant permissions sudo chmod u+rwx /usr/share/fonts/custom/* # Navigate to the font directory cd /usr/share/fonts/custom/ # Create font cache sudo mkfontscale # Refresh cache sudo fc-cache -fv Alternatively, you can download a new .ttf file from the web and copy it to the target directory. If youâ€™re using a GUI-based Ubuntu system, you can simply double-click the font file to install it ğŸ¥°.\nFonts may be installed redundantly, as the system doesnâ€™t check for duplicates. If this happens, manually locate and delete the duplicate files in the relevant directory ğŸ¥². Ubuntuâ€™s font tools can display all font information. Mounting Hard Drives # Since my Ubuntu system is installed on a portable hard drive, the main goal here is to access Windows partitions from Ubuntu. This section doesnâ€™t cover detailed partition operations. For tasks like formatting partitions, refer to: How to Partition and Mount Disks in Ubuntu.\n# View disks and partitions (sudo privileges required) sudo fdisk -l # Create a mount point (essentially creating a folder) # The subfolder under /mnt is named \u0026#34;E\u0026#34; because itâ€™s intended to mount the E drive sudo mkdir /mnt/E # Mount the new partition directly sudo mount /dev/vdb /mnt/E # Set auto-mount at boot # Check the partitionâ€™s UUID sudo blkid # Edit the specific file vim /etc/fstab # Append to the end of the file UUID=xxxxxxxx /mnt/E ntfs defaults 0 2 Replace the UUID above with the output from blkid. Replace ntfs with the appropriate filesystem type (common types include ntfs and ext 4). defaults: This is a combination of default mount options, such as rw (read-write) and relatime (reduces inode access time updates). 0 and 2: These values control backup and filesystem check order. Typically, the first value is 0 (no backup), and the second is 1 or 2 (1 for the root filesystem, 2 for others). Testing method:\n# If no errors occur, the configuration is correct sudo mount -a Creating Shortcuts # A common task: placing a quick link to a frequently used folder on the desktop for easy access.\n# Place a link to /target/dir in the Desktop folder # Replace with your target directory ln -s /target/dir ~/Desktop # Testâ€”if you can cd into it, it works cd ~/Desktop/dir Configuring Git # One of the standout features of Linux is its extreme simplicity, which is why using the command line to manage Git is the preferred choice for Linux users ğŸ˜ƒ. Ubuntu comes with Git pre-installed, so there\u0026rsquo;s no need to install it separately. If you want to upgrade, follow these steps:\ngit --version # Check the Git version sudo add-apt-repository ppa:git-core/ppa # Add the official repository sudo apt update \u0026amp;\u0026amp; sudo apt upgrade # If possible, proceed with the upgrade Setting Up SSH for GitHub (Password-Free Configuration) # Of course, you can also use HTTPS directly, but the downside is that youâ€™ll need to enter your password every time. Moreover, with GitHub\u0026rsquo;s increasing security measures, the password isnâ€™t necessarily your account password but rather a dedicated token ğŸ¥².\nSuch a cumbersome process is unbearable on Linux. Iâ€™d rather go through a tedious setup once than have to enter a long token every time.\nThis section is mainly referenced from: Configuring Git to Push by Default Without Entering Credentials (Ubuntu, SSH).\ngit config --global user.name \u0026#39;xx\u0026#39; # Configure the global username git config --global user.email \u0026#39;xxx@qq.com\u0026#39; # Configure the global email account # Generate an SSH key pair. Here, I choose to press Enter all the way through. ssh-keygen -t rsa -b 4096 -C \u0026#34;your_email@example.com\u0026#34; # Start the SSH agent and load the private key into the agent eval \u0026#34;$(ssh-agent -s)\u0026#34; ssh-add ~/.ssh/id_rsa # View and copy the public key content cat ~/.ssh/id_rsa.pub # Add this new SSH key to your GitHub account. # Change an existing HTTPS-linked repository to an SSH link git remote rm origin git remote add origin git@github.com:username/repository.git Installing and Managing Software # Software installation on Ubuntu generally falls into the following categories:\nVia the built-in Snap store Via apt Via .deb packages Via curl Different installation methods require different management approaches. curl installations are the most cumbersome to manage, while others can be handled easily with their respective package managers.\nSnap # Simply open the Snap store to install software effortlessly, though the packages are often outdated.\napt # # Install software sudo apt install xxx # Update packages sudo apt update # Sync package info from remote repositories without upgrading apt list --upgradable # View upgradable packages # Upgrade all available packages without handling dependency changes sudo apt upgrade sudo apt full-upgrade # Full upgrade sudo do-release-upgrade # Upgrade across major Ubuntu versions # Check software packages # Search for all packages containing \u0026#34;wps\u0026#34; and their description information sudo apt-cache search wps # View package names containing the keyword \u0026#34;wps\u0026#34; sudo apt-cache pkgnames | grep -i wps # Remove packages sudo apt remove xxx sudo apt autoremove # Clean up residuals deb # After downloading a .deb package from a browser, double-clicking it will install it directly. Internally, this uses apt, so management is the same as with apt.\n# Install via double-click # Uninstall via apt sudo apt remove xxx sudo apt autoremove # Clean up residuals AppImage # AppImage is a portable software packaging format for Linux systems, designed to simplify application distribution and execution. Its core philosophy is \u0026ldquo;one app = one file,\u0026rdquo; allowing users to run applications directly without installation or administrator privileges.\nIn newer versions of Ubuntu, attempting to run the file directly may result in an error. To resolve this, you need to install libfuse2 using the following command:\nsudo apt install libfuse2 After installation, you can simply double-click the package to launch the application ğŸ˜ƒ.\nNever install fuse directly, as this will automatically uninstall fuse3, causing the file system in newer Ubuntu versions to crash! If you accidentally install it, remove fuse and check the apt operation log to manually reinstall any automatically removed packages. If you want to uninstall the software, itâ€™s very straightforward: just delete the package. However, if youâ€™re a perfectionist like me, you can check the following directories to completely clean up any residual files:\nls ~/.config -a # Check configuration files ls ~/.local/share -a # Check shared configuration files ls ~/.cache -a # Check cache # Check disk usage of folders under .cache du -sh ~/.cache/* | sort -h -r curl # Download and execute installation scripts directly from a URL using curl. Software installed this way is harder to manage because the actual installation process is script-driven and difficult to monitor.\n# Example: Installing the Zed editor curl -f https://zed.dev/install.sh | sh # Uninstalling is usually messy # First, fetch the installation script curl -f https://zed.dev/install.sh -o install.sh # Have an AI parse the script # Then follow the AIâ€™s instructions to manually uninstall Office Suite # As we all know, Microsoft Office cannot run directly on Linux ğŸ˜…. However, viewing and editing doc files is often unavoidable.\nTherefore, hereâ€™s a recommended Office alternative for Linux: LibreOffice. The installation steps are as follows:\nsudo add-apt-repository ppa:libreoffice/ppa sudo apt update sudo apt install libreoffice Before installing LibreOffice, I also tried using WPS to edit Office files, but for some reason, it kept causing system errors, so I eventually abandoned it.\nIf switching away from Office makes you feel lost, Wine might be your saviorâ€”itâ€™s the sorcery that runs Windows apps on Linux! Storage Cleanup # # Remove orphaned dependencies sudo apt autoremove # Clear apt cache sudo du -sh /var/cache/apt # Check apt cache size sudo apt autoclean # Auto-clean sudo apt clean # Full clean # Clear system logs journalctl --disk-usage # Check system log size sudo journalctl --vacuum-time=3 d # Remove logs older than 3 days # Clear .cache du -sh ~/.cache/* | sort -h -r # Check cache size rm -r folder_name # delete the folder recursively # Clean up old Snap versions snap list --all # List all Snap packages # List all disabled packages (single-line command) echo -e \u0026#34;\\033[1 mDisabled Snap Packages and Their Sizes:\\033[0 m\u0026#34; \u0026amp;\u0026amp; snap list --all | awk \u0026#39;/disabled|å·²ç¦ç”¨/{print $1}\u0026#39; | while read -r pkg; do size=$(snap info \u0026#34;$pkg\u0026#34; | awk \u0026#39;/installed:/ {print $4}\u0026#39;); printf \u0026#34;%-30 s %10 s\\n\u0026#34; \u0026#34;$pkg\u0026#34; \u0026#34;$size\u0026#34;; done | sort -k 2 -h # Remove all disabled Snap packages (single-line command) snap list --all | awk \u0026#39;/disabled|å·²ç¦ç”¨/{print $1, $3}\u0026#39; | while read snapname revision; do sudo snap remove \u0026#34;$snapname\u0026#34; --revision=\u0026#34;$revision\u0026#34;; done # Clean up old kernels sudo dpkg --list | grep linux-image # List all kernels sudo apt autoremove --purge # Automatically remove unnecessary kernels Miscellaneous # This section includes some simple yet commonly used commands.\nExtracting Files # The command varies depending on the file format you need to extract.\n# Extract a .zip file unzip file.zip -d /target/directory # Extract a .tar file tar -xvf file.tar # Extract a .tar.gz file tar -xzvf file.tar.gz References # How to Partition and Mount Disks in Ubuntu LibreOffice Suite åœ¨ Ubuntu å®‰è£…é…ç½® Fcitx 5 ä¸­æ–‡è¾“å…¥æ³• Configuring Git to Push by Default Without Entering Credentials (Ubuntu, SSH) ","date":"1 May 2025","externalUrl":null,"permalink":"/en/blog/ubuntu-note/","section":"Blogs","summary":"\u003cdiv class=\"lead text-neutral-500 dark:text-neutral-400 !mb-9 text-xl\"\u003e\n  Summarizing and documenting the process of tinkering with Ubuntu for future reference.\n\u003c/div\u003e\n\n\n\n\u003ch2 class=\"relative group\"\u003eIntroduction \n    \u003cdiv id=\"introduction\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#introduction\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h2\u003e\n\u003cp\u003eUbuntu, as a popular Linux distribution, offers better ecosystem support compared to other Linux distros. The most notable advantage is that when you encounter issues, you\u0026rsquo;re more likely to find tutorials and solutions for Ubuntu.\u003c/p\u003e","title":"Ubuntu Tinkering Notes","type":"blog"},{"content":" Keywords: Mutual Reasoning; Small Models; Reasoning Capability Enhancement Research Background # Existing Work # The following content is generated by AI to quickly explain what RAP is.\nThe research foundation is a framework called RAP (Reasoning via Planning), which aims to enhance the reasoning capabilities of models in complex tasks by using language models (LLMs) as both world models and reasoning agents, combined with the Monte Carlo Tree Search (MCTS) algorithm for strategic exploration.\nProblem Background:\nCurrent large language models (LLMs) have limitations in reasoning tasks, mainly due to the lack of mental representations of the environment (world models), making it difficult to predict the outcomes and long-term effects of actions. Additionally, LLMs lack a reward mechanism to evaluate reasoning paths and cannot balance exploration and exploitation, leading to inefficient reasoning. RAP Framework:\nLanguage Model as World Model: Defines states and actions through natural language, models the reasoning process as a Markov Decision Process (MDP), and uses LLMs to predict the outcomes of each action. Reward Design: Uses the log probability of actions, confidence levels, and the LLM\u0026rsquo;s own evaluation results as rewards to guide reasoning towards an ideal state. Monte Carlo Tree Search (MCTS): Iteratively constructs a search tree through MCTS, balancing exploration and exploitation, and ultimately selects high-reward reasoning paths. Shortcomings # LLMs struggle to effectively explore the solution space, often generating low-quality reasoning paths. LLMs have difficulty accurately assessing the quality of reasoning paths. These issues are more pronounced in small models. Method Overview # Human-like Reasoning # The reasoning paths are still generated using MCTS, but with more actions that simulate human thinking processes: decomposing and searching a reasoning step, proposing sub-problems, problem transformation, etc.\nPath Evaluation # The mutual consistency principle is used to determine the quality of paths, i.e., introducing another small model of comparable capability as a \u0026ldquo;peer\u0026rdquo; to evaluate the quality of reasoning paths. The specific process is as follows:\nThe framework provides the \u0026ldquo;peer\u0026rdquo; small model with some local reasoning paths as prompts, and then asks this small model to complete the reasoning path. If the path generated by MCTS matches the path completed by the \u0026ldquo;peer\u0026rdquo; small model, then this reasoning path is considered high-quality and can be a candidate path. Using small models of comparable capability avoids distilling large models; using \u0026ldquo;peer models\u0026rdquo; to evaluate paths rather than directly guiding path generation; judging \u0026ldquo;path consistency\u0026rdquo; mainly relies on the final result. Detailed Methodology # Symbol Explanation Table:\nSymbol Meaning \\(x\\) Target Problem \\(M\\) Target Small Model \\(T\\) Search Tree Generated by Small Model Using MCTS \\(s\\) Intermediate Reasoning Step \\(t\\) Candidate Path, a Complete Reasoning Path in \\(T\\) \\(ans\\) Final Reasoning Path for \\(M\\) to Solve \\(x\\) \\(Score\\) Reasoning Path Evaluation Function \\(a\\) An Action Sampled from the Action Space \\(s_{d}\\) Termination Reasoning Step, Contains the Answer \\(\\hat{M}\\) \u0026ldquo;Peer\u0026rdquo; Small Model \\(T_{validate}\\) \\(T\\) Pruned by Path Evaluation Function \\(Estimate\\) Path Evaluation Function Problem Formalization # Formalize the abstract natural language description of \u0026ldquo;small model solving reasoning problems\u0026rdquo;:\n$$ t=x\\oplus s_1 \\oplus s_2 \\oplus \u0026hellip;\\oplus s_d $$\n$$ T=\\left \\{ t^1, t^2, \u0026hellip;, t^n \\right \\} $$\n$$ T_{validate}=Estimate(T) $$\n$$ ans = max(Score(T_{validate})) $$\nHuman-like Reasoning # Action Space # One-step Thinking: Given the existing reasoning path, let the model generate the next reasoning step. Quick Thinking: Directly let the model complete all reasoning steps until the final result is produced. Sub-problem + Answer: Let the model propose a sub-problem and answer it. Sub-problem Re-answer: The answer generated in the previous step may be inaccurate, so provide an additional action option to re-answer the sub-problem. Problem Restatement: Let the model reorganize the conditions of the problem. Note that action 4 can only occur after action 3, and action 5 can only occur after the problem itself (root node). Reward Function # Inspired by AlphaGo, the evaluation (reward) of intermediate steps is set as their contribution to the correct result. The specific implementation is as follows:\nInitialize \\(Q(s_{i},a_{i})=0\\); randomly generate the next step until a termination node \\(s_{d}\\) is encountered. Use consistency voting to calculate \\(Q(s_{d},a_{d})\\), which is also the confidence score of the termination node. Backpropagation: \\(Q(s_{i},a_{i})=Q(s_{i},a_{i})+Q(s_{d},a_{d})\\). MCTS # Generally, the classic MCTS is used: selection, expansion, simulation (Rollout), and backpropagation; however, to obtain more accurate reward values, multiple simulation evaluations are performed.\nThe exploration-exploitation balance still uses the classic UCT formula:\n$$ UCT(s,a) = \\frac{Q(s,a)}{N(s,a)}+c\\sqrt{ \\frac{\\ln N_{parent}(s)}{N(s,a)} } $$\nWhere \\(N(s,a)\\) represents the number of times a node has been visited, \\(Q(s,a)\\) is the cumulative reward value, and \\(c\\) represents the balance rate.\nPath Evaluation # For a reasoning path \\(t=x\\oplus s_1 \\oplus s_2 \\oplus \u0026hellip;\\oplus s_d\\), randomly select a reasoning step \\(s_{i}\\). Inject the reasoning path before \\(s_{i}\\), \\(t_{1}=x\\oplus s_1 \\oplus s_2 \\oplus \u0026hellip;\\oplus s_i\\), as a prompt into the \u0026ldquo;peer\u0026rdquo; small model \\(\\hat{M}\\). \\(\\hat{M}\\) completes the path, generating a new path \\(t\u0026rsquo;=x\\oplus s_1 \\oplus s_2 \\oplus \u0026hellip;\\oplus s_i \\oplus s_{i+1}\u0026rsquo; \\oplus \\dots \\oplus s_{d}\u0026rsquo;\\). If the \u0026ldquo;path consistency\u0026rdquo; is achieved, i.e., the problem answers are consistent, then \\(t\u0026rsquo;\\) is considered a candidate path. The process of selecting candidate paths is the pruning process of \\(T\\), ultimately producing \\(T_{validate}\\). Final Selection # For each candidate path in the candidate path tree \\(t=x\\oplus s_1 \\oplus s_2 \\oplus \u0026hellip;\\oplus s_d\\):\n$$ Score(t)=\\prod_{i=1}^{d} Q(s_{i},a) $$\nFinally, select the reasoning path with the highest score:\n$$ ans = max(Score(T_{validate})) $$\nOther Details # MCTS performs 32 Rollouts. When processing the MATH dataset, the maximum depth of MCTS is 8; in other cases, the maximum depth is 5. The reasoning of the \u0026ldquo;peer\u0026rdquo; small model and the target small model can be executed in parallel to improve computational efficiency. During path evaluation, the truncation point of the path should be between 20% and 80% of the total path. ","date":"20 March 2025","externalUrl":null,"permalink":"/en/blog/rstar-note/","section":"Blogs","summary":"\u003cp\u003e\n\n\u003cdiv class=\"lead text-neutral-500 dark:text-neutral-400 !mb-9 text-xl\"\u003e\n  Keywords: Mutual Reasoning; Small Models; Reasoning Capability Enhancement\n\u003c/div\u003e\n\u003c/p\u003e\n\n\n\u003ch2 class=\"relative group\"\u003eResearch Background \n    \u003cdiv id=\"research-background\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#research-background\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h2\u003e\n\n\n\u003ch3 class=\"relative group\"\u003eExisting Work \n    \u003cdiv id=\"existing-work\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#existing-work\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h3\u003e\n\u003cp\u003eThe following content is generated by AI to quickly explain what RAP is.\u003c/p\u003e","title":"rStar Reading Notes","type":"blog"},{"content":" Detailed documentation of the process of deploying an LLM application from scratch, focusing mainly on the initial setup of cloud servers, continuously updated\u0026hellip; Preface # Before deploying your LLM application, you should have already completed:\nA web front-end framework with core functionalities A relatively complete back-end code repository This document mainly records the setup process of cloud services for testing and development environments. Cloud servers for production environments are not covered here, so please refer to other articles. No practice, no sayğŸ«¡\nSolution Exploration # Although this article is about \u0026ldquo;Cloud Server Setup,\u0026rdquo; I still want to document some \u0026ldquo;non-cloud service\u0026rdquo; solutions, as not all testing and development environments require an expensive cloud server. If you have already decided to use a cloud server, then skip to: Cloud Service Operation Process\nThe following content assumes your main production environment is Windows, as I have not used Linux or MacOSğŸ˜¢\nLocal Services # General Idea # If you are a truly \u0026ldquo;independent\u0026rdquo; developer with no need for collaborative development, then you don\u0026rsquo;t need a cloud server at all. You just need Docker.\nAfter installing Docker Desktop, you need to perform container orchestration based on your existing back-end service code.\nThis step mainly involves:\nEstablishing API interfaces: Writing the main operational logic, which can be done in any programming language you prefer Creating necessary environment files: Such as .env, pyproject.toml, etc. Creating a DockerFile: Essentially a set of command-line instructions to initialize your container Creating a docker-compose.yml file: In this file, you need to orchestrate the images used by your app, specifying internal networks, ports, mounted volumes, and other necessary settings Then, run the command: docker-compose up --build\nAfter Docker initialization, you can directly access your service via the localhost domain, just like using a cloud server.\nDocker Path Issues # When orchestrating Docker containers, there are several necessary working path parameters to understand, otherwise, you might encounter \u0026ldquo;file not found\u0026rdquo; errorsğŸ˜¢\nbuild.context: This parameter exists in docker-compose.yml and refers to the \u0026ldquo;build context,\u0026rdquo; pointing to a real directory on your local machine WORKDIR: This parameter exists in the Dockerfile and is used to specify the \u0026ldquo;working directory\u0026rdquo;; subsequent RUN, COPY, and other commands will execute in this directory if relative paths are used, but absolute path parameters are not affected Port Forwarding # If you have built a Docker service locally but your team members need a unified testing environment, you can simply perform port forwarding on your local service.\nThere are many applications that support port forwarding, and I won\u0026rsquo;t list them all here. I personally use Sakura Frp\nAlthough port forwarding is very convenient, if you need front-end and back-end interaction, it\u0026rsquo;s best not to use port forwardingğŸ˜¢\nMy initial idea was: run the front-end web page locally, and also run the back-end service in a local Docker container, then use port forwarding to forward the front-end web page to users and the back-end to the front-end.\nIt seemed fine, but since your network service is not SSL verified, it cannot send https requests, causing the browser to block cross-origin insecure resource requests from the front-end to the back-endğŸ˜¢ The result is that only your computer can run the complete service process, and other devices will fail; even if you configure a self-signed certificate, it\u0026rsquo;s hard to pass the client browser\u0026rsquo;s security mechanismğŸ˜­\nAfter unsuccessful attempts, I chose to use a cloud server.\nCloud Services # Now, the operation process of cloud servers is very simple, but there is one drawback: expensiveğŸ˜¢\nOf course, if it\u0026rsquo;s not for a production environment but a testing and development environment, you don\u0026rsquo;t need to choose a top-tier server. Choose a suitable one based on your financial situation and team size.\nI ultimately chose Tencent Cloud\u0026rsquo;s lightweight server: 2 cores + 2GB RAM + 4M bandwidth + 50GB system disk + 300GB monthly traffic\nNo other reason, mainly because it\u0026rsquo;s cheap, only ï¿¥88 for the first year.\nCloud Service Operation Process # Register a Domain # Set Up DNS Resolution # Purchase a Cloud Server # Configure the Server # The server operating system I use here is Ubuntu Server 24.04 LTS 64bit, and subsequent commands are based on this system.\nFile Transfer # To transfer files from the local machine to the server, I chose to use some more modern terminals, such as: Tabby, WindTerm, and Warp, etc.\nI randomly chose Tabby, which makes remote connections more convenient and has built-in SFTP for easy file transfer. If you have the energy to customize the terminal interface, Tabby can be very aesthetically pleasingğŸ˜„\nOf course, more traditional solutions include using FileZilla; and if you don\u0026rsquo;t mind the hassle, you can directly use terminal commands: use rsync or scp commands.\nIf you are using a Tencent Cloud server and are curious about what the lighthouse folder isğŸ¤¨: this folder is the account for one-click password-free login. Install Docker # Execute the following commands one by one, each command has an explanation:\nsudo apt-get update # Upgrade sudo apt install -y apt-transport-https ca-certificates curl gnupg lsb-release # Install dependency tools, mainly https transmission and verification-related toolkits # Install Docker\u0026#39;s GPG key to ensure the Docker image you download has not been tampered with sudo curl -fsSL https://mirrors.cloud.tencent.com/docker-ce/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc # Add Docker\u0026#39;s official software repository to the system\u0026#39;s APT source list # I don\u0026#39;t think you want to know what this bunch meansÙ©(â€¢Ì¤Ì€áµ•â€¢Ì¤Ìà¹‘) sudo install -m 0755 -d /etc/apt/keyrings sudo chmod a+r /etc/apt/keyrings/docker.asc echo \u0026#34;deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://mirrors.cloud.tencent.com/docker-ce/linux/ubuntu/ \\ $(. /etc/os-release \u0026amp;\u0026amp; echo \u0026#34;$VERSION_CODENAME\u0026#34;) stable\u0026#34; | sudo tee /etc/apt/sources.list.d/docker.list \u0026gt; /dev/null # Update to let apt recognize the newly added Docker software source sudo apt-get update # Install Docker engine sudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin # Start Docker sudo docker info # Verify installation information sudo systemctl start docker # Start service sudo systemctl enable docker # Enable auto-start on boot sudo systemctl status docker # Verify service status After running the last command, you will enter a paged log view mode, similar to a Vim editor. Enter :q to return to the regular command line. Use Docker # sudo systemctl start docker # Start service # Configure Tencent Cloud\u0026#39;s Docker source sudo vim /etc/docker/daemon.json # Create configuration file # Press the I key to switch to I mode and enter { \u0026#34;registry-mirrors\u0026#34;: [ \u0026#34;https://mirror.ccs.tencentyun.com\u0026#34; ] } # Press esc, then enter \u0026#39;:wq\u0026#39;, this is a Vim operation, meaning save and exit sudo systemctl restart docker # Restart Docker service sudo systemctl stop docker # Stop Docker service # Verify source information, you should see the Registry Mirrors value is the URL you set sudo docker info # Pull image sudo docker pull nginx # If you want to use docker-compose # On Ubuntu, use: docker compose sudo docker compose up --build # Start service without rebuilding sudo docker compose up -d # Stop all services sudo docker compose down Before using the docker compose command, please create docker-compose.yml and Dockerfile according to the official tutorial. Configure pip/poetry # Strangely, Tencent Cloud installs Python by default at the system level but does not install pipğŸ¤” So you need to manually install it. Before installation, please confirm whether Python is installed.\nIf you are not using pip at the system level but pip inside a Docker container, then changing the pip source at the system level will not have any effect. Change pip source at the system level:\n# Check Python version python3 -V # Check the path of python3 which python3 # Confirm Python is installed and pip is not installed sudo apt install python3-pip # If using Tencent\u0026#39;s server, you can use the intranet domain for faster speed pip config set global.index-url https://mirrors.tencentyun.com/pypi/simple # If not, use the external domain pip config set global.index-url https://mirrors.cloud.tencent.com/pypi/simple Change pip source inside Docker container: Directly write the following command in your Dockerfile\n# Configure pip source # Use Tencent Cloud mirror source, note this is the intranet domain RUN pip config set global.index-url https://mirrors.tencentyun.com/pypi/simple \\ \u0026amp;\u0026amp; pip config set global.trusted-host mirrors.tencentyun.com If you use poetry at the system level:\n# Install poetry using online script curl -sSL https://raw.githubusercontent.com/python-poetry/poetry/master/get-poetry.py | python # Install poetry using apt sudo apt install python3-poetry # Use Tencent Cloud mirror source, note this is the intranet domain sudo poetry config repositories.tencentyun https://mirrors.tencentyun.com/pypi/simple If your system does not have pip installed by default, it is not recommended to install pip first and then install poetry. SSL/TLS # SSL/TLS is a set of encrypted transmission protocols, carefully designed by cryptography experts. The widely used Hypertext Transfer Protocol Secure (HTTPS) is obtained by encrypting the plaintext transmission protocol HTTP using SSL/TLS.\nA core part of HTTPS is the handshake before data transmission, during which the password for data encryption is determined. During the handshake, the website sends an SSL certificate to the browser. The SSL certificate is similar to our daily ID card, serving as an identity proof for HTTPS websites. The SSL certificate contains the website\u0026rsquo;s domain name, certificate validity period, certificate issuing authority, and the public key used for encrypting the transmission password. Before generating the password, the browser needs to verify whether the currently accessed domain name matches the domain name bound to the certificate, and also verify the Certificate Authority (CA). If the verification fails, the browser will give a certificate error prompt.\nGenerally, SSL certificates need to be purchased from CA institutions, but some CA institutions provide free SSL certificate services, such as: Letâ€™s Encrypt\nAfter long-term development, the certificates issued by Letâ€™s Encrypt can now be automatically processed through Certbot, and the certificates can be automatically re-applied when they expire. Of course, such convenience comes at a costğŸ˜¢: You need to configure certbot, which is quite troublesome.\nHere, I chose to directly deploy a certbot inside a Docker container to apply for SSL certificates, meaning you can directly modify the content in docker-compose.yml to pull the official certbot image.\n# First, stop all Docker services sudo docker compose down # Start all services sudo docker compose up -d # Start part of the services sudo docker compose up nginx -d # View logs of a specific image sudo docker logs server-app-1 # View logs of a specific service sudo docker compose logs certbot-init # Use recursive deletion to clear cache rm -rf ./certbot/conf/* # Enter a container\u0026#39;s shell sudo docker exec -it server-app-1 /bin/sh # Remove a Docker image forcefully sudo docker rmi -f server-frontend # Restart a specific service in Docker Compose sudo docker compose restart app # Clear npm cache forcefully npm cache clean --force # Activate a Python virtual environment source your_venv_name/bin/activate Filing # Filing using a corporate entity is not difficult, so I won\u0026rsquo;t elaborate too much here. This section mainly discusses individual entity filing. Main reference: Tencent Cloud Server Filing Full Process 40 Days of Filing Blood and Tears - Blog Garden\ngraph LR; p1(Server Platform Real-name Authentication)--\u003ep2(Real-name Authentication Purchase Domain)--\u003ep3(Apply for Filing)--\u003ep4(Platform Review)--\u003ep5(Regulatory Authority Review)--\u003ep6(Public Security Filing) If you are a Tencent Cloud user, you can use the Tencent Cloud Website Filing Mini Program; after submitting the application, the platform will first review it, usually taking about 8 hours; then the platform will submit it to the regulatory authority for review, usually taking about 7 working days; after the regulatory authority review is passed, you need to complete the public security filing within 30 working days.\nMiscellaneous # This section mainly documents issues that may arise during the entire operation process.\nCertbot Access Denied? # First, of course, check the network reasons. Check whether the security group of the server has opened ports 443 and 80, and check whether the firewall of the server operating system has blocked port access.\n# Ubuntu installs ufw firewall by default # Check ufw status, inactive status means not started, which is the ideal state sudo ufw status If you\u0026rsquo;re based in China, here\u0026rsquo;s another issue: Certbot can\u0026rsquo;t reach your server if you don\u0026rsquo;t have ICPå¤‡æ¡ˆ (ICP license). ğŸ˜¢\nReferences # Using Third-party SSH Terminal to Log in to Linux Instance - Operation Guide - Tencent Cloud Using Local Built-in SSH Terminal to Log in to Linux Instance - Operation Guide - Tencent Cloud Installing Docker in Linux Environment Ubuntu 22.04 Install Docker What is Docker\u0026rsquo;s Official GPG Key For? Cloud Server Setup Docker - Practice Tutorial - Tencent Cloud Installing Docker and Configuring Mirror Acceleration Source - Practice Tutorial - Tencent Cloud Debian 12 / Ubuntu 24.04 Install Docker and Docker Compose Tutorial - Shaobing Blog Step-by-Step Guide to Python pip Source Change Operation in Linux System - Tencent Cloud pip Source Configuration - Tencent Cloud Developer Community - Tencent Cloud How to Change Domestic Source for Poetry - Data Science SourceResearch Linux Stop Docker Container: Single, Multiple, or All Where to View Docker Logs? How to View Logs in Linux Server - CSDN Blog Troubleshooting and Solving Linux Firewall Closed but Still Unable to Access Web About Ubuntu Console Opened All Ports, but External Hosts Still Cannot Access Corresponding Port Services - CSDN Blog Three Minutes to Explain SSL Authentication and Encryption Technology - CSDN Blog One Article to Thoroughly Understand SSL/TLS Protocol - Zhihu HTTPS and SSL Certificate Overview | Rookie Tutorial Four Common Free Certificate Application Methods - CSDN Blog Using Docker to Deploy Nginx and Configure HTTPS - TandK - Blog Garden Using Docker + Nginx + Certbot to Automate SSL Certificate Management - CSDN Blog The Certificate Authority Failed to Download the Temporary Challenge Files Created by Certbot \u0026ndash; Connection Refused - Help - Let\u0026rsquo;s Encrypt Community Support ICP Filing First Filing - Tencent Cloud ICP Filing Requirements by Province - Tencent Cloud ","date":"6 March 2025","externalUrl":null,"permalink":"/en/blog/cloud-server-build/","section":"Blogs","summary":"\u003cdiv class=\"lead text-neutral-500 dark:text-neutral-400 !mb-9 text-xl\"\u003e\n  Detailed documentation of the process of deploying an LLM application from scratch, focusing mainly on the initial setup of cloud servers, continuously updated\u0026hellip;\n\u003c/div\u003e\n\n\n\n\u003ch2 class=\"relative group\"\u003ePreface \n    \u003cdiv id=\"preface\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#preface\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h2\u003e\n\u003cp\u003eBefore deploying your LLM application, you should have already completed:\u003c/p\u003e","title":"Cloud Service Deployment","type":"blog"},{"content":" A detailed record of the core features of the Qdrant vector database, helping developers design better databases in practical applications ğŸ˜‹ Preface # Vector database-based RAG is the most fundamental form of RAG and is widely used in production. As the name suggests, vector databases primarily store vectors. Through vector operations, we can achieve more precise relevance searches, which serve as the cornerstone for many application scenarios.\nThere are many vector database providers: Weaviate, Qdrant, Milvus, Chroma, and more. However, they are generally similar. For a detailed comparative analysis of mainstream vector databases, see: Vector Database Comparison Report: A Detailed Comparison of 12 Mainstream Vector Databases.\nThis article focuses solely on Qdrant. All content is based on the Qdrant Official Documentation (as of March 2025).\nThis article focuses less on specific syntax and more on features and principles, helping developers quickly design product workflows without being constrained by cumbersome syntax, thereby improving imagination and efficiency.\nBasic Data Types # Qdrant defines some abstract data types to better handle vector data. Understanding these types is fundamental to flexible usage.\nPoint # Points are the core data type in a vector database. All operations revolve around points.\nA very basic point contains only its vector, but typically, points are tagged with additional metadata to provide more information beyond the vector data. These tags are called Payload ğŸ‘‡.\nThus: Point = Vector + Payload Tags\n// A simple point { \u0026#34;id\u0026#34;: 129, \u0026#34;vector\u0026#34;: [0.1, 0.2, 0.3, 0.4], \u0026#34;payload\u0026#34;: {\u0026#34;color\u0026#34;: \u0026#34;red\u0026#34;}, } Qdrant points can be configured with various types of vectors: Dense Vectors, Sparse Vectors, Multi-Vectors, and Named Vectors.\nVector Type Description Dense Vectors Standard vectors; most embedding models generate this type. Sparse Vectors Variable-length vectors with few non-zero elements; typically used for exact token matching and collaborative filtering. Multi-Vectors Matrices composed of multiple dense vectors; dimensions must match across points, but the number of vectors can vary. Used to store different vector descriptions of the same target. Named Vectors A hybrid of the above types, allowing different vector types to coexist in a single point. These vectors are abstracted as named vectors. Basic CRUD operations are straightforward and won\u0026rsquo;t be elaborated here.\nPayload # Additional metadata attached to vectors, described and stored using JSON syntax. Here\u0026rsquo;s an example:\n{ \u0026#34;name\u0026#34;: \u0026#34;jacket\u0026#34;, \u0026#34;colors\u0026#34;: [\u0026#34;red\u0026#34;, \u0026#34;blue\u0026#34;], \u0026#34;count\u0026#34;: 10, \u0026#34;price\u0026#34;: 11.99, \u0026#34;locations\u0026#34;: [ { \u0026#34;lon\u0026#34;: 52.5200, \u0026#34;lat\u0026#34;: 13.4050 } ], \u0026#34;reviews\u0026#34;: [ { \u0026#34;user\u0026#34;: \u0026#34;alice\u0026#34;, \u0026#34;score\u0026#34;: 4 }, { \u0026#34;user\u0026#34;: \u0026#34;bob\u0026#34;, \u0026#34;score\u0026#34;: 5 } ] } Since this data is stored in the database, it serves a purpose: vector databases primarily rely on semantic similarity matching, and these tags allow you to add additional logical filtering conditions on top of that.\nFor more on filtering, see: Filtering\nCollection # A collection is simply a group of points. At this level, you can define:\nSimilarity algorithms between points Vector dimensions Optimizer configurations HNSW algorithm configurations: The key parameter to adjust is ef, which determines the number of neighboring nodes the algorithm visits. Higher values yield more accurate but slower queries. WAL configurations Quantization configurations Common Operations # These are the most fundamental and important operations when using a vector database.\nSearch # In the context of vector databases, \u0026ldquo;search\u0026rdquo; primarily refers to similarity search. The theoretical premise is that objects with higher similarity in the real world are closer in vector space.\nThe term \u0026ldquo;closer\u0026rdquo; implies a similarity metric. Qdrant supports several popular similarity algorithms:\nDot Product Similarity Cosine Similarity Euclidean Distance Manhattan Distance To improve performance, all vectors are normalized before storage. This means dot product similarity and cosine similarity are equivalent in Qdrant. For a better user experience, Qdrant provides a complete API for easy invocation: Search API - Qdrant.\nIn summary, the functionalities you can invoke include:\nBasic Operations: Input a vector to perform similarity matching in the database. If your points store \u0026ldquo;named vectors,\u0026rdquo; you need to specify which vector to use for matching. Search Algorithm Control: You can enable exact search, which performs similarity matching on every point, taking longer (more parameters are adjustable but rarely used). Result Filtering: Filter results before the actual search using payload tags to narrow the scope, or after the search using similarity score thresholds. Result Count: The limit parameter controls the number of results returned (the top limit most similar results). Batch Search: Input multiple vectors for search in one go. Search Grouping: Group search results by certain tags. The group_size parameter sets the number of results per group. Search Planning: Based on optional indexes, filter complexity, and the total number of points, a heuristic method selects an appropriate search approach (improving performance ğŸ¤”). If both group_size and limit are set, limit represents the number of groups. Additionally, sparse and dense vector searches in Qdrant have key differences:\nComparison Sparse Vectors Dense Vectors Similarity Algorithm Defaults to dot product; no need to specify You can specify supported algorithms Search Method Only exact search Can use HNSW for approximate search Search Results Returns only vectors with shared non-zero elements Returns the top limit vectors Explore # Explore operations are more flexible searches: they can query based on similarity as well as dissimilarity.\nRecommendation # \u0026ldquo;Recommendation\u0026rdquo; allows you to provide both positive and negative vectors for search. Here\u0026rsquo;s an official example:\nimport { QdrantClient } from \u0026#34;@qdrant/js-client-rest\u0026#34;; const client = new QdrantClient({ host: \u0026#34;localhost\u0026#34;, port: 6333 }); client.query(\u0026#34;{collection_name}\u0026#34;, { query: { recommend: { positive: [100, 231], negative: [718, [0.2, 0.3, 0.4, 0.5]], strategy: \u0026#34;average_vector\u0026#34; } }, filter: { must: [ { key: \u0026#34;city\u0026#34;, match: { value: \u0026#34;London\u0026#34;, }, }, ], }, limit: 3 }); In the official example, 100 and 231 are vector IDs, each corresponding to a 4-dimensional vector. The strategy parameter controls the search algorithm. Here are the details:\nAverage Algorithm: Averages positive and negative examples separately, then combines them into a final search vector. Best Score Algorithm: Each candidate point is matched against all positive and negative examples to compute scores. The highest scores are selected, and the final score is calculated as: let score = if best_positive_score \u0026gt; best_negative_score { best_positive_score } else { -(best_negative_score * best_negative_score) }; Negative-Only Algorithm: Uses the best score algorithm ğŸ‘† without positive examples, yielding a reverse scoring algorithm to find the least relevant points. Multi-vectors and other special vectors can also be processed with similar logic, though the syntax differs. Discovery # \u0026ldquo;Discovery\u0026rdquo; operations partition the sample space. You provide pairs of positive and negative vectors, each dividing the space into positive and negative regions. The search returns points that lie more in positive regions or less in negative regions.\nSimilar to Recommendation, but here you pair positive and negative vectors together as input.\nDue to hard partitioning, consider increasing the HNSW ef parameter to compensate for precision loss. Discovery operations enable Qdrant to handle two new search requirements:\nDiscovery Search: Given a target vector and a set of positive-negative vector pairs as contextual constraints. Region Partition Search: A special case of discovery search ğŸ‘† where no target vector is provided. The database partitions the space directly and returns points lying most in positive regions. In discovery search, contextual constraints are enforced with higher priority. In other words, discovery search first performs region partitioning, then standard similarity search. Distance Matrix # This operation resembles batch search. Batch search inputs multiple vectors and searches for similar vectors for each. Distance matrix randomly selects a subset of vectors, then searches for similar vectors within this subset for each vector.\nFor example, if sample=100 vectors are selected to form a subset, and limit=10 similar vectors are searched for each, the returned distance matrix will be a \\(100 \\times 10\\) matrix, where each row represents the top 10 similar vectors for one point.\nThis operation is typically used for data visualization or dimensionality reduction.\nFiltering # Official English guide: A Complete Guide to Filtering in Vector Search. The guide explains how Qdrant internally performs filtering, which helps design efficient systems.\nThe guide briefly lists some functionalities. For complete details, see: Filtering.\nFilter Conditions # These refer to individual filter conditions, the basic units of filtering. Here are the types:\nType Function Match The condition is a specific value; the attribute must exactly match it. Match Any The condition is a set of options; the attribute must match any of them. Match Except The condition is a set of options; the attribute must not match any of them. Range The condition is a range; the attribute must lie within it. Values Count The attribute is an array; filtering is based on the number of elements. Is Empty Filters based on whether the attribute exists. These are the basic filter types. Syntax varies for different payload types:\nJSON Payload: A point\u0026rsquo;s payload can be a JSON object, and any field can participate in filtering. See Nested Key. Date Range: Similar to numeric ranges, date ranges can also filter. Geofiltering: Geographic locations can be filtered. Named Vectors: Named vectors contain vectors of different dimensions. Filtering can check for the presence of specific vectors (e.g., filtering points with image embeddings). These basic conditions can be nested using logical keywords ğŸ‘‡ to form complex filters.\nLogical Keywords # Similar to SQL\u0026rsquo;s AND, OR, NOT, Qdrant uses must, should, must_not to express similar logic. These keywords build complex filters.\nmust: Returns true only if all listed conditions are met. should: Returns true if any listed condition is met. must_not: Returns true if none of the listed conditions are met. Advanced Operations # Hybrid Queries # Optimization # Storage # Indexing # Snapshots # References # Vector Database Comparison Report: A Detailed Comparison of 12 Mainstream Vector Databases | SynDataWorks Qdrant Official Documentation ","date":"5 March 2025","externalUrl":null,"permalink":"/en/blog/qdrant-feature-guide/","section":"Blogs","summary":"\u003cp\u003e\n\n\u003cdiv class=\"lead text-neutral-500 dark:text-neutral-400 !mb-9 text-xl\"\u003e\n  A detailed record of the core features of the Qdrant vector database, helping developers design better databases in practical applications ğŸ˜‹\n\u003c/div\u003e\n\u003c/p\u003e\n\n\n\u003ch2 class=\"relative group\"\u003ePreface \n    \u003cdiv id=\"preface\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#preface\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h2\u003e\n\u003cp\u003eVector database-based RAG is the most fundamental form of RAG and is widely used in production. As the name suggests, vector databases primarily store vectors. Through vector operations, we can achieve more precise \u003cstrong\u003erelevance searches\u003c/strong\u003e, which serve as the \u003cstrong\u003ecornerstone\u003c/strong\u003e for many application scenarios.\u003c/p\u003e","title":"Qdrant Feature Guide","type":"blog"},{"content":" This is an independent full-stack development journey, driven by the goal of building an LLM application. It covers various issues encountered during development.\nIt\u0026rsquo;s also the introductory articleâ€”article zeroâ€”of the \u0026ldquo;AI Engineering\u0026rdquo; series, serving both as an index to the series and a summary of personal insights.\nIntroduction # Nowadays, we\u0026rsquo;ve become accustomed to effortlessly interacting with AI chat assistants like Doubao, Kimi, and DeepSeek. It seems like creating something similar ourselves shouldn\u0026rsquo;t be too difficult, right?\nThat innocent thought was exactly where my dream began ğŸ˜¢. Sounds silly, I know, but that\u0026rsquo;s genuinely how I felt. And from there began a long, challenging development journey.\nIf you\u0026rsquo;ve ever had a similar idea, the following content might help you ğŸ¤—.\nAfter countless late nights, I have to admit: building an LLM application is a massive endeavor, especially for an independent developer. Choosing independent development or assembling a small team means stepping onto a path filled with challenges. There are no \u0026ldquo;senior mentors\u0026rdquo; guiding your technical choices; you have to explore everything on your own.\nBut, that\u0026rsquo;s exactly the joy of independent developmentâ€”you have complete control over your product and get to witness every incremental improvement.\nYou might wonder: \u0026ldquo;What if my product doesnâ€™t perform well?\u0026rdquo; \u0026ldquo;What if I hit a development roadblock?\u0026rdquo; \u0026ldquo;If nobody uses it, wouldnâ€™t my effort go to waste?\u0026rdquo; ğŸ¤”\nAdmittedly, independent development demands significant investment, and these worries are inevitable, even universal, among independent developers.\nHowever, if you\u0026rsquo;ve genuinely committed yourself to independent development, regardless of the outcome, you\u0026rsquo;ll certainly gain something valuable. If you succeed, congratulations ğŸ¥³â€”your hard work has paid off. If you fail or give up, I understand your frustration, disappointment, even anger.\nBut no one finds happiness or success by completely dismissing their past experiences. Momentarily set aside the pain, and move forward to your next adventure.\nIn life, there are no wasted steps; every step counts ğŸ«¡.\nIdentifying Needs # Draft draft draft\u0026hellip;\nExploring Implementation Options # Draft draft draft\u0026hellip;\nProject Construction # Draft draft draft\u0026hellip;\nDeploying the Service # Cloud Service Deployment ","date":"4 March 2025","externalUrl":null,"permalink":"/en/blog/llm-app-driven-fullstack-dev/","section":"Blogs","summary":"\u003cdiv class=\"lead text-neutral-500 dark:text-neutral-400 !mb-9 text-xl\"\u003e\n  \u003cp\u003eThis is an independent full-stack development journey, driven by the goal of building an LLM application. It covers various issues encountered during development.\u003c/p\u003e\n\u003cp\u003eIt\u0026rsquo;s also the introductory articleâ€”article zeroâ€”of the \u0026ldquo;AI Engineering\u0026rdquo; series, serving both as an index to the series and a summary of personal insights.\u003c/p\u003e","title":"A Guide to Independent Full-Stack Development Driven by LLM Applications","type":"blog"},{"content":" The basic info and operations of Neo4j Graph Database. Introduction # Neo4j is a high-performance graph database that stores data and the relationships between data in the form of graphs.\nThe specific form of the graph is a labeled property graph, and the query language used is Cypher.\nLabeled Property Graph # A labeled property graph is a specific type of graph:\nA node has one or more labels to define its type. Relationships and nodes are treated equally, holding the same level of importance. Each node and relationship possesses accessible properties. Cypher # Cypher is aÂ declarativeÂ query language that allows you to identifyÂ patternsÂ in your data using anÂ ASCII-art style syntaxÂ consisting ofÂ brackets,Â dashesÂ andÂ arrows.\nPatterns # A pattern in a graph database is a specific combination of nodes and relationships. For example, a person (Person) acting in a movie (Movie) is a pattern, represented in code as:\n(p:Person)-[r:ACT_IN]-\u0026gt;(m:Movie) Here, the parts enclosed in parentheses represent nodes, while the parts enclosed in square brackets represent relationships. In the node section, p and m are variables referring to the respective nodes, with Person and Movie being the labels of the nodes, connected by a colon. In the relationship section, r is the variable referring to the relationship, and ACT_IN is the specific type of relationship. Translated into natural language, this means: use p to refer to a node labeled Person, use m to refer to a node labeled Movie, and represent the relationship between them with r, which is of type ACT_IN.\nData Reading # Data reading operations rely on pattern matching, using the keyword MATCH. This is equivalent to sending an instruction to the database to filter out only the node-relationship pairs that match the specified pattern, i.e., a collection of triples (p, r, m).\nMATCH (p:Person)-[r:ACT_IN]-\u0026gt;(m:Movie) Similar to SQL, you can further filter using the WHERE keyword:\nMATCH (p:Person)-[r:ACT_IN]-\u0026gt;(m:Movie) WHERE p.name = \u0026#39;Tom Hanks\u0026#39; RETURN p, r, m Here, name is a property of the node p, accessed using the . operator. The RETURN keyword marks the values to be returned.\nBelow is a more complex pattern matching command, where the AS keyword is used to set aliases:\nMATCH (p:Person)-[:ACTED_IN]-\u0026gt;(m:Movie)\u0026lt;-[r:ACTED_IN]-(p2:Person) WHERE p.name = \u0026#39;Tom Hanks\u0026#39; RETURN p2.name AS actor, m.title AS movie, r.role AS role This command filters out all actors who have acted in the same movie as Tom Hanks, returning their names, the movies they acted in, and the roles they played.\nSorting by a specific property and pagination are also supported:\nMATCH (m:Movie) WHERE m.released IS NOT NULL RETURN m.title AS title, m.url AS url, m.released AS released ORDER BY released DESC LIMIT 5 This will filter out the 5 most recent movies.\nCypher keywords are case-insensitive; property names, variable names, and other identifiers are case-sensitive. Data Writing # Data writing uses the MERGE keyword, which means merging a node or relationship into the data graph.\nMerging nodes: MERGE (m:Movie {title: \u0026#34;Arthur the King\u0026#34;}) SET m.year = 2024 RETURN m This command creates a new Movie node with the title property set to \u0026quot;Arthur the King\u0026quot; and the year property set to 2024.\nYou might wonder why not write it like this:\nMERGE (m:Movie) SET m.year = 2024, m.title = \u0026#34;Arthur the King\u0026#34; RETURN m The reason is that the content within the curly braces is used to check whether the pattern you want to create already exists, avoiding duplicate patterns.\nMerging relationships: MERGE (m:Movie {title: \u0026#34;Arthur the King\u0026#34;}) MERGE (u:User {name: \u0026#34;Adam\u0026#34;}) MERGE (u)-[r:RATED {rating: 5}]-\u0026gt;(m) RETURN u, r, m Installation # As of February 15, 2025, after extensive testing by the developer community, it has been found that Neo4j Desktop is blocked for users in mainland China, causing the software interface to fail to display properly. While bypassing the block through methods such as disabling the network is possible, this removes the key advantage of the desktop versionâ€”easy deployment.\nThus, I recommend using the Docker deployment method.\nDocker Desktop # Simply install Docker Desktop and run it in the background.\nPulling the Image # Normally, you can simply pull the latest image with the command: docker pull neo4j.\nHowever, if your project requires the APOC plugin, you should consider the version of APOC, as the image may be ahead of the APOC version. The community version of APOC is released on Releases Â· neo4j/apoc.\nFor APOC compatibility, use the following command: docker pull neo4j:5.26.2.\nBuilding the Container # docker run -d -p 7474:7474 -p 7687:7687 -v E:/neo4j/data:/data -v E:/neo4j/logs:/logs -v E:/neo4j/conf:/var/lib/neo4j/conf -v E:/neo4j/import:/var/lib/neo4j/import -v E:/neo4j/plugins:/var/lib/neo4j/plugins -e NEO4J_dbms_security_procedures_unrestricted=\u0026#34;apoc.*\u0026#34; -e NEO4J_dbms_security_procedures_allowlist=\u0026#34;apoc.*\u0026#34; -e NEO4JLABS_PLUGINS=\u0026#39;[\u0026#34;apoc\u0026#34;]\u0026#39; -e NEO4J_AUTH=neo4j/mo123456789 --name neo4j neo4j:5.26.2 Explanation of the parameters:\nThe -p option exposes ports; here, we open two ports. The -v option mounts directories from the host machine (this means specifying where the Docker application will store its data). The -e option configures environment variables. The apoc-related configurations are for the APOC plugin; NEO4J_AUTH sets the username to neo4j and the password to mo123456789. If you use Neo4j for language model-enhanced generation (RAG), be sure to include the APOC-related configurations. Otherwise, you can omit these settings. After running the command above, check the host machine\u0026rsquo;s mounted directory to confirm if the APOC plugin is installed:\nManual Installation of APOC # Visit Releases Â· neo4j/apoc, download the latest apoc-5.26.2-core from the \u0026ldquo;Assets\u0026rdquo; section, and paste it into the host machine\u0026rsquo;s specified directory. Then restart the Docker container.\nBrowser UI # By default, the 7474 port is used for the browser UI, and the 7687 port is for other backend applications.\nWhen the container is running in the background, access the UI at: http://localhost:7474/browser/preview.\nConnect to the database using: neo4j://localhost:7687.\nYou should now be able to access the browser UI successfully. ğŸ˜„\nReferences # Docker: Docker Deployment of Neo4j Graph Database - Angry Radish - Blog ","date":"5 February 2025","externalUrl":null,"permalink":"/en/blog/neo4j-basics/","section":"Blogs","summary":"\u003cdiv class=\"lead text-neutral-500 dark:text-neutral-400 !mb-9 text-xl\"\u003e\n  The basic info and operations of Neo4j Graph Database.\n\u003c/div\u003e\n\n\n\n\u003ch2 class=\"relative group\"\u003eIntroduction \n    \u003cdiv id=\"introduction\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#introduction\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/neo4j/neo4j\" target=\"_blank\"\u003eNeo4j\u003c/a\u003e is a high-performance graph database that stores data and the relationships between data in the form of graphs.\u003c/p\u003e","title":"Neo4j Basics","type":"blog"},{"content":"TODO_UPDATE Analysis Report on the Current State of Schedule Management Software Functionality Preface # The term \u0026ldquo;schedule management\u0026rdquo; frequently appears in our daily lives, yet it remains elusive: What exactly is schedule management? Why do we need it? What purpose does it serve? Who engages in schedule management, and how effective is it?\nWhat is Schedule Management? # On the surface, \u0026ldquo;schedule management\u0026rdquo; seems like a common concept, but in reality, it encompasses complex and multifaceted content.\nIn summary, schedule management is a method of clearly recording, planning, organizing, and optimizing personal or team time and tasks to achieve set goals.\nDepending on different needs, it can be categorized into the following types:\nPersonal Growth and Self-Improvement:\nCharacteristics: Entirely dependent on personal will, often with a broad goal but no specific events or timelines, possibly with a quantitative metric. Primary Purpose: Facilitate long-term personal growth, improve quality of life, and enhance satisfaction. Professional and Academic Management:\nCharacteristics: Highly specific events and timelines, not driven by personal preference, often requiring collaboration with others, usually with one or more quantitative metrics. Primary Purpose: Achieve concrete career goals, enhance professional skills, and improve efficiency. Social and Relationship Management:\nCharacteristics: Involves complex interpersonal dynamics, lacks quantitative metrics, is unpredictable, and has low controllability. Primary Purpose: Maintain, expand, and optimize social networks, increase social capital. Financial and Daily Life Management:\nCharacteristics: Tasks are trivial, frequently repetitive, flexible yet follow periodic patterns, and generally short in duration. Primary Purpose: Ensure a sense of order and stability in life. In reality, schedules are a mix of the above aspects. Thus, when we casually refer to \u0026ldquo;schedule management,\u0026rdquo; we vaguely mean \u0026ldquo;managing all aspects of life,\u0026rdquo; leading to a very ambiguous understanding of the concept.\nWhat Constitutes \u0026ldquo;Good\u0026rdquo; Schedule Management? # Generally, \u0026ldquo;good schedule management\u0026rdquo; is a clear, realistic, flexible system that effectively balances long-term goals with short-term actions, incorporates regular feedback and continuous adjustment mechanisms, significantly enhances personal control, reduces stress, and is easy to record and execute. Specifically, it should meet the following criteria:\nGoal Clarity Realistic and Executable Flexibility \u0026amp; Resilience Balance between Long-term and Short-term Feedback and Adjustment Mechanisms Reduce Stress and Enhance Control Ease of Use \u0026amp; Clear Recording From a user experience perspective, \u0026ldquo;good schedule management\u0026rdquo; simply needs to answer the following questions:\nHow to collect necessary information: User schedules, preferences, weather data, online information, etc. How to process the collected information: Different workflows for different types of schedules. How to enhance user experience: Streamlined and warm interactions, intuitive and concise information presentation. Industry Status # Below is a list of existing schedule management solutions. However, to be honest, there are few domestic applications in this niche, while international offerings, though numerous, suffer from severe homogenization.\nTickTick # TickTick offers comprehensive functionality with a clean design, representing a classic static schedule management app. Its logic for schedule management is roughly as follows:\ngraph LR; id1(\u0026#34;Schedule Collection\u0026#34;)--\u0026gt;id2(\u0026#34;Manual Schedule Arrangement\u0026#34;)--\u0026gt;id3(\u0026#34;Execute Schedule on Time\u0026#34;); id2(\u0026#34;Manual Schedule Arrangement\u0026#34;)--\u0026gt;id4(\u0026#34;Schedule Not Executed on Time\u0026#34;)--\u0026gt;id5(\u0026#34;Manual Adjustment\u0026#34;); Dola # Dola is a futuristic AI-powered schedule management app: it has no interface, and all interactions with the AI occur via messaging platforms like WhatsApp or Apple Messages. However, it does not support mainstream Chinese apps like QQ or WeChat.\nBased on official descriptions, hereâ€™s an approximate user flow:\ngraph LR; id1(\u0026#34;Send Message via Platform\u0026#34;)--\u0026gt;id2(\u0026#34;AI Analyzes and Generates Schedule\u0026#34;)--\u0026gt;id3(\u0026#34;Store Schedule Internally\u0026#34;); id1(\u0026#34;Send Message via Platform\u0026#34;)--\u0026gt;id4(\u0026#34;AI Analyzes and Modifies Schedule\u0026#34;); id1(\u0026#34;Send Message via Platform\u0026#34;)--\u0026gt;id5(\u0026#34;AI Analyzes and Returns Query Results\u0026#34;) Motion # Motion is an AI-driven work management app primarily targeting entrepreneurial teams, with many features designed for collaboration. During testing, we focused on its schedule arrangement capabilities but found the results underwhelming.\nThe core logic is: divide time into work and personal blocks, then let the AI insert tasks into available slots. The outcome essentially devolves into simple task insertion: if thereâ€™s free time, a task is addedâ€”even if the deadline is a month away or the day is already packed.\nSummary # Static Schedule Management # Manual adjustments are unavoidable in static schedule management apps and represent the most labor-intensive part of the process. While these apps can create time blocks for scheduling, they lack dynamic adjustment capabilities. This isnâ€™t an issue for fixed, external schedules with clear start and end times.\nHowever, for internal, flexible, or trivial tasksâ€”like memorizing 15 words, doing laundry, or reading a magazineâ€”these tasks often lack specific start times and only need completion by the end of the day. They also lack clear end times; for example, memorizing 15 words might take 30 minutes, but on a good day, it could take only 20.\nThese trivial tasks pose a significant challenge to static scheduling. While a single taskâ€™s delay or early completion may not disrupt the schedule, their cumulative effect can be highly disruptive.\nAdditionally, if these scattered tasks arenâ€™t aligned, they can create time vacuums, leaving users unsure of what to do next. For instance, if a user finishes memorizing words early and has 10 minutes to spare, how should they use it? Without clear guidance, they might default to scrolling through short videos, leading to further delays and more time vacuums.\nTo break this vicious cycle, three strategies exist:\nManual Adjustment by the User: Highly cumbersome, wasting precious time on trivial arrangements, and potentially causing further delays. Ignore Adjustments and Time Control: This renders schedule management meaningless, reducing it to a simple to-do list without time controlâ€”defeating its purpose. Strict Adherence to the Schedule: Theoretically perfect, but practically ineffective. Life is unpredictable, and delays or early completions disrupt subsequent tasks, leading to rushed work or wasted time. Time vacuums and their cascading effects are the Achilles\u0026rsquo; heel of static schedule management apps, severely limiting their adoption. Consequently, schedule management is often seen as a niche activity for highly disciplined individuals, and the software is perceived as catering only to this small group.\nIâ€™ve used several static schedule management apps but struggled to stick with them: either the learning curve was too steep, or rigid schedules were impractical to follow. Thus, a truly meaningful schedule management tool shouldnâ€™t demand a life free of surprises or strict adherence to plans. Instead, it should provide solutions for when things go off track.\nDynamic Schedule Management # Existing dynamic schedule management apps partially address the inflexibility of static tools but remain constrained by traditional paradigms, requiring extensive manual input. This creates a paradox: if users are inputting so much manually, why use AI at all? Why not rely on faster, more precise algorithms?\nBolder innovations are needed to leverage the full potential of large language models for generalized processing.\nUnderstanding the Current Landscape # Understanding public perceptions and needs regarding schedule management is crucial for defining the appâ€™s form.\nWe conducted a small-scale survey, revealing:\nMost respondents only occasionally plan their schedules. Dedicated schedule management apps are less popular than built-in phone notes. Over half reported that most planned schedules werenâ€™t completed as intended. Among desired features, \u0026ldquo;ease of use\u0026rdquo; ranked highest, followed by \u0026ldquo;personalization.\u0026rdquo; Though the sample size was small, we can infer:\nPublic awareness of schedule management is low. Simplicity and lowering barriers to use should be foundational. Personalization is another key area for optimization. Technical Proposal # Based on the above, we propose the following goals:\nBreak the illusion that mental scheduling is efficient, reduce reliance on deadlines, address the high cost and inflexibility of traditional tools, and solve AIâ€™s inability to capture implicit preferences or balance long- and short-term goals. Make schedule management a practical part of daily life, genuinely improving efficiency and quality of lifeâ€”truly enabling: Easy Schedule, Simple Life.\nFor feasibility, we exclude: social/relationship management, financial tracking, and team collaboration, focusing solely on individual usersâ€™ needs for personal growth and daily task management.\nInterface Design # To keep the interface intuitive, we draw inspiration from life: a boss communicates simply with a secretary to arrange schedules.\nThus, the interface adopts the simplest form: a chat dialog, akin to messaging a personal assistant on QQ or WeChat.\nEssential Data Collection # Since user interaction occurs via chat, all user-related data must be extracted from messages. Key data includes:\nSchedule Data: Tasks the user needs to complete, e.g., \u0026ldquo;Submit academic English homework by next Monday.\u0026rdquo; User Preferences: Unconscious habits, e.g., procrastinating on English homework. Execution Data: Whether scheduled tasks were completed. External Data: Weather, news, etc., fetched from the web. Schedule data comes from user messages, preferences are inferred from adjustments to \u0026ldquo;unreasonable\u0026rdquo; schedules, and external data is retrieved as needed.\nProcessed data is stored long-term in a \u0026ldquo;memory system,\u0026rdquo; technically implemented as RAG: vector databases for schedules/preferences, relational databases for arranged tasks, with AI granted direct access for \u0026ldquo;memory updates.\u0026rdquo;\nData Processing # The \u0026ldquo;memory system\u0026rdquo; uses semantic and temporal searches to populate prompts, guiding the AIâ€™s scheduling decisions.\nA pre-built knowledge base provides scientific principles for scheduling, ensuring plans are both personalized and scientifically sound.\nData Presentation # To maintain simplicity, only two elements are displayed:\nChat Dialog: For input and scheduling. TODO List: For tracking task completion. System Implementation # The system architecture is shown above, with the following workflow:\nUser input is saved to an internal message history database for global access. Parameter Extraction: The LLM extracts parameters from input and handles unsupported messages. Memory Query: Relevant data is fetched from schedules and vector databases, formatted into XML for the LLM to decide the next action and extract parameters. Action Execution: Database APIs update schedules and vector stores based on the LLMâ€™s decision. Response Generation: Update logs are formatted into XML prompts, and the LLM generates a user-friendly response. Schedule Storage Implementation # Vector Schedule Database # Schedule Table # Parameter Extraction Implementation # Memory Query Implementation # Action Decision Implementation # Database Interface Implementation # Response Generation Implementation # Testing Analysis # ","date":"27 January 2025","externalUrl":null,"permalink":"/en/blog/schedule-management-report/","section":"Blogs","summary":"\u003cp\u003eTODO_UPDATE\n\u003cdiv class=\"lead text-neutral-500 dark:text-neutral-400 !mb-9 text-xl\"\u003e\n  Analysis Report on the Current State of Schedule Management Software Functionality\n\u003c/div\u003e\n\u003c/p\u003e\n\n\n\u003ch2 class=\"relative group\"\u003ePreface \n    \u003cdiv id=\"preface\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#preface\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h2\u003e\n\u003cp\u003eThe term \u0026ldquo;schedule management\u0026rdquo; frequently appears in our daily lives, yet it remains elusive: What exactly is schedule management? Why do we need it? What purpose does it serve? Who engages in schedule management, and how effective is it?\u003c/p\u003e","title":"Schedule Management Report","type":"blog"},{"content":" Summary of CUMCM 2024 competition experience, focusing on the A problem and the collaborative approach with MATLAB code improvement Process Overview # graph LR; id1(\"Task Allocation\")--\u003eid2(\"VS Code Collaboration\")--\u003eid3(\"MATLAB Code Execution\"); Task Allocation # There are two key points:\nTasks are divided into two parts: a main part and a secondary part. Tasks should be independent of each other, meaning no dependencies between them. Code Collaboration # Software Tools # VS Code Editor, along with the Live Server plugin, MATLAB plugin MATLAB. Naming Conventions # Main function should be uniformly named main: The main function is the one that directly computes the final result, while others should be called auxiliary functions. Data processing code: This refers to code that does not return any value but generates data tables. It should start with data, e.g., converting solar altitude angle \\(\\phi\\) to cosine value, dataCosPhi. Internal data conversion code: This should start with to, e.g., converting coordinates from a natural coordinate system to a Cartesian coordinate system StoXY. Plotting code: Code related to plotting graphs should start with fig. Testing code: Should start with test. For other special types of functions, they should be named based on their functionality. Functions that return boolean values should start with is, such as a collision detection function isCollided; functions that return other data should start with get. File Documentation Comments # Except for the common types of files mentioned in the naming conventions (data, test, fig, main), all other files should include documentation comments.\nThe documentation should be concise yet clear. It generally includes a brief description of the function, the input parameters, and the output parameters.\nIf you\u0026rsquo;re unsure how to write documentation, you can use AI assistance.\nInternal Variable Naming # Fixed Parameters # All fixed parameters should be placed in a config.m file located in the root directory for centralized management. Each parameter should be followed by a comment explaining its purpose, for example:\nlearningRate = 0.001; % Learning rate batchSize = 32; % Batch size numEpochs = 100; % Number of epochs To use this configuration file in other code files, simply include the following line:\nrun(\u0026#39;config.m\u0026#39;); Even the parameters are transformed into another file, the VS Code can also recognizes it and provide complement suggestion. Function-Internal Parameters # All variables used within a function should be explicitly defined at the beginning of the function and should include brief Chinese comments.\nIf similar parameters are used across multiple files, make sure to use consistent naming, especially in AI-generated code. Use F2 in VS Code to rename them. Code Formatting # Code formatting is mainly handled using the official MATLAB plugin. After you activate the plugin, press Shift+Alt+F to format your code.\nGit Version Control # A GitHub repository needs to be created to store the entire project files. Although the Live Server plugin enables faster real-time code collaboration, it is still necessary to commit and save at key development milestones to maintain the ability to roll back code.\nAll Git version control operations are performed on the lead developer\u0026rsquo;s computer, while other supporting developers use the Live Server plugin for more immediate code collaboration.\nLocal commit to save small improvements push operations are done in major increments Code Execution # Thanks to the latest function of the plugin MATLAB, we can now directly run and debug in the VS Code IDE. Though there are some limitations, it deserves a standing ovation.\nOther # AI Instruction Set # Since generative AI\u0026rsquo;s code style might differ from the project\u0026rsquo;s standards, if you need to generate an entire file, please include the following code standard instructions at the beginning of your request:\nYou are a mature and standardized MATLAB programmer. While correctly implementing the user\u0026#39;s goal, you should follow these code standards: 1. Clear and concise file documentation comments: The documentation should include a brief description of the function, input parameters, and output parameters. 2. Function-internal variable names should be concise and easy to read. 3. All variables used within a function should be explicitly defined at the beginning of the function, with brief Chinese comments added next to them. User\u0026#39;s instruction: Example Project # In order to develop a project as fast and well-defined as possible, I have created an Example Project which contains all the regulations mentioned before. You can directly change the files in the example project without mnemonic cost. ğŸ˜„\nCode Tips # Parallel Execution\nMATLAB supports multi-threaded computing. You can convert a typical for loop to a parfor loop for parallel execution. The execution of parfor functions is subject to strict requirements. Please refer to the official documentation for more details. Huge Table Process # The comment output of the Mathematical Model is a Huge table in .mat file, which is usually hard to abstract the target data.\nHere I directly write a simply Python program, Data extractor, to automatically operate the huge table data. With tiny modification, you can get any data you want from the .mat file.\nUtilizing the online LaTeX table editor, we can get the capacity to quickly insert the table data into your thesis.\n","date":"16 January 2025","externalUrl":null,"permalink":"/en/blog/code-collaboration-scheme/","section":"Blogs","summary":"\u003cp\u003e\n\n\u003cdiv class=\"lead text-neutral-500 dark:text-neutral-400 !mb-9 text-xl\"\u003e\n  Summary of CUMCM 2024 competition experience, focusing on the A problem and the collaborative approach with MATLAB code improvement\n\u003c/div\u003e\n\u003c/p\u003e\n\n\n\u003ch2 class=\"relative group\"\u003eProcess Overview \n    \u003cdiv id=\"process-overview\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#process-overview\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h2\u003e\n\u003cdiv class=\"mermaid\" align=\"center\"\u003e\n  \u003cpre\u003e\ngraph LR;\nid1(\"Task Allocation\")--\u003eid2(\"VS Code Collaboration\")--\u003eid3(\"MATLAB Code Execution\");\n\u003c/pre\u003e\n\u003c/div\u003e\n\n\n\n\u003ch2 class=\"relative group\"\u003eTask Allocation \n    \u003cdiv id=\"task-allocation\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#task-allocation\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h2\u003e\n\u003cp\u003eThere are two key points:\u003c/p\u003e","title":"Code Collaboration Scheme","type":"blog"},{"content":" MySQL Installation and Deployment Process + Quick Syntax CookBook + Study Notes Information Source # SQL Tutorial - Liao Xuefeng\u0026rsquo;s Official Website\nThis is an extremely user-friendly MySQL tutorial website with an embedded web-based database, making it easy for beginners to get a hands-on understanding of MySQL operations. It also provides concise yet essential explanations of the background of SQL. MySQL Installation # The simplest way to install MySQL is through Docker Desktop. It takes only two steps to complete:\nRun the following command in your terminal:\ndocker pull mysql Run MySQL With Shell # Then initialize and run the SQL container:\ndocker run -d --name mysql -p 3306:3306 -e MYSQL_ROOT_PASSWORD=password -v /Users/liaoxuefeng/mysql-data:/var/lib/mysql mysql Parameter Explanation:\nParameter Description -d Run the container in the background --name Assign a name to the container (if not specified, Docker will choose one automatically) -p 3306:3306 Map the container\u0026rsquo;s port 3306 to the local machine, allowing you to connect to MySQL through port 3306 -e MYSQL_ROOT_PASSWORD=password Set the root password for MySQL (in this case, the password is password). If not set, Docker will generate a password, which youâ€™ll need to check the logs to retrieve. Itâ€™s recommended to set a password. -v /path:/var/lib/mysql Mount a local directory to /var/lib/mysql in the container, which will store MySQL data. Replace /path with the actual directory path on your machine mysql Specifies the name of the Docker image you want to run When using Docker to run MySQL, you can always delete the MySQL container and rerun it. If you delete the locally mounted directory, rerunning the container is equivalent to starting with a fresh MySQL instance. Run MySQL With docker-compose # Create docker-compose.ymlï¼š\nservices: mysql: image: mysql ports: - \u0026#34;3306:3306\u0026#34; environment: MYSQL_ROOT_PASSWORD: password volumes: - ../data:/var/lib/mysql restart: unless-stopped Run docker-compose build up -d to start service.\nMySQL Basic Syntax # Querying # Comment Syntax:\n-- This is a comment Basic Query Syntax:\nSELECT * FROM \u0026lt;table_name\u0026gt;; Conditional Query:\nSELECT * FROM \u0026lt;table_name\u0026gt; WHERE \u0026lt;condition\u0026gt;; In the condition expression, you can use various logical operators such as: AND, NOT, OR.\nProjection Query:\nSELECT \u0026lt;column1\u0026gt;, \u0026lt;column2\u0026gt;, \u0026lt;column3\u0026gt; FROM \u0026lt;table_name\u0026gt;; SELECT \u0026lt;column1\u0026gt; alias1, \u0026lt;column2\u0026gt; alias2, \u0026lt;column3\u0026gt; alias3 FROM \u0026lt;table_name\u0026gt;; Sorting:\n-- Sort by score in ascending order: SELECT id, name, gender, score FROM students ORDER BY score; -- Sort by score in descending order: SELECT id, name, gender, score FROM students ORDER BY score DESC; -- Sort by score, gender: SELECT id, name, gender, score FROM students ORDER BY score DESC, gender; -- Sorting with a WHERE condition: SELECT id, name, gender, score FROM students WHERE class_id = 1 ORDER BY score DESC; Pagination Query:\n-- Query the first page: SELECT id, name, gender, score FROM students ORDER BY score DESC LIMIT 3 OFFSET 0; -- Start from the 0th record, fetch up to 3 records, which may be less than 3. Aggregation Query, using MySQLâ€™s aggregation functions:\n-- Count the total number of records: SELECT COUNT(*) FROM students; -- Set the alias for the count result: SELECT COUNT(*) num FROM students; -- Conditional aggregation: SELECT COUNT(*) boys FROM students WHERE gender = \u0026#39;M\u0026#39;; Even though COUNT(*) returns a scalar value, the result is still a two-dimensional table, but with only one row and one column. Other commonly used aggregation functions: MAX(), MIN(), AVG(), SUM(), etc., similar to COUNT().\nGrouped Aggregation Query:\n-- Group by class_id and count records, similar to a for loop: SELECT COUNT(*) num FROM students GROUP BY class_id; -- Include class_id in the result: SELECT class_id, COUNT(*) num FROM students GROUP BY class_id; -- Group by multiple fields, e.g., class_id and gender: SELECT class_id, gender, COUNT(*) num FROM students GROUP BY class_id, gender; Multi-table Query (Cartesian Product):\n-- Querying from students and classes: SELECT * FROM students, classes; -- Set aliases and distinguish columns with table names: SELECT students.id sid, students.name, students.gender, students.score, classes.id cid, classes.name cname FROM students, classes; -- Using aliases for both tables to make it cleaner: SELECT s.id sid, s.name, s.gender, s.score, c.id cid, c.name cname FROM students s, classes c; The result of a multi-table query is still a two-dimensional table, but this table is organized using a Cartesian product, which is why itâ€™s also known as a Cartesian Query.\nJoin Queries, unlike multi-table queries where the tables are combined using Cartesian products, join queries select one table as the main table and combine it with an auxiliary table based on a relationship:\nInner Join (using INNER): -- Select all students and their corresponding class names: SELECT s.id, s.name, s.class_id, c.name class_name, s.gender, s.score FROM students s INNER JOIN classes c ON s.class_id = c.id; Outer Join, with three variations: RIGHT, LEFT, FULL: -- Using RIGHT OUTER JOIN: SELECT s.id, s.name, s.class_id, c.name class_name, s.gender, s.score FROM students s RIGHT OUTER JOIN classes c ON s.class_id = c.id; Understanding method: You can think of it as sets, where tableA is the main table (also known as the left table) and tableB is the related table (the right table).\n-- Here tableA is the main table, also known as the left table, and tableB is the related table. SELECT ... FROM tableA ??? JOIN tableB ON tableA.column1 = tableB.column2; Modifying Data # Inserting Data: -- Insert a new record: INSERT INTO students (class_id, name, gender, score) VALUES (2, \u0026#39;Daniu\u0026#39;, \u0026#39;M\u0026#39;, 80); -- Insert multiple records at once: INSERT INTO students (class_id, name, gender, score) VALUES (1, \u0026#39;Dabao\u0026#39;, \u0026#39;M\u0026#39;, 87), (2, \u0026#39;Erbao\u0026#39;, \u0026#39;M\u0026#39;, 81), (3, \u0026#39;Sanbao\u0026#39;, \u0026#39;M\u0026#39;, 83); Updating Data: -- Update the record with id=1: UPDATE students SET name=\u0026#39;Daniu\u0026#39;, score=66 WHERE id=1; -- Update records with score \u0026lt; 80: UPDATE students SET score=score+10 WHERE score\u0026lt;80; -- Update record with id=999, but no matching records will be found: UPDATE students SET score=100 WHERE id=999; -- Without a WHERE clause, the update will affect all records in the table: UPDATE students SET score=60; Deleting Data: -- Delete the record with id=1: DELETE FROM students WHERE id=1; -- Deleting without a WHERE clause will remove all records from the table: DELETE FROM students; Here\u0026rsquo;s a fluent and natural English translation:\nCreating Database and Tables # Creating the Database: CREATE DATABASE your_db_name -- name of the database CHARACTER SET utf8mb4 -- character set of the database COLLATE utf8mb4_unicode_ci; -- collation rule Creating Tables: CREATE TABLE table_name ( column1 datatype constraints, column2 datatype constraints, column3 datatype constraints, ... PRIMARY KEY (primary_key_column) ); -- Example of a user table CREATE TABLE users ( id INT AUTO_INCREMENT, username VARCHAR(50) NOT NULL UNIQUE, email VARCHAR(100) NOT NULL UNIQUE, password VARCHAR(255) NOT NULL, created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP, PRIMARY KEY (id) ); Creating Triggersï¼š CREATE TRIGGER trigger_name BEFORE INSERT ON orders FOR EACH ROW BEGIN SET NEW.order_time = NOW(); END; ","date":"15 January 2025","externalUrl":null,"permalink":"/en/blog/mysql-basics/","section":"Blogs","summary":"\u003cdiv class=\"lead text-neutral-500 dark:text-neutral-400 !mb-9 text-xl\"\u003e\n  MySQL Installation and Deployment Process + Quick Syntax CookBook + Study Notes\n\u003c/div\u003e\n\n\n\n\u003ch3 class=\"relative group\"\u003eInformation Source \n    \u003cdiv id=\"information-source\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#information-source\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://liaoxuefeng.com/books/sql/introduction/index.html\" target=\"_blank\"\u003eSQL Tutorial - Liao Xuefeng\u0026rsquo;s Official Website\u003c/a\u003e\u003cbr\u003e\nThis is an extremely user-friendly MySQL tutorial website with an embedded web-based database, making it easy for beginners to get a hands-on understanding of MySQL operations. It also provides concise yet essential explanations of the background of SQL.\u003c/li\u003e\n\u003c/ul\u003e\n\n\n\u003ch3 class=\"relative group\"\u003eMySQL Installation \n    \u003cdiv id=\"mysql-installation\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#mysql-installation\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h3\u003e\n\u003cp\u003eThe simplest way to install MySQL is through Docker Desktop. It takes only two steps to complete:\u003c/p\u003e","title":"MySQL Basics","type":"blog"},{"content":" Familiarizing oneself with the project analysis process, this is a practical analysis report. After I had completed my small plugin, I realized that there doesn\u0026rsquo;t seem to be a one-click solution in the note-to-blog field. So, I wrote this article to analyze whether it would make sense to create a new project to fill this gap.\nThere is no such thing as a \u0026ldquo;better\u0026rdquo; or \u0026ldquo;worse\u0026rdquo; project; there is only whether it is suitable or not. The evaluation criteria in this article are based on whether they meet the requirements of a blog website. Therefore, some projects may not be suitable for creating a blog site, but this does not diminish their value.\nField Definition # Before we begin the analysis, it\u0026rsquo;s important to have a clear understanding of the note-to-blog field.\nNotes: In this context, \u0026ldquo;notes\u0026rdquo; specifically refers to the notes in Obsidian, which use Obsidian Flavored Markdown. This adds certain complexities to the process of converting notes to a webpage.\nBlog: A blog, by definition, is a means to gain traffic. Creators spend time writing notes, and then using conversion software, generate beautifully formatted, feature-rich blog websites.\nEvaluation criteria are as follows:\nPrivacy:\nDoes it run locally? Is it open-source? Usability:\nHow well does it adapt to Obsidian\u0026rsquo;s syntax? How complex is the service deployment? How detailed is the documentation? How easy is it to customize settings? Web Functionality:\nDoes the default web template include all essential functions (search, day/night mode, etc.)? How visually appealing is the default webpage? Does it support SEO? How smoothly does it convert Obsidian\u0026rsquo;s native syntax (for example, are there untranslatable code blocks in internal links, or does it discard some Obsidian syntax features)? Project Overview # In my vision, this project would be an Obsidian plugin that seamlessly exports Obsidian notes into Hugo blog webpages, supporting all of Obsidian\u0026rsquo;s basic core features.\nResult: It greatly reduces the cost of creating a blog webpage, allowing anyone who can use Obsidian to have their own blog.\nMarket and User Feasibility Analysis # Market Demand Analysis # Overview # Basic Needs: The demand to build a personal website and continuously produce content, including for self-improvement, self-expression, and creating a unique and comprehensive personal skill showcase (for corporate recruitment), etc.\nTarget User Group: Heavy Obsidian users who want to share notes; knowledge creators who want to build a personal blog but have abandoned the idea due to technical difficulty.\nRelevant Data # Flowershow: As of October 2024, the plugin had been downloaded 3,355 times; by January 2025, this number had risen to 4,594, while the most downloaded plugin had reached 3,211,992 downloads. Quartz: As of January 2025, Quartz had accumulated 7.7k stars on GitHub. Existing Solutions # Quartz (Hugo) # Recommendation: â¤ï¸â€ğŸ”¥â¤ï¸â€ğŸ”¥â¤ï¸â€ğŸ”¥â¤ï¸â€ğŸ”¥â¤ï¸â€ğŸ”¥\nIntroduction # Quartz is a toolset that converts Obsidian notes into web pages. The latest version, v4, has undergone a complete rewrite compared to v3, removing its dependency on Hugo and optimizing the user customization experience. The v4 version is now primarily built with TypeScript, and the original Hugo templates have been replaced with JSX.\nAs a result, Quartz in its current form is almost entirely disconnected from Hugo. However, much of the information available in Internet still advertises Quartz as being built on top of Hugo.\nOfficial example website: Welcome to Quartz 4\nReview # Pros:\nExtremely complete functionality The only toolset that successfully handles display wiki links Detailed documentation Cons:\nVirtually no drawbacks, but one notable limitation is the lack of Chinese documentation. Summary: An excellent project, where the styles displayed in Obsidian are the same as those displayed on the webpage. It has garnered the most stars on GitHub among all available solutions.\nFlowershow # Recommendation: â¤ï¸â€ğŸ”¥â¤ï¸â€ğŸ”¥â¤ï¸â€ğŸ”¥â¤ï¸â€ğŸ”¥ğŸ©¶\nIntroduction # Flowershow is an overall publishing service based on Obsidian, which can convert your Obsidian notes into an online digital garden website with directory structure. Vercel is a cloud service for front-end deployment, enabling serverless front-end deployment via GitHub. Each content submission triggers automatic deployment. For domestic users, Netlify is an alternative.\nSubjectively, the development team behind Flowershow is very passionate and mission-driven. Their core philosophies are detailed in their About page.\nObjectively, Flowershow\u0026rsquo;s positioning as a blog webpage generation platform based on Obsidian is spot-on, and the final result is very good both from a front-end and back-end perspective.\nReview # Pros:\nClear positioning and a straightforward workflow Comprehensive feature support Professional team behind maintenance and operations Highly customizable, suitable for creators who enjoy personalizing their setup Cons (as of January 2025):\nSome Obsidian features are not handled, such as display wiki links. At least this section is omitted in the documentation. A reverse link feature is mentioned on the homepage, but itâ€™s unclear in the siteâ€™s details. Summary: Overall, the project is well done, but some details still need improvement. This solution is suitable for creators who don\u0026rsquo;t require high support for Obsidian syntax.\nOfficial Publish # Recommendation: â¤ï¸â€ğŸ”¥â¤ï¸â€ğŸ”¥â¤ï¸â€ğŸ”¥ğŸ©¶ğŸ©¶\nIntroduction # Examples of websites using Obsidian\u0026rsquo;s official publishing service:\nObsidian Chinese Tutorial - A Chinese tutorial website, using the official Obsidian publishing service. Digital 3D Garden - Deep front-end customizations. Mister Chad - A simple, neat site with rich content. Discrete Structures for Computer Science - Official simple style. Review # Pros:\nThe official publish service offers top-notch support for Obsidianâ€™s internal representations, ensuring all Obsidian features are correctly displayed on the webpage. Continuous maintenance ensures quick adaptation to updates from Obsidian. Highly customizable settings for users with coding experience, and a wide range of themes from other developers. Privacy settings, password protection, and access control for internal document management. SEO support and mobile platform adaptation for greater potential traffic. Cons:\nCosts $8 per month. Since personal websites typically have very little traffic initially, this can become a significant expense over time. This is the major drawback of the official service. If you stop paying, the website becomes inaccessible. Limited support in certain regions, with traffic constraints in China. Summary: The official service is suitable for users with sufficient funds and moderate customization needs.\nDigital Garden # Recommendation: â¤ï¸â€ğŸ”¥â¤ï¸â€ğŸ”¥â¤ï¸â€ğŸ”¥ğŸ©¶ğŸ©¶\nIntroduction # Digital Garden is an Obsidian plugin that exports notes as webpages and hosts them on GitHub, with deployment via Vercel or Netlify.\nExample sites:\nDigital Garden - Official example Aaron Youn - Created by an individual user John\u0026rsquo;s Digital Galaxy - Rich content showcasing Digital Gardenâ€™s features, including display links. Review # Pros:\nComprehensive feature support Supports Obsidian theme migration Cons:\nNot friendly with Chinese paths Web interface customization requires direct handling of source code (HTML, JavaScript, CSS), and the default interface is not very visually appealing. Summary: The workflow is simple, and the feature support is extensive. However, the interface requires effort to improve, and creators who care less about aesthetics can jump straight into using it.\nPerlite # Recommendation: â¤ï¸â€ğŸ”¥â¤ï¸â€ğŸ”¥ğŸ©¶ğŸ©¶ğŸ©¶\nIntroduction # Perlite is an open-source alternative to Obsidian\u0026rsquo;s official publishing service, providing a browser\n-based file reader with an interface nearly identical to Obsidianâ€™s.\nReview # Pros:\nSupports almost all Obsidian features. Classic native interface, offering a familiar experience for users. Cons:\nIt is not a blog page but a \u0026ldquo;file reader\u0026rdquo; instead. Requires Docker, which can result in slower startup times compared to the simplicity of a plugin experience. Summary: Perlite is best suited for those who need a browser-based Obsidian experience, rather than as a public-facing blog.\nJekyll + Netlify + GitHub Pages # Recommendation: â¤ï¸â€ğŸ”¥â¤ï¸â€ğŸ”¥ğŸ©¶ğŸ©¶ğŸ©¶\nIntroduction # This method is derived from obsidian\u0026rsquo;s most perfect free publishing solution.\nExample website by the author: oldwinterâ€™s Digital Garden\nReview # Pros:\nSimple configuration Highly customizable Cons:\nDoes not support certain Obsidian features like callout syntax No dark mode support No search functionality Summary: A good solution for converting Obsidian to a blog, but missing some core features, making it unsuitable for creators seeking a complete web experience.\nTiddlyWiki # Recommendation: â¤ï¸â€ğŸ”¥ğŸ©¶ğŸ©¶ğŸ©¶ğŸ©¶\nIntroduction # TiddlyWiki is an older note-taking framework that remains very active today, with developers continuously enhancing it.\nReview # Pros:\nExtremely simple and lightweight Widely used with a strong user base Domestic services available with no need for VPNs Cons:\nThe simplicity might result in a somewhat primitive interface. Not a full-fledged personal blog; lacks SEO and is difficult to access via search engines, limiting traffic potential. Summary: TiddlyWiki is ideal for personal note storage but not for creators seeking a blog that attracts traffic.\nConclusion # Before conducting a thorough analysis, I was unaware of the actual landscape in the note-to-blog field, which led me to consider creating a simplified plugin. ğŸ’¡\nHowever, after systematic research, I must admit that Quartz stands out as the best project in this space. Whether it\u0026rsquo;s the adaptation to Obsidian\u0026rsquo;s syntax, ease of configuration, front-end aesthetics, customization options, or backend blog creation, there is very little room for improvement.\nThus, there is no need for me to initiate a project to duplicate whatâ€™s already been done. I salute all the teams involved in the note-to-blog field, whether mentioned in this article or not. ğŸ«¡\nThere are no \u0026ldquo;better\u0026rdquo; or \u0026ldquo;worse\u0026rdquo; projects, only those that are suitable or not. The evaluation criteria in this article focus on whether the solution meets the requirements for a blog webpage, and thus, some projects may not be ideal for blogging but still offer great value.\nSaluting open-source pioneers! ğŸ«¡ğŸ«¡ğŸ«¡\n","date":"10 January 2025","externalUrl":null,"permalink":"/en/blog/note-to-blog-report/","section":"Blogs","summary":"\u003cdiv class=\"lead text-neutral-500 dark:text-neutral-400 !mb-9 text-xl\"\u003e\n  Familiarizing oneself with the project analysis process, this is a practical analysis report.\n\u003c/div\u003e\n\n\u003cp\u003eAfter I had completed my \u003ca href=\"https://morethan987.github.io/en/blog/plugin-writing-experience/\"\u003esmall plugin\u003c/a\u003e, I realized that there doesn\u0026rsquo;t seem to be a one-click solution in the \u003cstrong\u003enote-to-blog\u003c/strong\u003e field. So, I wrote this article to analyze whether it would make sense to create a new project to fill this gap.\u003c/p\u003e","title":"Analysis Report on the Note-to-Blog Project","type":"blog"},{"content":" A blog website journey, from hand-coding to Hugo, a story of twists and turns. Why Hugo? # It all started with hearing that Hugo could generate webpages and that it was incredibly efficient at compiling static pages. I decided to dive into the researchâ€”Hugo is said to be the fastest static site generator in the world, as claimed on its official website.\nOf course, words are just words, so hereâ€™s the output I got when I compiled and ran Hugo locally without the public folder at the beginning:\nZH-CN EN Pages 53 51 Paginator pages 0 0 Non-page files 13 13 Static files 7 7 Processed images 3 0 Aliases 18 17 Cleaned 0 0 Built in 872 ms\nIn total, compiling 104 pages (both Chinese and English) took just 0.872 seconds, including the time to build the local server. That speed is hard to criticize. And the local server can listen for changes to the source code in real time and do incremental refactoring, depending on the size of the change, usually around 0.03 seconds.\nI haven\u0026rsquo;t used other page generators for setting up blogs, so I can\u0026rsquo;t compare Hugo\u0026rsquo;s speed with others. References # Here are all the resources I used during the blog setup process:\nè±ç‰¹é›·-letere This is a bloggerâ€™s site also built with Hugo. It contains a lot of tutorials on other web tools as well. The series is also available on Bilibili video tutorial. Blowfish This is the Hugo theme I used, and the documentation is excellent. Iâ€™ve never seen such a patient author. Official Hugo Website Hugo Themes Full Deployment Process # Setting Up Hugo # This part is covered in detail in the webpage and video tutorial by the blogger. If you donâ€™t like reading text, you can follow the video tutorial. ğŸ˜\nHonestly, setting up Hugo is one of the easiest setups I\u0026rsquo;ve ever seen, no exaggeration. You simply download Hugo from the official website, place it in a folder, and unzip it. Youâ€™ll find just one file, hugo.exeâ€”itâ€™s that simple.\nHugo is really convenient. I tried Hexo before, but the Node.js setup turned me away. Even now, I have no idea why it failed to compile. ğŸ˜¢ The only slight difficulty is adding the directory containing hugo.exe to your environment variables.\nCreating a Template System # Open the terminal in the folder where hugo.exe is located and run the command hugo new site your-site-name. Youâ€™ll see a new folder appear in the current directory.\nThe template system sounds advanced, but itâ€™s just a special folder structure created in the same directory as hugo.exe. You canâ€™t arbitrarily modify its contents because each folder has a specific purpose.\nName Purpose asset Stores images, icons, and other assets used by the website config Website configuration folder (may not exist initially; some themes require it) hugo.toml One of the website configuration files content All content goes here public The folder containing the fully compiled website (empty initially) themes Stores your websiteâ€™s themes Basic Theme Configuration # Hugo has a lot of themes, and you can browse them on Hugo Themes. You can download the theme you like and place it in the themes folder. The process might sound abstract, but you can check out the video tutorial for more guidance.\nHereâ€™s one important tip: most themes come with a sample site located in the exampleSite folder. If you donâ€™t want to configure everything from scratch, you can just use the sample configuration.\nAfter configuring the theme, youâ€™ll need to customize it. I highly recommend the Blowfish theme, which is fantastic, and I truly respect the author ğŸ«¡.\nBlowfish Theme # The Blowfish official documentation is incredibly detailed, so I wonâ€™t repeat it here. Any additional words would be disrespectful to such a comprehensive guide ğŸ«¡.\nHowever, there are some issues you might encounter, and Iâ€™ll briefly mention them below. You should carefully read the official documentation to fully understand these points ğŸ¤”.\nWhy does the \u0026ldquo;Recent Articles\u0026rdquo; section still show up even if params.homepage.showRecent = false is set? If you face this issue, itâ€™s likely because, like me, you lazily used the exampleSite configuration. This is because the homepage layout is controlled by more than one interface, and another one is located in the layouts\\partials\\home\\custom.html file.\nIf you donâ€™t mind, just ignore it. But if you care (like I did ğŸ¤ª), you can comment out the following code in the file:\n\u0026lt;section\u0026gt; {{ partial \u0026#34;recent-articles-demo.html\u0026#34; . }} \u0026lt;/section\u0026gt; Why doesnâ€™t the logo change between day and night modes when I use an svg logo? This is a bug I discovered, and Iâ€™ve already submitted an improvement to the theme author. See code improvement or SVG support\nWhy does the small icon in the browser window still show the blowfish logo even after I change the siteâ€™s logo? This is mentioned in the official documentation, but itâ€™s buried deep. You can find it under Partials Documentation for Blowfish.\nTo be honest, the official documentation is excellent. ğŸ‘ After going through the entire process, I only encountered a few minor issues that were not easy to understand ğŸ˜‹.\nPlugins I Use # I prefer using Obsidian for writing articles. However, the format used by Obsidian and the one used by Blowfish is quite different, so converting between the two can be a hassle ğŸ¤”.\nAfter searching around, I found that there werenâ€™t any suitable plugins! So, I developed my own plugin: Hugo-Blowfish-Exporter.\nWhile the plugin is simple, it covers almost all of my needs, including:\n- Callouts (supports all official callout names, with additional icons) - Inline math formulas (Blowfish supports block-level formulas)\n- Mermaid (supports Mermaid diagrams)\n- Image embedding (automatically exports images)\n- Wiki-style links (only support the none-displayed link ğŸ˜¢)\nThe none-displayed link is simply exported as normal hyperlink in the HTML file;\nThe displayed link is more complex: change(override) the source code of Blowfish to support the file injection through the shortcode, mdimporter; every Obsidian file should includes a meta data slug to tag the folder that contains the target markdown file in your website repository.\nThe overriding of the theme\u0026rsquo;s source code can be found in the mdimporter and the stripFrontMatter used to remove metadata from the injected files\u0026rsquo; headers. For overriding the directory, refer to the configuration on GitHub.\nI put a lot of effort into this plugin, even though it only took a few days ğŸ¤”. But those few days were quite exhausting ğŸ˜µâ€ğŸ’«.\nIf this plugin helps you, feel free to share it! If youâ€™re not happy with the functionality, you can submit an issue on GitHub ğŸ«¡. If you\u0026rsquo;re familiar with the code, you can modify it directly; the code is well-commented and quite standard ğŸ¤—.\nAnd if you modify and upgrade the code, Iâ€™d be very grateful if you share your changes with me (via a pull request on GitHub)! â˜ºï¸\nFinal Thoughts # Setting up a blog site is just the first step in a long journey; the real challenge is filling it with content.\nAs I mentioned in An experience of writing plugins, many personal blogs fade into obscurity in as little as a year, from the initial burst of excitement to the eventual silence.\nIn this fast-paced world, most meaningless and inefficient things are eventually replaced by efficiency, and the original enthusiasm and dreams often compromise with reality. I too no longer have the passion I once had, and my actions have become more like those of a real adult.\nBut there\u0026rsquo;s still a bit of unwillingness in me. This website is a form of resistance, and Iâ€™ll do my best to maintain it. Thatâ€™s also why I developed the pluginâ€”to make updating the blog easier.\nI hope this tutorial helps anyone planning to set up their own blog. Letâ€™s keep moving forward, together ğŸ«¡.\n","date":"7 January 2025","externalUrl":null,"permalink":"/en/blog/hugo-blog/","section":"Blogs","summary":"\u003cdiv class=\"lead text-neutral-500 dark:text-neutral-400 !mb-9 text-xl\"\u003e\n  A blog website journey, from hand-coding to Hugo, a story of twists and turns.\n\u003c/div\u003e\n\n\n\n\u003ch2 class=\"relative group\"\u003eWhy Hugo? \n    \u003cdiv id=\"why-hugo\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#why-hugo\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h2\u003e\n\u003cp\u003eIt all started with hearing that Hugo could generate webpages and that it was incredibly efficient at compiling static pages. I decided to dive into the researchâ€”Hugo is said to be the \u003cstrong\u003efastest static site generator\u003c/strong\u003e in the world, as claimed on its official website.\u003c/p\u003e","title":"Hugo Blog Setup","type":"blog"},{"content":" A Reflection on Writing a Plugin and What I Learned from the Experience. The Beginning # It all started with my blog website. I stumbled upon an article on WeChat about building a blog with Hugo, and since I wanted to revamp my old, simple site, I decided to give it a try. My original site was extremely rudimentary, and the whole writing process involved jumping between HTML, JS, and CSS in a rather awkward manner. On top of that, I had always admired the blog of a great tech guru, Lilian Weng, which was also built with Hugo. This further strengthened my resolve to change my site\u0026rsquo;s underlying platform.\nSo, I quickly started diving into Hugo.\nTo my surprise, the results were extraordinary! My old webpage took me nearly a month to build, but with Hugo, I was able to finish everything in less than half a day. What shocked me even more was that Hugo, a program written in Go, didnâ€™t require users to set up a Go environment! ğŸ˜®\nAt the same time, I discovered an incredibly well-documented Hugo themeâ€”Blowfish. This was by far the most detailed documentation I had ever seen for any project, bar none (à¹‘â€¢Ì€ã…‚â€¢Ì)Ùˆâœ§.\nWith Hugo and Blowfish working in tandem, my small site quickly took shape. Of course, Iâ€™m not great at designing, so I just used the default layout from Blowfish, as I felt any changes would ruin the beauty of the page.\nTo be honest, after all this work, I didnâ€™t have any strong emotional reactions, except for deep respect for the coding skills of the authors of Hugo and Blowfish.\nThat was until I wanted to upload the massive amount of notes I had in Obsidian to my new blog.\nThe Bitter Taste of Originality # I soon realized that there wasnâ€™t a plugin available to directly convert the format of my Obsidian notes to fit the Blowfish theme. Fueled by the earlier \u0026ldquo;pleasant experience,\u0026rdquo; I decided to write a plugin myself! (ğŸ˜„ Although, I would soon stop laughing ğŸ˜¢)\nThe rest of the experience wasnâ€™t anything particularly excitingâ€”just endless switching between webpages, searching through API documentation, and never-ending conversations with AI bots. After countless revisions, I finally ended up with something exceedingly simple: a plugin that identifies specific patterns in documents and performs content replacement.\nIt was quite laughable. Compared to the few hours it took to set up the website, the nearly forty hours I spent writing that plugin felt almost negligible. At one point, I seriously considered just deleting my few hundred lines of code.\nYes, such a simple plugin drained me mentally and physically. I truly tasted the bitterness of originality.\nNow, looking back at Hugo and Blowfish, I feel deeply shocked by their complexity and the effort required to implement all of those features. If they were getting paid for this work, I could at least understand the level of effort involved. But they were both open-source, relying entirely on user goodwill and appreciation.\nI saw the last update of the Blowfish authorâ€™s blog, which was in March 2024, and I fell into deep thought.\nSentiments and Idealism # I imagine that the author of Blowfish must have paused the development of the theme for some reasonâ€”perhaps due to life circumstances. After all, this project didnâ€™t bring in much real income.\nSuddenly, I remembered the changes I had noticed beforeâ€”those GitHub profiles, once full of green squares, gradually becoming sparse, and eventually disappearing. Beneath this peaceful change, there might be a shift in someone\u0026rsquo;s life. Whether it\u0026rsquo;s because of busy work or the gradual fading of motivation, the original passionate drive eventually drowns in silence. I can\u0026rsquo;t stop this from happening, but I understand the reasons behind it.\nOpen-source is driven by passion, but passion doesnâ€™t pay the bills. People need to live in the present.\nI recalled a tech YouTuber, Ma Nong Gao Tian, a core Python developer who humorously complained about the harsh realities of open-source life. His prematurely graying hair made me feel a pang of empathyâ€”he had spent most of his life writing code and yet found himself out of work, surviving on a few extra bucks from his videos.\nIn Conclusion # Life is rarely as we wish. Once again, I looked at my forty-plus hours of work and couldnâ€™t help but laugh and shake my head.\nAfter writing this, Iâ€™m off to bed. Itâ€™s now 1:48 AM on January 6, 2025, and I still havenâ€™t reviewed for my English final exam tomorrow.\nLooking at this blog again, I laughed and shook my head.\nSuch is life.\n","date":"6 January 2025","externalUrl":null,"permalink":"/en/blog/plugin-writing-experience/","section":"Blogs","summary":"\u003cdiv class=\"lead text-neutral-500 dark:text-neutral-400 !mb-9 text-xl\"\u003e\n  A Reflection on Writing a Plugin and What I Learned from the Experience.\n\u003c/div\u003e\n\n\n\n\u003ch3 class=\"relative group\"\u003eThe Beginning \n    \u003cdiv id=\"the-beginning\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#the-beginning\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h3\u003e\n\u003cp\u003eIt all started with my blog website. I stumbled upon an article on WeChat about building a blog with \u003ccode\u003eHugo\u003c/code\u003e, and since I wanted to revamp my old, simple site, I decided to give it a try. My original site was extremely rudimentary, and the whole writing process involved jumping between \u003ccode\u003eHTML\u003c/code\u003e, \u003ccode\u003eJS\u003c/code\u003e, and \u003ccode\u003eCSS\u003c/code\u003e in a rather awkward manner. On top of that, I had always admired the \u003ca href=\"https://lilianweng.github.io/\" target=\"_blank\"\u003eblog\u003c/a\u003e of a great tech guru, \u003ccode\u003eLilian Weng\u003c/code\u003e, which was also built with \u003ccode\u003eHugo\u003c/code\u003e. This further strengthened my resolve to change my site\u0026rsquo;s underlying platform.\u003c/p\u003e","title":"An experience of writing plugin","type":"blog"},{"content":" Moravec\u0026rsquo;s Paradox # High-level reasoning requires relatively little computational power, whereas low-level perception and motor skills demand enormous computational resources.\nThis phenomenon implies that computers and robots find it relatively easy to handle complex logic and mathematical problems but struggle with basic perceptual and motor tasks such as walking, grasping, and visual recognition.\nAnalysis # Low-level perception and motor skills feel subjectively simple but are computationally complex from a computational theory perspective.\nHigh-level tasks: The term itself is highly subjectiveâ€”\u0026ldquo;high-level\u0026rdquo; essentially equates to \u0026ldquo;difficult for humans.\u0026rdquo; However, these so-called difficult tasks are mostly polynomial-time solvable, meaning that, by their very nature, high-level tasks do not involve inherently complex problems.\nLow-level tasks: The problems they encompass are mostly NP-hard or even PSPACE-hard, indicating that these problems are fundamentally complex.\nHumans effortlessly solve low-level tasks due to evolutionary optimization over time: it took eons for humans to develop the ability to efficiently and accurately perform low-level tasks, whereas acquiring high-level reasoning abilities took comparatively little time.\nThis paradoxical phenomenon seems to stem from the generalizability of knowledge. Some knowledge is highly compressible, while other forms are not. A more precise concept here is computational reducibility.\nA simple example: exams. Logically, all exam questions are derived from basic knowledge through reasoning, right? In theory, exams should be entirely solvable through logical deduction. Yet, anyone who has taken an exam knows that the process from reading a question to forming a solution is not particularly \u0026ldquo;logical.\u0026rdquo; In fact, it often feels like thereâ€™s no clear techniqueâ€”just an intuitive sense of how to approach the problem.\nTwo Types of Knowledge # Knowledge that can be derived from existing information through finite symbolic logic expressions. Its characteristics include precision, high generalizability, well-defined problem boundaries, and the ability to clearly specify given conditions, methods, and outcomes. Under explicit conditions, it can accurately predict results (with no room for \u0026ldquo;probability\u0026rdquo; within the problem\u0026rsquo;s scope).\nFor problems where outcomes cannot be derived from given conditions through precise logical reasoning, we rely on statistical experimentation to extract valuable patterns. These patterns are characterized by extensive trial-and-error, poorly defined problem boundaries, and difficulty in obtaining necessary conditions. Despite these challenges, they forcibly correlate conditions with outcomes, producing rules that are volatile, uncertain, and locally correct.\nProportion of the Two Types # Clearly generalizable knowledge is far less abundant than non-generalizable knowledge. In a sense, generalizable knowledge is a special case of non-generalizable knowledge.\nThe Trend of Non-Generalizable Knowledge Transforming into Generalizable Knowledge # The nature of non-generalizable knowledge determines the difficulty of acquiring it (massive experimentation consumes vast energy, an unavoidable step). Operating non-generalizable knowledge is highly energy-intensive (ungeneralizable knowledge requires significant resources to maintain). Moreover, non-generalizable knowledge struggles to transcend the boundaries of individual human lifespans (it tends to vanish with the individual, as it cannot be easily recorded or transmittedâ€”though machine intelligence may differ fundamentally here).\nHuman individuals have limited energy and cannot rely solely on non-generalizable knowledge to interact with the world. Thus, there is a tendency for non-generalizable knowledge to transform into generalizable knowledge. Although this process is arduous and energy-intensive for individuals (being itself a form of non-generalizable knowledge), it saves enormous energy at the collective human level.\nAcceptable Power Input Determines the Upper Limit of Intelligence # Here, we might propose another criterion for classifying intelligence levels: the power input an individual can accept. The higher the level, the more non-generalizable knowledge an individual can master. Since energy is always finite (humans may be limited to Earth\u0026rsquo;s resources, whereas machine intelligence could harness stellar-scale energy), there will always be some degree of knowledge generalization. However, due to the inherent nature of non-generalizable knowledge, the generalizable knowledge of higher-level intelligence may remain non-generalizable to lower-level intelligence. The generalizability of knowledge is relative.\nFrom this perspective, acceptable power input is crucial for an intelligent system. Alternatively, we might artificially limit the power consumption of machine intelligence to observe the transformation of non-generalizable knowledge into generalizable knowledge.\n","date":"3 January 2025","externalUrl":null,"permalink":"/en/blog/moravecs-paradox/","section":"Blogs","summary":"\u003ch2 class=\"relative group\"\u003eMoravec\u0026rsquo;s Paradox \n    \u003cdiv id=\"moravecs-paradox\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#moravecs-paradox\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003cp\u003eHigh-level reasoning requires relatively little computational power, whereas low-level perception and motor skills demand enormous computational resources.\u003c/p\u003e","title":"Reflections on Moravec's Paradox","type":"blog"},{"content":" Preface # This article is primarily a review and summary of the entire process of CUMCM 2024.\nOur team was formed in the winter of 2023, and CUMCM 2024 was our first participation in the \u0026ldquo;Mathematical Modeling\u0026rdquo; competition. After numerous mock contests, we finally made it to the national competition. After submitting the final paper, we won the first prize at the provincial level and were recommended for the first prize at the national level, ultimately receiving the second prize at the national level.\nThere were moments of excitement and surprise, as well as disappointment; we must have done some things right in the competition, which is why we won a national award in our first attempt; but there are definitely shortcomings, after all, there must be a reason for going from \u0026ldquo;recommended for the first prize at the national level\u0026rdquo; to \u0026ldquo;second prize at the national level\u0026rdquo;.\nIn short, this experience is truly unforgettable, and it is even more worth summarizing and learning from the experience to prepare for next year\u0026rsquo;s competition.\nCUMCM stands for Chinese Undergraduate Mathematical Contest in Modeling; it is commonly referred to as the \u0026ldquo;National Mathematical Modeling Competition\u0026rdquo;. Terminology Explanation # Term Explanation Computational System The traditional modeling process, encapsulating a large function Optimization System A system used to optimize the adjustable parameters in the computational system to generate the best parameter configuration Computational Flow The process of handling input data in the computational system Computational Flow Node A key intermediate step in the workflow Optimization Flow The main logic of the optimization system Main Body of the Paper Includes the abstract, restatement, descriptions of computational and optimization flows, results presentation and analysis, that is, all content before the conclusion of the paper Conclusion of the Paper Includes sensitivity analysis and model extension Objective Conditions # Task Division # Although there were many topics to choose from for the competition, our group chose to focus on optimization problems, which is Topic A.\nMe: Modeling + Coding + Part of Paper Writing CL: Modeling + Paper Writing + Part of Coding HWJ: Paper Beautification Workflow # The coding part of the entire Topic A can be roughly divided into two systems:\nComputational System: Function: Accept input data and parameters, return the required results Nature: Directly determined by the problem, different topics have different computational systems, which need to be constructed temporarily Optimization System: Function: Accept the computational system as the target function to be optimized, execute its own optimization logic, and finally return the computational results Nature: The method system is relatively mature and can be prepared in advance of the competition with various optimization systems The paper writing part is divided into:\nOverall Framework: Determined by the LaTeX template Main Content Filling: Clear description of the workflow and optimization flow Typesetting and Beautification: Adjust the details of each part, with illustrative images (flowcharts, schematics) Concluding Content Pre-Modeling # Objective: Under the premise of accurately understanding the problem, quickly carry out preliminary modeling, basically determine the direction of modeling and calculation methods;\nEstimated Time: 3 hours\nWork: All team members conduct a web search to see if there are any literature materials that basically hit the topic.\nHit Successful: The most ideal situation, at this time, you can directly study the papers and collect ideas; Hit Unsuccessful: Although there are no ready-made materials for reference, some ideas have been accumulated in the process of literature review. Early Modeling # Overall Objective: Construct a precise and optimization method adaptable computational system\nModeling: Clarify the operations between input data and each computational flow node Coding: Implement the computational flow with code and achieve data visualization Paper: Fill in the content of the first question and initially typeset Estimated Time: 30 hours\nWork:\nAll team members model together, first clarify the modeling ideas, and provide a complete mathematical derivation process Me and CL: Code implementation and paper content filling are carried out simultaneously HWJ: Draw more vivid schematic diagrams that cannot be generated by code Mid-Modeling # Overall Objective: Construct a suitable optimization system\nModeling: According to the particularity of the computational system, choose the most matching optimization system Coding: Make minor changes in the implementation of the optimization system to match the computational system Paper: Complete the main part of the paper and start local detail fine-tuning Estimated Time: 20 hours\nWork: Similar to the previous, but the focus of work has shifted from code writing to paper writing\nSimplify the paper, at this time, the paper is very bloated Fine-tune the logic of the paper to make the context more closely related Beautify the typesetting, reduce text, increase images Late Modeling # The basic modeling is completed, and all members check for loopholes: Conventional checks such as typos, inaccurate expressions, formula spelling errors, etc. Optimize code comments to make them more readable Focus on checking personal information Personal information must not be retained in the competition paper, including file paths in the code, such as C:\\Users\\Morethan; retaining personal information is a very serious mistake! Actual Combat Effectiveness # When we applied the above strategies to the actual combat process, that is, the formal competition of CUMCM 2024, the results were as follows:\nEffective Time: The total duration of the competition is three days, a total of 72 hours The team works from seven in the morning to eight in the evening, excluding meal times, with an effective time of 12 hours a day Time utilization rate is \\(50\\%\\) (quite low in comparisonğŸ¤”) Completed Work: The main body of the paper is 28 A4 pages The code part is 35 A4 pages, excluding the reused code between each sub-question, there should be about 20 pages A total of 25 illustrations in the paper The above data is after the paper has been streamlined, with the initial draft of the paper being nearly 50 pages Uncompleted Work: The final result calculation, due to the large amount of calculation (the code efficiency is not high), the code was finished two hours in advance, but there was not enough time to calculate the resultsğŸ˜­ğŸ˜­ The calculation accuracy of the model is not enough, the accuracy is 1s which does not meet the standard answer\u0026rsquo;s precision The conclusion part of the paper was not actually completed Strengths # Topic Selection # Focused on Topic A, accumulated sufficient experience in mock contests, and polished a set of efficient workflows\nThe methodology for Topic A is relatively well-constructed\nWorkflow # The workflow is relatively clear, and the efficiency is high\nGuided by the final paper, modeling, paper, and code are carried out simultaneously, ensuring sufficient content in the paper\nDivision of Labor # Adopted a blurred division of labor, each team member has a main job and a secondary job, can work independently on their main job, and can also complete some work on the secondary job, greatly improving time utilization\nThe team members are very capable, as handling two division tasks means more learning costs\nWeaknesses # Workflow # The plan is perfect, but some necessary links were not well done in practice\nEffective time ratio: finishing work at eight in the evening is too early! More time should be taken to model trial and error to ensure the correctness and accuracy of the model\nDivision of Labor # The code writing, code debugging, code visualization, result calculation, and result visualization involve too much code, which is difficult for one person to handle;\nTask overlap caused by blurred division of labor increases collaboration costs\nModeling # Topic understanding accuracy: This time, there was a significant deviation in our understanding of the topic, which led to wasting a lot of time on model correction; Code # Code efficiency: Due to no time limit before, there was insufficient preparation for \u0026ldquo;very long\u0026rdquo; code, no experience with code parallelism;\nResult precision: The initial modeling was too rough, and a bad characteristic was used: setting the time step to 1, and using it as an array index, which made it difficult to reduce the time step later, resulting in insufficient precision of the final results\nImprovement Plans # Carefully select the venue, increasing effective timeâœ¨is the most importantâœ¨ Division of Labor # Slightly change the division of labor, increase the investment of human resources in coding\nIncrease learning input in each main and secondary division to increase work efficiency\nModeling # Focus more on understanding the topic, don\u0026rsquo;t rush; correcting modeling errors is not worth the loss Code # Build a set of effective code collaboration plans to enhance code writing speed\nStart building code writing standards:\nVariable naming Documentation at the beginning of the file Code writing process standards Code parallelization: Add some parallelizable code to the code to increase running speed\nAll code improvements must be implemented in a document! Not just slogans! The final output: Code Collaboration Scheme Paper # Study excellent papers\nPay attention to its paper framework Pay attention to its language style, text readability, detail, illustration logic, and image readability Improve ourselves\nOptimize the paper\u0026rsquo;s main logic framework, refine the content of each section Improvements in language style, text readability, detail, illustration logic, and image readability, etc. The results are fixed in the form of comments in the LaTeX template! Summary # A test paper without full marks is more rewarding than one with full marks!\nAccumulating knowledge of applied mathematics, enhancing paper writing skills, and improving the ability to discover problems are more meaningful than the competition itself. ğŸ«¡\nCUMCM, every MathModeler can benefit from it. ğŸ¤—\n","date":"12 September 2024","externalUrl":null,"permalink":"/en/blog/cumcm2024/","section":"Blogs","summary":"\u003ch2 class=\"relative group\"\u003ePreface \n    \u003cdiv id=\"preface\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#preface\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h2\u003e\n\u003cp\u003eThis article is primarily a review and summary of the entire process of CUMCM 2024.\u003c/p\u003e","title":"CUMCM 2024 Summary","type":"blog"},{"content":" Creating Virtual Environments # Standard Python Operations # Creation # Here are some common code examples ğŸ‘‡\n# Create a virtual environment python -m venv your_env_name # Create a virtual environment with a specific Python version (if Python is installed in the default location) python -m venv your_env_name --python=python3.11 # If Python is installed in a custom location D:\\Python\\Python311\\python.exe -m venv your_env_name Below are some optional parameters for creating customized virtual environments:\nParameter Description --system-site-packages Includes packages from the global Python environment to avoid redundant installations --clear Clears the target directory if it already exists before creating the virtual environment --version Confirms the Python version in the virtual environment All parameter descriptions can be obtained by running python -m venv -hâ€”no need to search elsewhere for documentation~ğŸ˜† Activation # By default, the virtual environment is inactive. In the \u0026ldquo;your_env_name/Scripts/\u0026rdquo; directory, there will be a file named \u0026ldquo;activate,\u0026rdquo; which can be executed via the command line.\n# Activate the virtual environment your_env_name/Scripts/activate Poetry # Poetry is a Python package management tool. According to its changelog, it has been in development since February 2018, making it not a new tool but one that has gained significant popularity in recent years ğŸ’«.\nIts advertised highlights include:\nMore comprehensive third-party dependency analysis Automated virtual environment creation More intuitive commands ğŸ¤” Installation # Straightforward: pip install poetry for a global installation.\nGlobal Configuration # Poetry has some unique mechanisms that may differ from tools like pip. You might need to configure certain settings as follows:\n# List all configuration items poetry config --list # By default, Poetry creates virtual environments in a dedicated folder rather than the project directory # Use the following command to create the environment folder in the project directory poetry config virtualenvs.in-project true # Poetry can automatically create virtual environments when none exist in the project # For reassurance, you can run this command poetry config virtualenvs.create true # Switch to a domestic mirror poetry config repositories.tencentyun https://mirrors.tencentyun.com/pypi/simple # Other settings can generally be left as defaults Initializing a Project # If you\u0026rsquo;re starting a new project from scratch, you\u0026rsquo;ll need to: create a project folder, navigate to it in the command line, and simply run:\npoetry init An interactive command-line interface will guide you through creating a new project, generating a pyproject.toml file to record dependencies, and creating Poetry\u0026rsquo;s signature poetry.lock file to lock package versions. Those familiar with Node will find this familiar ğŸ˜.\nIf you\u0026rsquo;re using someone else\u0026rsquo;s project, you\u0026rsquo;ll typically only find the pyproject.toml file, which is compatible across different package management tools. Some authors may also include the poetry.lock file. In such cases, you can choose to: 1) delete the lock file and use another package manager, or 2) keep the lock file and use Poetry to manage dependencies.\nIf you need a simple directory structure, you can use:\npoetry new my-package This will automatically create a directory structure suitable for most projects:\nmy-package â”œâ”€â”€ pyproject.toml â”œâ”€â”€ README.md â”œâ”€â”€ my_package â”‚ â””â”€â”€ __init__.py â””â”€â”€ tests â””â”€â”€ __init__.py For more complex needs: new Command.\nCreating a Virtual Environment # Here are a few scenarios to consider.\nYou\u0026rsquo;re building your project from scratch: poetry env use python # Remember how you installed Poetry? This uses the default Python version. poetry env use python3.7 # Uses Python 3.7 from your system. poetry env use 3.7 # For the lazy. poetry env use /full/path/to/python # If the shorthand doesn\u0026#39;t work, use this. You\u0026rsquo;re using someone else\u0026rsquo;s project: There are two very similar commands, and the comments below highlight their differences. The official recommendation is the sync command, as it avoids installing packages not tracked by poetry.lock, which might have been accidentally added by the original developer.\nHowever, in practice, install is often more reliable, as sync can sometimes produce strange errors, like removing itself and crashing the command line ğŸ¤”.\npoetry install # Automatically creates a virtual environment and installs all dependencies listed in pyproject.toml. # Ensure virtualenvs.create is set to true. # Automatically creates a virtual environment and installs all dependencies listed in poetry.lock, removing any extra packages from pyproject.toml. poetry sync Then, enter the virtual environment with:\npoetry shell Adding Packages # Poetry allows developers to distinguish between production and development dependencies.\n# Defaults to the production group. poetry add your-package # Adds to the development group. poetry add your-package -D Updating Packages # poetry update # Automatically analyzes dependencies and updates all possible packages. poetry update your-package1 your-package2 # Updates only the listed packages. Removing Packages # This is one of Poetry\u0026rsquo;s standout features: pip installs a package and its dependencies but only removes the specified package, leaving its dependencies behind.\nPoetry, however, can safely and completely remove a package and its dependencies without affecting other packages.\n# Removes a production dependency. poetry remove your-package # Removes a development dependency. poetry remove your-package -D Displaying Dependencies # poetry show --tree # Displays the dependency tree from pyproject.toml. poetry show your-package --tree # Displays the dependency tree for a specific package. UV # A package management tool written in Rust, with a command structure very similar to poetry.\nInstallation # First, configure environment variables:\ncd ~ # Ensure you\u0026#39;re in the default directory. ls -a # Check if the .bashrc file exists. vim .bashrc # Edit the file with vim. # Append these three lines to the end of the file. # Domestic mirror for the installer. export UV_INSTALLER_GHE_BASE_URL=https://ghproxy.cn/https://github.com # Domestic mirror for Python installation. export UV_PYTHON_INSTALL_MIRROR=https://ghproxy.cn/https://github.com/indygreg/python-build-standalone/releases/download # Domestic mirror for Python packages. export UV_DEFAULT_INDEX=https://mirrors.tuna.tsinghua.edu.cn/pypi/web/simple Installation via curl (recommended): # Replace \u0026#34;/custom/path\u0026#34; with your desired installation location. curl -LsSf https://astral.sh/uv/install.sh | env UV_INSTALL_DIR=\u0026#34;/custom/path\u0026#34; sh Installation via pip/pipx (recommended): pipx install uv # Install uv using pipx. pip install uv # Install uv using pip. Usage # Usage is very similar to poetry.\nConda # I personally don\u0026rsquo;t use Conda much for environment management, but since many lab servers default to using Conda, Iâ€™ll document the basics here for reference.\nUsage # The following commands are fundamental for checking environment information. Theyâ€™re typically used when accessing a new server environment or troubleshooting environment-related issues.\n########## Checking Environment Information ########## arch # View the system architecture conda --version # Get the Conda version python --version # Get the Python version conda env list # List all available environments conda activate xxx # Activate the virtual environment named \u0026#34;xxx\u0026#34; conda deactivate # Exit the current virtual environment whereis python # Check the location of Python in the currently activated environment nvidia-smi # Check CUDA status (not a Conda command but frequently used) conda config --show channels # View Conda\u0026#39;s download sources conda remove -n xxx # Delete the virtual environment named \u0026#34;xxx\u0026#34; conda create -n env_name # Create a virtual environment with the specified name, using the default Python version Adding domestic (Chinese) download sources:\nconda config --show channels # View Conda\u0026#39;s download sources # Add Tsinghua\u0026#39;s mirror source conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/msys2 Managing Python packages:\n# Install a specific package conda install package_name # Remove a specific package conda remove package_name # Update a specific package conda update package_name # Search for a specific package conda search package_name # List all installed packages and their versions conda list # Clean unused packages and cache to save disk space conda clean --all # Install packages from a requirements.txt file conda install -f requirements.txt Manual installation of dependency packages for handling extreme offline situations:\n# Directly use pip download command to download all related whl files pip download scikit-image --dest ./scikit_image_files --only-binary :all: --python-version 3.13 --platform manylinux_2_17_x86_64 --implementation cp --abi cp313 # Manually transfer to the server, switch to the target folder, and run (note the whl files) # --find-links specifies the folder containing the whl files pip install scikit_image-0.25.2-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl --no-index --find-links=./ Do not rename the whl files ğŸ˜¢ Passwordless Login via VSCode # For students without a local GPU, editing and running files directly on a server is very convenient. The VSCode extension remote-ssh can also display a VSCode interface directly on the server. To avoid the hassle of repeatedly entering passwords, you can set up passwordless login as follows ğŸ˜„.\nFor a local Windows machine, SSH-related configuration files are usually stored in the C:\\Users\\\u0026lt;User_name\u0026gt;\\.ssh folder. Passwordless login can be achieved by modifying files in this directory.\nConfigure the config file. A sample configuration is as follows: # Replace \u0026lt;User_name\u0026gt; with your local machine\u0026#39;s username Host 3090 HostName xx.xxx.xx.xx User morethan IdentityFile \u0026#34;C:\\Users\\\u0026lt;User_name\u0026gt;\\.ssh\\id_rsa\u0026#34; Generate the authentication file id_rsa.pub.\nLocally create an authorized_keys file and copy the contents of id_rsa.pub into it.\nOn the server, create a .ssh folder in the default directory and copy the authorized_keys file from your local machine into it.\nNow, when logging into the server via VSCode, youâ€™ll find that passwordless login is enabled.\nThe steps above only need to be performed once initially. When configuring passwordless login for another server, you only need to modify the config file and copy the authorized_keys file to the serverâ€”no further file modifications are required ğŸ˜„. Packaging Applications # We often need to share Python programs we\u0026rsquo;ve written. However, sharing only the source code can be frustrating for users unfamiliar with coding, as running the source requires setting up a local environment. Hence, packaging tools come into play.\npyinstaller # Installation is straightforward, just like any other Python package: pip install pyinstaller. To use it, navigate to the directory containing the target file in the terminal and run the command.\nFeatures: Fast packaging speed; larger resulting executable size.\nCommon commands:\n# Package main.py into a standalone executable; disable the console window when running. pyinstaller -F -w main.py # Package main.py into a project folder; enable the console window when running. pyinstaller -D main.py Parameter Description -h, --help Show help message and exit. -v, --version Show program version and exit. -F, --onefile Package everything into a single executable. -D, --onedir Package everything into a directory (default). -w, --windowed, --noconsole Disable the console window (Windows only). -c, --console, --nowindowed Run the program with a console window (default, Windows only). -a, --ascii Exclude Unicode character set support (included by default). -d, --debug Generate a debug version of the executable. -n NAME, --name=NAME Specify the name of the generated executable or directory (default: script name). -o DIR, --out=DIR Specify the directory for the spec file (default: current directory). -p DIR, --path=DIR Set the Python import path (similar to PYTHONPATH). -i \u0026lt;FILE\u0026gt;, --icon \u0026lt;FILE\u0026gt; Set the executable\u0026rsquo;s icon (supports .ico or .icns formats). --distpath DIR Specify the output directory for the executable (default: ./dist). --workpath WORKPATH Specify the directory for temporary files (default: ./build). --add-data \u0026lt;SRC;DEST or SRC:DEST\u0026gt; Add additional data files or directories (Windows uses semicolons; Linux/OSX uses colons). --add-binary \u0026lt;SRC;DEST or SRC:DEST\u0026gt; Add additional binary files. --hidden-import MODULENAME Add modules not automatically detected. --exclude-module EXCLUDES Exclude specified modules. --clean Clean PyInstaller cache and temporary files. --log-level LEVEL Set verbosity for console messages (options: TRACE, DEBUG, INFO, WARN, ERROR, FATAL). Nuitka # Converts Python code into an exe executable by first translating Python to C and then compiling the C code.\nFeatures: Very slow packaging speed; requires a C compiler (though installation can be automated, it may not suit users with strict memory constraints); resulting executable is very small (about one-tenth the size of pyinstaller\u0026rsquo;s output).\nInstallation command:\npip install -U nuitka Common usage commands:\n# Package main.py into a single exe file with link-time optimization and clean up temporary files afterward. python -m nuitka --lto=yes --remove-output --onefile main.py Parameter Description --standalone Creates a standalone executable with all dependencies. --onefile Packages everything into a single .exe file. --optimize=N Sets optimization level (0, 1, or 2); higher numbers mean more optimization. --lto Enables Link Time Optimization (options: no, yes, or thin). --enable-plugin=\u0026lt;plugin_name\u0026gt; Enables specified plugins (e.g., tk-inter, numpy, anti-bloat). --output-dir=\u0026lt;dir\u0026gt; Specifies the output directory for compilation. --remove-output Deletes intermediate .c files and other temporary files after compilation. --nofollow-imports Does not recursively process any imported modules. --include-package=\u0026lt;package_name\u0026gt; Explicitly includes an entire package and its submodules. --include-module=\u0026lt;module_name\u0026gt; Explicitly includes a single module. --follow-import-to=\u0026lt;module/package\u0026gt; Specifies modules or packages to process recursively. --nofollow-import-to=\u0026lt;module/package\u0026gt; Specifies modules or packages to exclude from recursive processing. --include-data-files=\u0026lt;source\u0026gt;=\u0026lt;dest\u0026gt; Includes specified data files. --include-data-dir=\u0026lt;directory\u0026gt; Includes all data files in a directory. --noinclude-data-files=\u0026lt;pattern\u0026gt; Excludes data files matching a pattern. --windows-icon-from-ico=\u0026lt;path\u0026gt; Sets the icon for the Windows executable. --company-name, --product-name, --file-version, --product-version, --file-description Sets properties for the Windows executable. References # Poetry-related:\nPoetry Beginner\u0026rsquo;s Guide_Using Poetry-CSDN Blog Very detailed resource. Poetry How to Change Poetry\u0026rsquo;s Domestic Mirror - Data Science SourceResearch UV-related:\nInstalling and Using Python Project and Package Manager UV - Deep Sea Xiao Tao One of the few blogs that includes the domestic mirror address for uv python install. Conda-relatedï¼š\nSummary of Commonly Used Conda Commands - Zhihu ","date":"10 August 2024","externalUrl":null,"permalink":"/en/blog/pytips/","section":"Blogs","summary":"\u003ch2 class=\"relative group\"\u003eCreating Virtual Environments \n    \u003cdiv id=\"creating-virtual-environments\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#creating-virtual-environments\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h2\u003e\n\n\n\u003ch3 class=\"relative group\"\u003eStandard Python Operations \n    \u003cdiv id=\"standard-python-operations\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#standard-python-operations\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h3\u003e\n\n\n\u003ch4 class=\"relative group\"\u003eCreation \n    \u003cdiv id=\"creation\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#creation\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h4\u003e\n\u003cp\u003eHere are some common code examples ğŸ‘‡\u003c/p\u003e","title":"Python Tips and Tricks","type":"blog"},{"content":" Reference # Honestly, I\u0026rsquo;m not familiar with BayesianOPT, the opinions mentioned stem from the below. ğŸ‘‡\nã€æœºå™¨å­¦ä¹ ã€‘ä¸€æ–‡çœ‹æ‡‚è´å¶æ–¯ä¼˜åŒ–/Bayesian Optimization\nä¸€æ–‡è¯¦è§£è´å¶æ–¯ä¼˜åŒ–ï¼ˆBayesian Optimizationï¼‰åŸç†\nè´å¶æ–¯ä¼˜åŒ–(BayesianOptimization)\nè¶…å‚æ•°ä¼˜\u0026mdash;è´å¶æ–¯ä¼˜åŒ–åŠå…¶æ”¹è¿›ï¼ˆPBTä¼˜åŒ–ï¼‰\nè´å¶æ–¯ä¼˜åŒ– (Bayesian Optimization)\nMATLAB Offical Document\nExploring Bayesian Optimization\nAdvantages \u0026amp; Algorithm Principle # Here we are going to talk about the advantages \u0026amp; algorithm principle of BayesianOPT. If you only want to konw how to use it, you can read the #Advantage section, then go to the MATLAB Practice\nAdvantages # Algorithm Principle # MATLAB Practice # Well, we can put Bayesian Optimization into practice even though we don\u0026rsquo;t understand it, using the predefined function of MATLAB, the bayesopt. Here is the official guidance of the function: bayesopt\nFinal code display # % define the obj function function y = objectiveFcn(x) y = (1 - x.x1)^2 + 100 * (x.x2 - x.x1^2)^2; end % define the variables vars = [optimizableVariable(\u0026#39;x1\u0026#39;, [-2, 2]) optimizableVariable(\u0026#39;x2\u0026#39;, [-2, 2])]; % conduce the optimizer results = bayesopt(@objectiveFcn, vars, ... \u0026#39;AcquisitionFunctionName\u0026#39;, \u0026#39;expected-improvement-plus\u0026#39;, ... \u0026#39;MaxObjectiveEvaluations\u0026#39;, 30, ... \u0026#39;IsObjectiveDeterministic\u0026#39;, true, ... \u0026#39;Verbose\u0026#39;, 1); % get result bestPoint = results.XAtMinObjective; bestObjective = results.MinObjective; % result output fprintf(\u0026#39;æœ€ä¼˜è§£ x1: %.4f, x2: %.4f\\n\u0026#39;, bestPoint.x1, bestPoint.x2); fprintf(\u0026#39;æœ€ä¼˜ç›®æ ‡å€¼: %.4f\\n\u0026#39;, bestObjective); I\u0026rsquo;d commit that the code is generated by AI. ğŸ¥² AI is a better coder, at least when comparing with me. ğŸ«  Parameters Explaination # Params Meaning AcquisitionFunctionName select a Acquisition Function, which determines the method how bayesopt choose the next acquisition point MaxObjectiveEvaluations the maximize evalu turns IsObjectiveDeterministic If the obj function contains noise, set to true ; Otherwise, set to false Verbose Determine the detailing extend of console output, the complete output includes many figures. Want more detailed information? Refer to the Offical document: bayesopt. It\u0026rsquo;s more completed and with amount of examples.\nIt\u0026rsquo;s basic for every MathModeler to read the offical document. ğŸ˜ ","date":"5 August 2024","externalUrl":null,"permalink":"/en/blog/bayesianopt/","section":"Blogs","summary":"\u003ch2 class=\"relative group\"\u003eReference \n    \u003cdiv id=\"reference\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#reference\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h2\u003e\n\u003cp\u003eHonestly, I\u0026rsquo;m not familiar with BayesianOPT, the opinions mentioned stem from the below. ğŸ‘‡\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca href=\"https://blog.csdn.net/qq_27590277/article/details/115451660\" target=\"_blank\"\u003eã€æœºå™¨å­¦ä¹ ã€‘ä¸€æ–‡çœ‹æ‡‚è´å¶æ–¯ä¼˜åŒ–/Bayesian Optimization\u003c/a\u003e\u003c/p\u003e","title":"Bayesian Optimization","type":"blog"},{"content":" Background Overview # You should know how to interact with the computer via the command line, including but not limited to: how to open the command line/terminal in Windows, when a running command ends, etc.\nA little knowledge of bypassing internet censorship is helpful. OverLeaf is foreign software, and its related LaTeX projects are also hosted abroad. Therefore, directly accessing foreign traffic when downloading dependencies can save a lot of trouble. If you donâ€™t have a VPN, you will need to specify a domestic source for each package manager, though sometimes the updates from domestic sources are not timely.\nBasic familiarity with Vim operations is useful, such as: how to enter insert mode, how to save and exit, how to exit without saving, etc.\nFull Deployment Process # Installing Linux # Search for a Linux distribution in the Windows App Store and download it. The author chose Kali. After installation, you can open it directly from the Start menu. Upon opening, a command-line window will pop up, and you will need to register with a username and password.\nAt this point, your command line should display a warning. This is because you havenâ€™t installed WSL (Windows Subsystem for Linux). Also, when entering the password, your input will not be displayed in the command line, but it has been recorded. Why do you need a Linux system? Because OverLeaf\u0026rsquo;s ShareLaTeX model requires a Linux environment. It is said that OverLeaf runs more smoothly on Linux systems.\nInstalling WSL # To install WSL2, run the following in the Windows command line:\nwsl --install After installation, you can open it directly. Another warning will appear. At this point, you need to create a text file in the C:\\Users\\ASUS directory and rename it to .wslconfig.\nEnter the following content:\n[experimental] autoMemoryReclaim=gradual # gradual | dropcache | disabled networkingMode=mirrored dnsTunneling=true firewall=true autoProxy=true Installing Docker # Go to the Docker website to download Docker, which will be the container for the ShareLaTeX model. Docker is an open-source application container engine that includes images, containers, and repositories. Its purpose is to manage the lifecycle of application components, such as encapsulation, distribution, deployment, and operation, allowing users to \u0026ldquo;package once, run anywhere,\u0026rdquo; much like a container, developed and encapsulated by programmers, which users can directly move around.\nOnce Docker is installed, you can double-click to start it in the background. We will interact with Docker later via the command line.\nPulling the Image # Open Kali, and run the following command:\ngit clone https://github.com/overleaf/toolkit.git ./overleaf-toolkit Then run:\ncd ./overleaf-toolkit bin/init vim ./config/variables.env At this point, you should be in the document interface of the Vim text editor. Vim has many shortcuts, and pressing the \u0026quot;I\u0026quot; key will enter insert mode for text editing. Press \u0026quot;esc\u0026quot; to return to normal mode. In insert mode, type: OVERLEAF_SITE_LANGUAGE=zh-CN.\nAfter typing, press \u0026quot;esc\u0026quot; to return to normal mode, then type :wq to \u0026ldquo;save and quit.\u0026rdquo; If you make a mistake, type :e! to discard all changes and start over. This step will set your OverLeaf interface to Chinese.\nAfter successfully saving and quitting, return to the familiar Kali command-line interface and run bin/up. This will pull the ShareLaTeX image and related network tools. There will be a large amount of data transfer, so ensure that your network is stable (your VPN should be reliable!).\nConfiguring the User # Once the previous command finishes, run bin/start. At this point, open Docker and enter the ShareLaTeX container. You should see code \u0026ldquo;flashing.\u0026rdquo; If there are no red messages, everything is running normally.\nNow open a browser and visit http://localhost/launchpad.\nAfter registering an Administrator Account, you will be redirected to http://localhost/project. The basic OverLeaf webpage should now be displayed.\nIf you compile now, it will most likely report an error á••( á› )á•—. This is because ShareLaTeX is missing many required packagesğŸ™ƒ Installing Extension Packages # Open Kali, navigate to the appropriate directory, and run bin/shell. Then execute the following one by one:\ncd /usr/local/texlive # Download and run the upgrade script wget http://mirror.ctan.org/systems/texlive/tlnet/update-tlmgr-latest.sh sh update-tlmgr-latest.sh -- --upgrade # Change the TeX Live download source tlmgr option repository https://mirrors.sustech.edu.cn/CTAN/systems/texlive/tlnet/ # Upgrade tlmgr tlmgr update --self --all # Install the full TeX Live package (this will take time, so donâ€™t let the shell disconnect) tlmgr install scheme-full # Exit the ShareLaTeX command-line interface exit # Restart the ShareLaTeX container docker restart sharelatex After restarting, enter the shell again and run:\napt update # Install fonts apt install --no-install-recommends ttf-mscorefonts-installer fonts-noto texlive-fonts-recommended tex-gyre fonts-wqy-microhei fonts-wqy-zenhei fonts-noto-cjk fonts-noto-cjk-extra fonts-noto-color-emoji fonts-noto-extra fonts-noto-ui-core fonts-noto-ui-extra fonts-noto-unhinted fonts-texgyre # Install pygments apt install python3-pygments # Install Beamer and others apt install texlive-latex-recommended apt install texlive-latex-extra # Install English fonts echo \u0026#34;yes\u0026#34; | apt install -y --reinstall ttf-mscorefonts-installer # Install Chinese fonts apt install -y latex-cjk-all texlive-lang-chinese texlive-lang-english cp fonts/* /usr/share/fonts/zh-cn/ cd /usr/share/fonts fc-cache -fv # Update font cache fc-list :lang=zh-cn fc-match Arial Finally, in the shell directory, run:\nvim /usr/local/texlive/2023/texmf.cnf Open the configuration file and add shell_escape = t at the bottom.\nIâ€™m not sure what this does, but it was passed down by the predecessors ğŸ¤” Note, if the TeX Live version (the official name for extension packages) differs, the directory path may also change. You will need to adjust the path based on the actual version, for example, change 2023 to 2024.\nYou can use ls -l in the Linux command line to view all files in the current directory. Successful Deployment # Now you can happily use your local OverLeaf version without worrying about compilation timeouts~\nIf you\u0026rsquo;re lucky and happen to be a CQUer, hereâ€™s a graduation thesis template from Chongqing University, super user-friendly: CQUThesis\n","date":"12 July 2024","externalUrl":null,"permalink":"/en/blog/localoverleaf/","section":"Blogs","summary":"\u003ch2 class=\"relative group\"\u003eBackground Overview \n    \u003cdiv id=\"background-overview\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#background-overview\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eYou should know how to interact with the computer via the command line, including but not limited to: how to open the command line/terminal in Windows, when a running command ends, etc.\u003c/p\u003e","title":"Local OverLeaf Deployment","type":"blog"},{"content":"","date":"2025-05-14","externalUrl":null,"permalink":"/tags/%E6%9E%B6%E6%9E%84/","section":"æ ‡ç­¾","summary":"","title":"æ¶æ„","type":"tags"},{"content":"","date":"2025-05-14","externalUrl":null,"permalink":"/tags/%E7%AC%94%E8%AE%B0/","section":"æ ‡ç­¾","summary":"","title":"ç¬”è®°","type":"tags"},{"content":"","date":"2025-05-14","externalUrl":null,"permalink":"/tags/%E8%AE%BA%E6%96%87/","section":"æ ‡ç­¾","summary":"","title":"è®ºæ–‡","type":"tags"},{"content":"","date":"2025-05-14","externalUrl":null,"permalink":"/series/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/","section":"ç³»åˆ—","summary":"","title":"è®ºæ–‡ç¬”è®°","type":"series"},{"content":"","date":"4 May 2025","externalUrl":null,"permalink":"/en/series/ai-musings/","section":"Seires","summary":"","title":"AI Musings","type":"series"},{"content":"","date":"2025-05-04","externalUrl":null,"permalink":"/series/ai%E9%81%90%E6%83%B3/","section":"ç³»åˆ—","summary":"","title":"AIéæƒ³","type":"series"},{"content":"","date":"4 May 2025","externalUrl":null,"permalink":"/en/authors/","section":"Authors List","summary":"","title":"Authors List","type":"authors"},{"content":"","date":"4 May 2025","externalUrl":null,"permalink":"/en/tags/blog/","section":"Tags","summary":"","title":"Blog","type":"tags"},{"content":"","date":"4 May 2025","externalUrl":null,"permalink":"/en/blog/","section":"Blogs","summary":"","title":"Blogs","type":"blog"},{"content":"","date":"4 May 2025","externalUrl":null,"permalink":"/en/tags/llm/","section":"Tags","summary":"","title":"LLM","type":"tags"},{"content":" Hi, welcome to my info page. ğŸ‘‹ # Basic Info # My casual English name is Morethan because it resembles my Chinese name. ğŸ™ƒ\nNow I\u0026rsquo;m a university student in China. á••( á› )á•— Nothing more to say. ğŸ« \nBlog Focus # Personal Knowledge Base: to store frequently-used operations and valuable experience.\nMicro Paper Stack: to store inspirations for my Graduation Thesis, usually serious and logical, attempt to follow the standard thesis working stream.\nKnowledge Outlet: to put what I leant into practice.\nFinal # If you find the content is useful, click a like please at the beginning of that page. ğŸ¤—\nIf you want to share the content, cite this website please. ğŸ«¡\nIf you find some bug, push an issue on the GitHub please. ğŸ¥°\n","date":"4 May 2025","externalUrl":null,"permalink":"/en/authors/morethan/","section":"Authors List","summary":"\u003ch1 class=\"relative group\"\u003eHi, welcome to my info page. ğŸ‘‹ \n    \u003cdiv id=\"hi-welcome-to-my-info-page-\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#hi-welcome-to-my-info-page-\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h1\u003e\n\n\n\u003ch2 class=\"relative group\"\u003eBasic Info \n    \u003cdiv id=\"basic-info\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#basic-info\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h2\u003e\n\u003cp\u003eMy casual English name is Morethan because it resembles my Chinese name. ğŸ™ƒ\u003c/p\u003e","title":"Morethan","type":"authors"},{"content":" ","date":"4 May 2025","externalUrl":null,"permalink":"/en/series/","section":"Seires","summary":"\u003chr\u003e","title":"Seires","type":"series"},{"content":" ","date":"4 May 2025","externalUrl":null,"permalink":"/en/tags/","section":"Tags","summary":"\u003chr\u003e","title":"Tags","type":"tags"},{"content":"Here are notes and micro-thesises to reinforce knowledge through writing.\nHope my casual words can provide you something useful.ğŸ¤—\n","date":"4 May 2025","externalUrl":null,"permalink":"/en/","section":"Welcome to Morethan's website","summary":"\u003cp\u003eHere are notes and micro-thesises to reinforce knowledge through writing.\u003c/p\u003e\n\u003cp\u003eHope my casual words can provide you something useful.ğŸ¤—\u003c/p\u003e","title":"Welcome to Morethan's website","type":"page"},{"content":"","date":"2025-05-04","externalUrl":null,"permalink":"/tags/%E5%8D%9A%E5%AE%A2/","section":"æ ‡ç­¾","summary":"","title":"åšå®¢","type":"tags"},{"content":"","date":"1 May 2025","externalUrl":null,"permalink":"/en/tags/linux/","section":"Tags","summary":"","title":"Linux","type":"tags"},{"content":"","date":"1 May 2025","externalUrl":null,"permalink":"/en/series/technical-miscellany/","section":"Seires","summary":"","title":"Technical Miscellany","type":"series"},{"content":"","date":"1 May 2025","externalUrl":null,"permalink":"/en/tags/ubuntu/","section":"Tags","summary":"","title":"Ubuntu","type":"tags"},{"content":"","date":"2025-05-01","externalUrl":null,"permalink":"/series/%E6%8A%80%E6%9C%AF%E6%9D%82%E9%A1%B9/","section":"ç³»åˆ—","summary":"","title":"æŠ€æœ¯æ‚é¡¹","type":"series"},{"content":"","date":"20 March 2025","externalUrl":null,"permalink":"/en/tags/mcts/","section":"Tags","summary":"","title":"MCTS","type":"tags"},{"content":"","date":"20 March 2025","externalUrl":null,"permalink":"/en/tags/notes/","section":"Tags","summary":"","title":"Notes","type":"tags"},{"content":"","date":"20 March 2025","externalUrl":null,"permalink":"/en/tags/paper/","section":"Tags","summary":"","title":"Paper","type":"tags"},{"content":"","date":"20 March 2025","externalUrl":null,"permalink":"/en/series/paper-notes/","section":"Seires","summary":"","title":"Paper Notes","type":"series"},{"content":"","date":"20 March 2025","externalUrl":null,"permalink":"/en/tags/reasoningenhance/","section":"Tags","summary":"","title":"ReasoningEnhance","type":"tags"},{"content":"","date":"2025-03-20","externalUrl":null,"permalink":"/tags/%E6%8E%A8%E7%90%86%E5%A2%9E%E5%BC%BA/","section":"æ ‡ç­¾","summary":"","title":"æ¨ç†å¢å¼º","type":"tags"},{"content":"","date":"6 March 2025","externalUrl":null,"permalink":"/en/series/ai-project/","section":"Seires","summary":"","title":"AI-Project","type":"series"},{"content":"","date":"6 March 2025","externalUrl":null,"permalink":"/en/tags/cloud-service/","section":"Tags","summary":"","title":"Cloud-Service","type":"tags"},{"content":"","date":"2025-03-06","externalUrl":null,"permalink":"/tags/%E4%BA%91%E6%9C%8D%E5%8A%A1/","section":"æ ‡ç­¾","summary":"","title":"äº‘æœåŠ¡","type":"tags"},{"content":"","date":"5 March 2025","externalUrl":null,"permalink":"/en/tags/qdrant/","section":"Tags","summary":"","title":"Qdrant","type":"tags"},{"content":"","date":"4 March 2025","externalUrl":null,"permalink":"/en/tags/full-stack/","section":"Tags","summary":"","title":"Full-Stack","type":"tags"},{"content":"","date":"2025-03-04","externalUrl":null,"permalink":"/tags/%E5%85%A8%E6%A0%88%E5%BC%80%E5%8F%91/","section":"æ ‡ç­¾","summary":"","title":"å…¨æ ˆå¼€å‘","type":"tags"},{"content":"","date":"5 February 2025","externalUrl":null,"permalink":"/en/tags/neo4j/","section":"Tags","summary":"","title":"Neo4j","type":"tags"},{"content":"","date":"27 January 2025","externalUrl":null,"permalink":"/en/tags/management/","section":"Tags","summary":"","title":"Management","type":"tags"},{"content":"","date":"27 January 2025","externalUrl":null,"permalink":"/en/series/project-reports/","section":"Seires","summary":"","title":"Project Reports","type":"series"},{"content":"","date":"27 January 2025","externalUrl":null,"permalink":"/en/tags/report/","section":"Tags","summary":"","title":"Report","type":"tags"},{"content":"","date":"27 January 2025","externalUrl":null,"permalink":"/en/tags/schedule/","section":"Tags","summary":"","title":"Schedule","type":"tags"},{"content":"","date":"2025-01-27","externalUrl":null,"permalink":"/tags/%E6%8A%A5%E5%91%8A/","section":"æ ‡ç­¾","summary":"","title":"æŠ¥å‘Š","type":"tags"},{"content":"","date":"2025-01-27","externalUrl":null,"permalink":"/tags/%E6%97%A5%E7%A8%8B%E7%AE%A1%E7%90%86/","section":"æ ‡ç­¾","summary":"","title":"æ—¥ç¨‹ç®¡ç†","type":"tags"},{"content":"","date":"2025-01-27","externalUrl":null,"permalink":"/series/%E9%A1%B9%E7%9B%AE%E6%8A%A5%E5%91%8A/","section":"ç³»åˆ—","summary":"","title":"é¡¹ç›®æŠ¥å‘Š","type":"series"},{"content":"The CUMCM (Chinese Undergraduate Mathematical Contest in Modeling) is a competition where participants do not directly enter the national finals.\nInstead, they must go through a selection process involving the University-level competition (æ ¡èµ›), Provincial-level competition (çœèµ›), and only after that will their papers be submitted for evaluation by national experts. Thus, the competition is informally divided into three stages: University-level competition, Provincial-level competition, and National-level competition.\n","date":"16 January 2025","externalUrl":null,"permalink":"/en/tags/cumcm/","section":"Tags","summary":"\u003cp\u003eThe \u003cstrong\u003eCUMCM\u003c/strong\u003e (Chinese Undergraduate Mathematical Contest in Modeling) is a competition where participants do not directly enter the national finals.\u003c/p\u003e\n\u003cp\u003eInstead, they must go through a selection process involving the \u003cstrong\u003eUniversity-level competition\u003c/strong\u003e (æ ¡èµ›), \u003cstrong\u003eProvincial-level competition\u003c/strong\u003e (çœèµ›), and only after that will their papers be submitted for evaluation by national experts. Thus, the competition is informally divided into three stages: \u003cstrong\u003eUniversity-level competition\u003c/strong\u003e, \u003cstrong\u003eProvincial-level competition\u003c/strong\u003e, and \u003cstrong\u003eNational-level competition\u003c/strong\u003e.\u003c/p\u003e","title":"CUMCM","type":"tags"},{"content":"","date":"16 January 2025","externalUrl":null,"permalink":"/en/series/mathmodel/","section":"Seires","summary":"","title":"MathModel","type":"series"},{"content":"","date":"16 January 2025","externalUrl":null,"permalink":"/en/tags/matlab/","section":"Tags","summary":"","title":"MATLAB","type":"tags"},{"content":"","date":"15 January 2025","externalUrl":null,"permalink":"/en/tags/mysql/","section":"Tags","summary":"","title":"MySQL","type":"tags"},{"content":"","date":"10 January 2025","externalUrl":null,"permalink":"/en/tags/hugo/","section":"Tags","summary":"","title":"Hugo","type":"tags"},{"content":"","date":"10 January 2025","externalUrl":null,"permalink":"/en/series/project-report/","section":"Seires","summary":"","title":"Project Report","type":"series"},{"content":"","date":"6 January 2025","externalUrl":null,"permalink":"/en/series/casual-essay/","section":"Seires","summary":"","title":"Casual Essay","type":"series"},{"content":"","date":"6 January 2025","externalUrl":null,"permalink":"/en/tags/experience/","section":"Tags","summary":"","title":"Experience","type":"tags"},{"content":"","date":"2025-01-06","externalUrl":null,"permalink":"/tags/%E7%BB%8F%E5%8E%86/","section":"æ ‡ç­¾","summary":"","title":"ç»å†","type":"tags"},{"content":"","date":"2025-01-06","externalUrl":null,"permalink":"/series/%E9%9A%8F%E7%AC%94/","section":"ç³»åˆ—","summary":"","title":"éšç¬”","type":"series"},{"content":"","date":"3 January 2025","externalUrl":null,"permalink":"/en/tags/ai/","section":"Tags","summary":"","title":"AI","type":"tags"},{"content":"","date":"3 January 2025","externalUrl":null,"permalink":"/en/tags/musings/","section":"Tags","summary":"","title":"Musings","type":"tags"},{"content":"","date":"2025-01-03","externalUrl":null,"permalink":"/tags/%E9%81%90%E6%83%B3/","section":"æ ‡ç­¾","summary":"","title":"éæƒ³","type":"tags"},{"content":"","date":"12 September 2024","externalUrl":null,"permalink":"/en/tags/math/","section":"Tags","summary":"","title":"Math","type":"tags"},{"content":"","date":"10 August 2024","externalUrl":null,"permalink":"/en/tags/python/","section":"Tags","summary":"","title":"Python","type":"tags"},{"content":"","date":"12 July 2024","externalUrl":null,"permalink":"/en/tags/latex/","section":"Tags","summary":"","title":"LaTeX","type":"tags"},{"content":"","date":"12 July 2024","externalUrl":null,"permalink":"/en/tags/overleaf/","section":"Tags","summary":"","title":"Overleaf","type":"tags"},{"content":"","externalUrl":null,"permalink":"/en/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"Mathematical Modeling is the use of mathematical models to precisely and systematically describe objects in life, and it is an important combination of mathematics and practice.\n","externalUrl":null,"permalink":"/en/series/ai%E5%B7%A5%E7%A8%8B/","section":"Seires","summary":"\u003cp\u003e\u003ccode\u003eMathematical Modeling\u003c/code\u003e is the use of mathematical models to \u003cstrong\u003eprecisely and systematically\u003c/strong\u003e describe objects in life, and it is an important combination of mathematics and practice.\u003c/p\u003e\n\u003chr\u003e","title":"Mathematical Modeling","type":"series"},{"content":"Mathematical Modeling is the use of mathematical models to precisely and systematically describe objects in life, and it is an important combination of mathematics and practice.\n","externalUrl":null,"permalink":"/en/series/%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1/","section":"Seires","summary":"\u003cp\u003e\u003ccode\u003eMathematical Modeling\u003c/code\u003e is the use of mathematical models to \u003cstrong\u003eprecisely and systematically\u003c/strong\u003e describe objects in life, and it is an important combination of mathematics and practice.\u003c/p\u003e\n\u003chr\u003e","title":"Mathematical Modeling","type":"series"}]